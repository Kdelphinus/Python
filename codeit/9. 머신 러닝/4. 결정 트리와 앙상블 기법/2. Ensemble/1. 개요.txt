1. 결정트리의 한계와 배우는 이유
- 결정 트리는 부정확성이라는 큰 단점을 가지기에 머신 러닝 모델이 되기 어렵다
- 그러나 응용하면 성능이 좋은 다른 모델들을 만들 수 있기에 기초로 배우는 경우가 많다


2. 앙상블(ensemble)
- 수많은 모델들을 만들고 각 모델들의 예측을 합쳐서 종합적인 판단을 하는 방법


    2.1 Bagging(Bootstrap aggregate)
    - 임의의 bootstrap 데이터 셋들을 만든다
    - bootstrap 데이터 셋을 사용해서 수많은 모델들을 만든다
    - 이 모델들의 예측(aggregate)을 종합한다


        2.1.1 랜덤 포레스트
        - 트리 모델들을 임의로 많이 만들어서 다수결 투표로 결과를 종합하는 알고리즘
        - Bagging을 사용하는 여러 알고리즘 중 하나의 예시


            2.1.1.1 Bootstrapping
            - 갖고 있는 데이터 셋으로 다른 데이터 셋을 만들어내는 방법
            - 원래 데이터 셋에서 임의로 같은 크기의 데이터 셋을 만드는 것
                - 이때, 어떤 데이터는 중복되고 어떤 데이터는 사용되지 않아도 괜찮다
            - 이렇게 만들어진 데이터 셋을 Bootstrap 데이터 셋이라고 한다
            
            - 이 방법은 모든 모델들을 정확히 똑같은 데이터 셋으로 학습시키면 결과 다양성이 떨어질 수 있는 단점을 보완하는 효과가 있다
            - 즉, 각 모델을 임의로 만들어준 Bootstrap 데이터 셋으로 학습시켜 위의 문제를 방지하는 것


            2.1.1.2 임의로 결정 트리 만들기
            - Bootstrapping을 사용해서 임의로 데이터 셋을 만든다
            - 결정 트리를 만들 때 속성을 임의로 고르면서 만든다
            - 위 두 과정을 여러 번 반복하여 임의의 결정 트리를 많이 만든다


            2.1.1.3 투표
            - 임의로 만들어진 결정 트리들의 예측 값을 종합하여 투표한 결과를 리턴하는 것이 랜덤 포레스트


    2.2 Boosting
    - 일부러 성능이 안 좋은 모델들을 사용한다
    - 더 먼저 만든 모델들의 성능이, 뒤에 있는 모델이 사용할 데이터 셋을 바꾼다
    - 모델들의 예측을 종합할 때, 성능이 좋은 모델의 예측을 더 반영한다
    - 성능이 안 좋은 약한 학습자(weak learner)들을 합쳐서 성능을 극대화한다


        2.2.1 에다 부스트(Adaboost)
        - 루트 노드 하나과 분류 노드 두 개를 갖는 얕은 임의의 결정 트리들을 만든다
        - 이렇게 만들어진 결정 트리들(결정 스텀프)은 평균적으로 50%의 확률을 간신히 넘는다
        - 스텀프를 지날 때마다 맞은 데이터는 중요도를 낮추고 틀린 데이터는 중요도를 높여준다
        - 독감 예측 스텀프와 감기 예측 스텀프가 각각 2개일지라도 성능의 합이 더 좋은 결정에 따른다

        - 정리하자면
            - 성능이 좋지 않은 결정 스텀프를 많이 만든다
            - 각 스텀프는 전에 왔던 스텀프들이 틀린 데이터들을 더 중요하게 맞춘다
            - 예측을 종합할 때, 성능이 좋은 스텀프의 의견 비중을 더 높게 반영한다
        

            2.2.1.1 스텀프 성능 계산하기
            - 처음은 각 데이터의 중요도를 같게 설정한다(1/m, 즉 중요도의 합이 1이 되도록)
            - P_tree = log((1-total_error)/total_error) / 2
                - P_tree: 스텀프의 성능
                - total_error: 틀리게 예측한 데이터 중요도의 합
            -  total_error가 1에 가까워질수록 성능이 기하급수적으로 낮아지고 0에 가까워질수록 기하급수적으로 커진다
                - total_error가 1이면 다 틀렸단 의미고 0이면 다 맞췄다는 의미기 때문
            - total_error가 0.5면 절반만 맞췄다는 의미이기에 스텀프의 성능은 0으로 나온다

            - 정리
                - 첫 번째 스텀프는 결정 트리를 만들 때처럼 지니 불순도를 써서 만든다
                - 모든 데이터는 중요도가 있다. 이는 total_error를 계산하는데 사용한다
                - total_error를 이용하면 스텀프의 성능을 계산할 수 있다


            2.2.1.2 데이터 중요도 바꾸기
            - 틀리게 예측한 데이터 중요도
                - weight_new = weight_old * e^(P_tree)
                - P_tree: 스텀프의 성능

            - 맞게 예측한 데이터 중요도
                - weight_new = weight_old * e^(-P_tree)

            - 계산 후, 중요도의 합이 1이 되게 하기 위해서 각 중요도마다 밑의 계산을 해준다
                - 중요도 / 모든 중요도의 합
            

            2.2.1.3 스텀프 추가하기
            - 중요도에 따라서 각 데이터마다 범위를 지정하고 임의 숫자가 속한 범위를 가진 데이터를 추가한다
            - 이러면 틀렸던 데이터는 더 많이 들어가고 맞춘 데이터는 덜 들어가게 된다
            - 그 후, 지니 불순도를 이용하여 성능 좋은 새로운 스텀프를 만들어준다
            - 자세한 건 사진 참고

            - 정리
                - 전 모델이 틀리게 예측한 데이터의 중요도는 올려주고, 맞게 예측한 데이터의 중요도는 낮춘다
                - 데이터의 중요도를 사용하여 새로운 데이터 셋을 만든다
                - 새로운 데이터 셋으로 스텀프를 만들어 스텀프의 성능을 계산한다
                - 과정 반복
            

            2.2.1.4 최종 예측
            - 스텀프들이 리턴한 예측값이 같은 스텀프들의 성능 합을 구한다
            - 그리고 성능 합이 높은 예측값으로 리턴한다
            