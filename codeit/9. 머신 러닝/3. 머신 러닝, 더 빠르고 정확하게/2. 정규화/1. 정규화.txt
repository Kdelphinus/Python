1. 편향(bias)과 분산(variance)
- 편향이 높다: 모델이 너무 간단하여 복잡한 곡선 관계를 학습할 수 없을 때
- 편향이 낮다: 모델이 데이터들 사이의 관계를 완벽하게 학습한 것
- 그러나 편향이 낮다고 무조건 잘 맞는 모델은 아니다
    traing set에 완벽히 맞췄기 때문, 너무 북잡해서 맞지 않다

- 분산: 데이터 셋 별로 모델이 얼마나 일관된 성능을 보여주는가
    편향이 너무 낮으면 traing set을 외운 것으로 traing set에선 완벽하지만
    test set에선 편향이 높은 모델보다 낮은 정확도를 보이는 등 일관되지 못한 성능을 보인다
    -> 분산이 높다라고 할 수 있다

- 편향이 높은 모델은 너무 간단해서 주어진 데이터의 관계를 잘 학습하지 못한다
- 편향이 낮은 모델은 주어진 데이터의 관계를 잘 학습한다
- 분산은 다양한 테스트 데이터가 주어졌을 때 모델의 성능이 얼마나 일관적인지 보여준다


2. 편향-분산 트레이드오프(Bias-Variance Tradeoff)
    2.1 과소적합(underfit) 모델
    - 복잡도가 떨어지기 때문에 곡선 관계를 학습할 수 없다
    - 어떤 데이터가 주어져도 일관적인 성능을 낸다
    - 즉, 편향이 높고 분산이 낮은 모델

    2.2 과적합(overfit) 모델
    - traing 데이터에 대한 성능은 아주 높다
    - 처음보는 testing 데이터에 대한 성능은 떨어진다
    - 즉, 편향이 낮고 분산이 높다

    2.3 편향-분산 트레이드오프
    - 일반적으로 편향과 분산은 반비례 관계이다
    - 이를 편향-분산 트레이드오프라고 한다
    - 그렇기에 편향과 분산, 다르게는 과소적합과 과적합의 적당한 균형을 찾아야 한다


3. 과소적합과 과적합을 막는 방법
- 과소적합한 모델, 데이터가 적을 때는 복잡한 모델을 쓰면 된다


4. 정규화(Regularization)
- 모델 과적합 현상을 방지해 주는 방법 중 하나
- 과적합은 theta값이 너무 커지기에 발생한다
- 그렇기에 정규화는 theta값이 너무 커지지 않도록 하여 과적합을 방지하는 방법이다

    4.1 L1 정규화(Lasso Regression, Lasso 모델)
    - 손실 함수에 (theta 값들의 절댓값의 총합(theta_0는 상수므로 제외) * lambda)를 더해주는 방법
    - lambda가 크면 theta를 줄이는 것이 우선이고 작으면 평균 제곱 오차를 줄이는 게 우선으로 처리하는 것

    4.2 L2 정규화(Ridge Regression, Ridge 모델)
    - theta의 절댓값이 아니라 theta^2의 총합을 구하는 방식
    - lambda가 크면 theta를 줄이는 것이 우선이고 작으면 평균 제곱 오차를 줄이는 게 우선으로 처리하는 것

    4.3 L1, L2 정규화 일반화
    - 다중 회귀 또는 다항 회귀 모델을 만들 때, LinearRegression 대신 Lasso나 Ridge를 사용하면 된다
    - LogisticRegression 모델은 자동으로 L2 정규화를 적용하기에 모델을 바꾸지 않아도 된다
        - 아니면 penalty를 이용해 지정할 수 있다
        - 예시
            LogisticRegression(penalty='none') # 정규화 사용 안함
            LogisticRegression(penalty='l1')  # L1 정규화 사용
            LogisticRegression(penalty='l2')  # L2 정규화 사용
            LogisticRegression()  # L2 정규화 사용
    
    4.4 L1과 L2의 차이점
    - L1은 여러 theta 값들을 0으로 만들어 준다. 모델에 중요하지 않다고 생각되는 속성들을 아예 없애버림
    - L2는 theta 값들을 조금씩 줄여준다. L1처럼 모델에 사용되는 속성을 없애지는 않는다

    - 그렇기에 L1은 사용되는 속성 또는 변수를 줄이고 싶을 때 사용된다. 이는 과적합 뿐만 아니라 모델을 학습시킬 때 많은 자원을 소모를 막아준다.
    - 딱히 속성의 개수를 줄이지 않아도 될 경우, L2를 사용한다

    - 변수가 2개라 할 때
        - L1은 |theta_1| + |theta_2| = t로 마름모 형태의 그래프를 그린다
        - 그렇기에 6번 사진처럼 theta 중 한 쪽이 0일때 만날 확률이 높다

        - L2는 (theta_1)^2 + (theta_2)^2 = t로 원 형태의 그래프를 그린다
        - 그렇기에 7번 사진처럼 theta 둘 다 0이 아닐 확률이 높다
    - 이는 변수가 더 많아져도 그림으로 나타내긴 어렵지만 형태는 동일하다
