1. 선형 회귀(Linear Regression)
- 데이터에 가장 잘맞는 선(최적선)을 찾는 것
- 그리고 얻고 싶은 값을 최적선을 이용하여 예측하는 것
- 단순하면서 유용하고 다른 알고리즘의 기본이 됨
- 지도 학습 중 회귀(답을 알려주며 그 답은 연속적인 값, 즉 수많은 값 중 하나)
- 예시) 집 크기를 이용해 집 값을 맞추는 프로그램


2. 용어
- 목표 변수(target variable / output variable): 맞추려고 하는 값, y로 표시
    - 예시) 집 값
- 입력 변수(input variable / feature): 맞추는데 사용하는 값, x로 표시
    - 예시) 집 크기
- 학습 데이터의 수: m으로 표현 
- x^(1)은 1번 데이터의 입력 변수, y^(4)는 4번 데이터의 목표 변수를 의미


3. 가설 함수(hypothesis function)
- 최적선을 찾기 위해 대입해보는 모든 함수
- h(x) = θ0 + θ1 x1 (θ0 은 상수항, x0이 붙어도 x0은 1로 고정)
- x1, x2... 는 주어진 자료를 넣어야 하기에 실제적으론 θ에 여러 값을 넣어보며 가설 함수를 찾는다


4. 평균 제곱 오차(MSE)
- 가설 함수 평가를 평가하는 방법
- 가설 함수의 값과 실제 값의 차이를 모두 제곱하여 더한 뒤, 개수만큼 나눈다
- 오차를 제곱하는 이유는 양수와 음수의 차이를 없애고 오차가 클 수록 더욱 크게 나타나도록 하기 위해서이다
- 일반식은 구글링해서 보기
- 평균 제곱 오차가 작을수록 최적함수가 잘 맞는다


5. 손실 함수(Loss Function)
- 가설 함수의 성능을 평가하는 함수
- 손실 함수가 작을수록 데이터에 잘 맞는다
- 선형 회귀의 경우에는 평균 제곱 오차가 손실 함수이다
- 변수가 θ인데 이는 x들은 주어진 자료가 들어가는 곳이기에 변할 수 없고 θ를 바꿔가며 최적선을 찾기 때문


6. 경사 하강법(Gradient Descent)
- 손실 함수가 가장 작은, 극소점을 찾아가야 성능이 좋은 것이다
- 현재 위치의 기울기를 구하고 양수이면 작아지는 방향으로, 음수면 커지는 방향으로 움직여야 한다
- 2. 경사 하강법 업데이트.png 참고


7. 학습률 알파
    7.1 알파가 너무 큰 경우
    - 경사 하강을 한 번 할 때마다 θ의 값이 크게 바뀜
    - 극소점을 가운데 두고 왼쪽과 오른쪽을 왔다갔다 움직이며 내려갈 수 있다
    - 아주 크면 오히려 극소점에서 멀어질 수 있다

    7.2 알파가 너무 작은 경우
    - 극소점까지 너무 오래 걸린다

    7.3 알파가 적당한 경우
    - 경사 하강을 하면서 손실이 줄어들고 있는 것을 시각화 했을 때, 
        y = (a) ^X (0 < a < 1) 그래프와 유사한 형태가 적당하다
    - 학습률이 너무 크면 손실 그래프가 갈수록 기하급수적으로 커지고
    - 작으면 iteration 수가 너무 많아진다
    - 일반적으로 1.0 ~ 0.0 사이의 숫자로 정하고 여러 개를 실험해보면서 경사 하강을 제일 적게 하면서 손실이 잘 줄어드는 학습률을 선택함


8. 모델 평가
- 가설 함수는 세상에 일어나는 상황을 수학적으로 표현한다는 의미에서 '모델'이라고 부른다
- 우리가 구한 모델이 얼마나 예측을 잘하는지 평가해야 한다
- 평균 제곱근 오차(RMSE)를 이용한다
    - 평균 제곱 오차를 제곱근하면 된다
    - 평균 제곱 오차는 데이터의 단위의 제곱이 되어 있기 때문에 제곱근을 하는 것이다
- 학습 데이터로 평가를 하면 평균 제곱근 오차가 작게 나오는 것은 당연하다
- 그렇기에 학습 데이터(training set)와 평가 데이터(test set)로 나눠서 각각의 용도에 맞게 사용한다


