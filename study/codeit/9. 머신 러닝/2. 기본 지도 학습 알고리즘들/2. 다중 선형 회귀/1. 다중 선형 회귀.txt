1. 다중 선형 회귀
- 여러 입력 변수를 받아서 하는 선형 회귀
- 시각화는 어려우나 기본 개념은 선형 회귀와 거의 유사


2. 표기법
- 속성(feature): 입력 변수와 같음
    - x1, x2, x3... 등으로 표기
- 데이터의 개수: m
- X1^(3) (1은 밑에 (3)은 지수에 표기)
    - 3번 째 데이터의 1번 째 속성이란 의미


3. 다중 선형 회귀 가설 함수
- h(x) = θ0 + θ1 x1 + θ2 x2 + θ3 x3 + ...(θ0 은 상수항, x0이 붙어도 x0은 1로 고정)
- 선형 회귀와 목적과 사용법이 동일하다
- θ와 x를 벡터로 간단하게 표현할 수 있다
- h(x) = θ.T x


4. 다중 선형 회귀 경사 하강법
- 손실 함수로 모델을 평가하고 이를 줄이기 위해 경사 하강법을 이용한다
- 다중 선형 회귀에선 선형 회귀와 식의 모양은 똑같다
- 선형 회귀와 동일하나 모든 θ값을 업데이트하면 된다


5. 정규 방정식
- 극소점은 기울기가 0이기 때문에 미분에서 0이 되는 값을 찾는 방법


6. 경사 하강법 vs 정규 방정식
- 적합한 학습율을 찾거나 정해야 한다 vs 학습율을 정할 필요가 없다
- 반복문을 사용해야 한다 vs 한 단계로 계산을 끝낼 수 있다
- 입력 변수의 개수 n이 커도 효율적으로 연산할 수 있다 vs 입력 변수의 개수 n이 커질수록 월등히 비효율적이다
    - 행렬 연산을 하는 것이 경사 하강법을 하는 것보다 더 비효율적이기 때문
- 역행렬이 없어도 된다 vs 역행렬이 존재하지 않을 수도 있다
    - 그러나 pseudo inverse를 이용해서 다르게 계산하는 방법이 있기에 큰 문제는 되지 않는다

- 입력 변수(속성)의 수가 엄청 많을 때(1,000개를 기준으로 사용할 때가 많음) 경사 하강법을, 그 외는 정규 방정식을 보통 사용함


7. Convex 함수
- 경사 하강법이나 정규 방정식은 극대, 극소점이 여러개면 최저점을 찾아갈 수 없다
- 그렇기에 경사 하강법이나 정규 방정식은 Convex 함수(아래로 볼록한 함수)에서 사용해야 한다
- 선형 회귀 손실 함수로 사용하는 MSE는 항상 Convex 함수
