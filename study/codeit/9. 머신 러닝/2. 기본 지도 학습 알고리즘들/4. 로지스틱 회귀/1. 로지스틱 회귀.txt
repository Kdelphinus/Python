1. 분류 문제
- 선형 회귀는 예외적인 데이터 하나에 너무 민감하게 반응한다
- 그렇기에 분류 문제는 로지스틱 회귀를 이용한다


2. 로지스틱 회귀
- 로지스틱은 값이 0 또는 1만 나온다
- s(x) = 1 / (1 + e^(-x)) -> 시그모이드 함수
- 무조건 0 또는 1이 나오기에 분류에 적합하다
- 리턴 값이 0과 1 사이의 연속적인 값이기에 회귀라 부르지만 
- 대부분 0.5를 기준으로 큰지 작은지를 판별하기 때문에 분류에 많이 사용


3. 로지스틱 회귀 가설 함수
- h(x) = θ_0 + θ_1 * x + θ_2 * x^2 + θ_3 * x^3....
  -> h(x) = θ.T @ x (선형 회귀 가설 함수)
- 로지스틱 회귀 가설 함수는 선형 회귀 가설 함수를 시그모이드 함수에 넣어 0과 1사이의 값이 나오도록 만든다
    h(x) = 1 / (1 + e^(-θ.T @ x))


4. 로지스틱 회귀에서 하려는 것
- 입력 변수가 하나라고 가정할 때(즉 θ_0과 θ_1만 있음)
    - θ_0이 증가하면 곡선이 왼쪽으로 움직이고 감소하면 오른쪽으로 움직임
    - θ_1이 증가하면 s모양의 곡선이 조여지고 줄이면 늘어진다
- 이를 통해 학습 데이터에 가장 잘 맞는 시그모이드 모양의 곡선을 찾아내는 것이 목표


5. 결정 경계(Decision Boundary)
- 앞서 이야기했듯이 가설 함수의 값은 0과 1사이를 리턴한다
- 그렇기에 우리는 0.5를 기준으로 0과 1로 나눈다고 가정했다, 이때 0.5가 결정 경계가 된다
- 이를 통해 입력 변수가 하나일 땐 특정 값, 두 개일 땐 선으로 결정경계를 그릴 수 있다


6. 로그 손실
- 로지스틱 회귀의 가설 함수를 평가하는 손실 함수
- 로지스틱 회귀 손실 함수는 평균 제곱 오차 대신 로그 손실을 이용한다
- 실제 아웃풋이 0이냐 1이냐를 나눠 로그 그래프로 손실을 판단한다
- 사진 참고


7. 로지스틱 회귀 손실 함수
- 로그 손실을 이용하여 만듬
- 모든 학습 데이터에 대하여 로그 손실을 계산한 후, 평균을 낸다


8. 로지스틱 회귀 경사 하강법
- 임의의 θ 값으로 시작
- θ에 대하여 편미분하여 구한 것을 각 θ에 넣어 합치면 된다
- 선형 회귀와 같은 식이 나오지만 가설 함수 자리에 일차항이 아닌 시그모이드 함수가 들어가는 것을 기억해야 한다


9. 분류가 3개 이상일 때
- 0과 1로만 구분하는 것이 아니라 3개 이상일 경우
- 예를 들어 직장, 친구, 스팸 메일로 나눠야 할 때
  - 먼저 직장과 직장이 아닌 것으로 구분하는 가설 함수 h1
  - 그 후, 친구와 친구 아닌 것으로 구분하는 가설 함수 h2
  - 마지막으로 스팸인거와 아닌 것으로 구분하는 가설 함수 h3
  - h1, h2, h3에 각각 넣어 가장 높은 확률이 나오는 카테고리로 분류하면 된다


10. 로지스틱 회귀와 정규 방정식
- 선형 회귀는 손실 함수(MES)가 아래로 볼록하면서 편미분 원소들을 모두 선형식으로 나타낼 수 있었기에 단순 행렬 연산으로 최적의 세타를 찾을 수 있었음
- 그러나 로지스틱 회귀는 손실 함수(로그 손실)이 아래로 볼록하지만 편미분 원소들이 선형식이 아니기에 단순 행렬 연산만으로 찾아낼 수 없다(세타가 e의 지수에 포함되었기에 일차식 표현이 불가)
