1. MINIST 데이터
- Modified National Institute of Standards and Technology database
- 손글씨로 쓴 숫자들의 흑백 이미지를 모아놓은 것
- 28 x 28 픽셀 이미지
- 검정, 흰색, 회색이 0 ~ 1사이의 값으로 나타내어진다(어두울수록 작아짐)
    - 정확히는 0 ~ 255 사이의 숫자지만 이를 min-max normalization하여 0 ~ 1로 바꿈
    - normalization하는 이유는 신경망 모델의 학습 속도와 정확도를 향상시키기 위함
- 픽셀 데이터를 입력 변수, 이미지가 나타내는 숫자를 목표 변수로 사용
- 784개의 픽셀 데이터는 파이썬 리스트에 일렬로 정리되어 있다
    - 정확히는 밑에처럼 리스트와 이미지가 나타내는 숫자가 저장되어 있다
    ([0, 0, ..., 0.66, 0.12, 0.99, 0.80, ..., 0, 0], 5)

    1.1 로지스틱 회귀를 이용
    - 입력 변수 784개 + 1개(상수항)에 각각 theta값을 곱해준 뒤, 더해준다
    - 이를 시그모이드 함수에 넣으면 결과는 0과 1사이의 값을 가진다
    - 이 값을 가지고 숫자를 분류한다
    - 합이 클수록 1에 가깝고 작을수록 0에 가깝다
    - theta 값을 바꿔가면서 정답이면 1에 가깝고 오답이면 0에 가깝도록 만드는 theta값을 찾는 것

    - 2번 사진을 참고하면 빨강은 theta값이 낮고 초록은 theta값을 높여서 다른 숫자와 비교한다
    - 0 ~ 9까지 분류하는 로지스틱 모델을 만들고 각각 넣고 가장 큰 값이 나오는 모델의 숫자로 판명한다


2. 인공 뉴런
- 정보를 받아 처리하고 전달하는 뉴런의 행동 방식을 인공으로 만든 것
- 이는 로지스틱 회귀 모델의 작동 방식과 거의 동일하다
- theta는 결과값에 얼마나 영향을 미치는지 정하기에 가중치(weight)라 부르고 w 기호를 사용한다
- 이때 theta0는 가중치가 아니기에 편향(bias)이라고 부르며 b 기호를 사용한다
- 활성 함수(w.T @ x + b) 
    - 로지스틱 회귀에서 활성 함수가 시그모이드 함수


3. 인공 신경망
- 입력층(시작)과 바로 앞 층 뉴런들의 출력을 인풋으로 받는 로지스틱 모델들이 층으로 이루어져 있다
- 3번 사진 참고
- 입력 층부터 시작해서 층 단위로 뉴런의 출력을 계산
- MINIST 기준으로 출력층(마지막 층)에 0~9까지 10개의 뉴런이 있다
- 출력층에서 가장 활성이 높은 값으로 예측한다
- 입력층과 출력층 사이의 층들은 모두 은닉층이라고 한다

- 수많은 뉴런들을 서로 그물망처럼 묶어서 정보를 앞에서 뒤로 전달
- 출력층에 나온 값을 가지고 예측
- 데이터에 가장 잘 맞는 가중치와 편향들을 찾아내는 것이 목표

    3.1 은닉층
    - 4번 사진 참고
    - 세부적인 부분을 보며 활성정도를 예측
    - 입력층에 가까울수록 더 작은 단위의 패턴을 찾는데 최적화

- 층을 넘어갈 때마다 더 고차원적인 패턴을 찾아내는 것
- 신경계 뉴런들은 작은 패턴을 찾아내는 데 특화
- 작은 단위의 패턴을 다음 층의 인풋으로 사용해서 더 섬세하게 예측


4. 신경망 표현
    4.1 층의 출력
    - 층을 세거나 부를 땐 입력층을 제외한 은닉층의 첫번째부터 첫번째 층이다
    - 출력층까지 층을 센다
    - L로 층의 수를 나타낸다
    - 입력층은 0번째 층으로 나타내기도 한다

    4.2 뉴런의 출력
    - a를 사용하여 나타냄
    - 계수는 층, 밑은 뉴런의 위치를 나타냄
    - 5번 사진 참고

    4.3 가중치 표현
    - w의 계승에 소속된 층의 번호를 쓴다
        - 이때, 두 층 사이에 있는 가중치는 뒤의 층에 속한 것으로 간주한다
    - 6번 사진 참고
    - 하나의 가중치만 표현할 땐, 밑에 "출발 뉴런 위치, 도착 뉴런 위치"를 써준다
    - 행렬로 표현할 땐 l x (l - 1)크기의 행렬이 만들어진다(순전파 계산을 편하게 하기 위하여)

    4.4 편향 표현
    - b의 계수에 소속된 층을 쓴다
    - b의 밑에 소속된 뉴런을 쓴다
    - 실제 편향들은 상수임을 기억하자

    4.5 입력 변수, 목표 변수
    - 8번 사진 참고
    - 8번 사진에서 입력 변수는 픽셀들, 목표 변수는 숫자이다
    - 입력 변수는 계수에 입력 변수의 순서, 밑에 그 입력 변수 안에서 순서를 적는다
    - 목표 변수는 크고 작음에 의미가 없기 때문에 one-hot encoding을 사용한다


5. 순전파
- 신경망에서 input 데이터가 들어가서, 마지막 층까지 처리돼서 출력되는 과정
- 9번 사진 과정이 층 순서대로 진행됨
- 이렇게 입력층부터 출력층까지 진행되는 것을 순전파라고 한다

    5.1 순전파 행렬 연산
    - 식의 항이 너무 많기 때문에 행렬을 이용하여 표현함
    - 9번 사진 참고
    - 구해진 z를 활성함수(여기선 시그모이드 함수)에 넣어 출력을 구한다


6. 신경망 학습
- 입력 변수에 대한 목표 변수를 잘 예측하는 가중치와 편향을 찾는 것
- 가중치와 편향으로 예측할 수 있고 이 예측값이 얼마나 좋은지 판단할 수 있어야 한다
- 10번 사진 참고

    6.1 가설 함수(h)
    - 순전파를 이용하여 입력 변수들의 예측값을 구한다

    6.2 손실 함수(j)
    - 예측값과 목표 변수의 차이를 제곱하여 모두 더하는 것을 모든 학습 데이터에서 반복한다
    - 그 후, 그 값들을 모두 더하고 모든 데이터의 수와 그 층들의 뉴런 수로 나누어 제곱 평균 오차를 구하여 사용한다

    6.3 경사 하강법
    - 이론은 앞서 봐왔던 것과 동일
    - 손실 함수를 편미분한 값들을 빼준다

    - 신경망에서는 각 변수들에서 편미분을 구하는 것이 복잡하다
    - 하나의 가중치를 바꾸면 그 후에 있는 모든 층의 뉴런들에게 영향을 준다
    - 굉장히 복잡한 합성 함수이기에 계산이 어렵다
    - 11번 사진 참고

    6.4 합성 함수
    - 함수 안에 또 다른 함수가 들어 있는 함수
    - 함수의 인풋이 또 다른 함수
    - 신경망의 손실 함수는 모두 합성 함수로 이루어져 있다

    6.5 연쇄 법칙
    - 12번 사진 참고


7. 역전파
- 출력층부터 입력층 순서로 확인해보는 것
- 연쇄 법칙을 사용해서 각 변화를 더 작은 변화로 표현하고 뒤 층들에서 계산한 편미분 값들을 앞 층 요소들의 편미분을 계산할 때 사용함으로 편미분을 간단하게 만든다
- 13번 사진 참고

    7.1 각 층마다 하나의 노드만 있다고 할 때
        - 가중치의 변화, 편향의 변화, 전 노드 출력의 변화는 z, a, j에 영향을 준다
        - 전 노드의 가중치의 변화, 전 노드의 편향의 변화, 전전 노드의 출력의 변화는 각각 전 노드의 z, a를 변화시키고 이것은 위 노드의 z, a, j를 변화시킨다
        - 그렇기에 처음 구한 값들을 대입하면 된다
        - 즉, 뒤 층에서 계산한 전 층 뉴런에 대한 편미분 값을 앞 층들로 넘겨주면 계산을 간단하게 할 수 있다

    7.2 일반화
        7.2.1 마지막 층과 바로 직전 층
            - 가중치에 대한 편미분
                - 가중치를 바꿔도 직접 바뀌는 건 가중치와 연결된 노드 하나이다

            - 편향에 대한 편미분
                - 편향을 바꿔도 직접 바뀌는 건 편향과 연결된 노드 하나이다
            
            - 전 층 뉴런 출력에 대한 편미분
                - 마지막 전 층 뉴런의 출력이 바뀌면 마지막 층 모든 뉴런들의 출력이 변화한다

        7.2.2 나머지 층에서 편미분
            - 가중치에 대한 편미분
                - w가 z를 바꾸고 z가 a를 바꾸고 a는 j를 바꾼다
                - 이때 a가 j를 바꾸는 것은 이미 뒤 층에서 계산을 했기에 그 값을 가져온다

            - 편향에 대한 편미분
                - b가 z를 바꾸고 z가 a를 바꾸고 a는 j를 바꾼다
                - 이때 a가 j를 바꾸는 것은 이미 뒤 층에서 계산을 했기에 그 값을 가져온다

            - 전 층 뉴런 출력에 대한 편미분
                - 뉴런의 출력은 다음 층의 모든 뉴런 출력에 영향을 미친다
                - 그렇기에 이 변화를 모두 더해서 미분을 계산


8. 신경망 경사 하강
- 신경망에 대해서 경사 하강을 하려면 모든 뉴런, 모든 데이터에 대하여 역전파를 통해 변화한 손실 함수의 경사를 구하고 구한 경사들의 평균을 구한다
- 이것을 반복하여 경사 하강을 진행하면 된다


9. 역전파 행렬 연산
- 15, 16번 사진 참고
- 가중치와 편향은 처음 시작할 때 저장되어 있다
- 순전파를 진행하며 z와 a의 값들을 저장한다
- 그 후, 가지고 있는 z와 a의 값들을 역전파 계산에 이용한다


10. 신경망과 다양한 데이터
- 예를 들어 독감 환자를 열, 기침 정도, 목 통증 정도 몸살 여부로 판단한다면
- 입력층은 4개의 노드로 이루어지고 출력층은 독감을 판단할 노드 하나만 있으면 된다
- 그리고 은닉층은 그 사이에 출력층과 입력층 개수 중간의 개수들로 이루어진다


12. 신경망의 비선형성
- 로지스틱 회귀는 선형적인 결과만 얻을 수 있다(결정 경계가 일차식으로 나오기 때문)
- 그렇기에 적당량만 효과가 있고 너무 적거나 너무 많은 양을 투여했을 땐 효과가 없는 자료들은 선형으로 나눌 수 없다
- 그렇기에 선형적이지 않은 것들은 비선형성을 이용한다

- 신경망의 비선형성은 은닉층 활성 함수를 통해 이루어진다
- 이떄 활성함수를 비선형 함수로 이용해야 비선형성을 띈다(시그모이드 함수도 비선형 함수)
- 활성화 함수가 비선형이면 신경망을 엄청나게 복잡한 비선형 합성 함수로 볼 수 있다 
    - 그렇기에 결정 경계도 비선형적이다
- 그러나 활성화 함수를 선형함수로 쓰면 신경망을 사용해도 선형적인 결정 경계만 나온다


13. 은닉층 활성 함수
    13.1 시그모이드 함수
    - 입력값이 무엇이 들어와도 0과 1사이의 값을 출력한다
    - 손실함수의 경사가 0에 가까워지면 가중치와 편향이 바뀌지 않아 신경망이 학습하기 어렵다(기울기 소실 문제)
    - 그렇기에 현재는 시그모이드 함수를 잘 이용하지 않는다

    13.2 tanh 함수
    - (e^z - e^(-z)) / (e^z + e^(-z))
    - 입력값을 -1과 1사이로 나타낼 수 있다
    - 그러나 시그모이드 함수와 마찬가지로 가장자리로 갈수록 가중치와 편향이 바뀌지 않아 기울기 소실 문제가 일어난다

    13.3 ReLU 함수
    - max(0, z)
    - z가 양수면 기울기가 항상 1, 음수이면 항상 0이다
    - 17번 사진 참고
    - 기울기 소실 문제가 덜 일어남
    - 경사 계산이 훨씬 빠르다(0 아니면 1이기 때문)
    - 은닉층 활성 함수 중 가장 많이 사용된다
    - 음수의 기울기를 조금 변형시키는 leaky ReLU도 있다


14. 출력층 활성 함수
    14.1 시그모이드 함수
    - 0과 1사이의 값을 나타내며 이것은 확률이라고 해석할 수 있다
    - 분류를 할 때, 확률적인 판단을 할 수 있다
    - 다중 분류에서 확률을 모두 더하면 1을 넘어버리기에 사용은 가능하나 자주 사용하지 않는다

    14.2 softmax 함수
    - 모든 출력을 더하면 1이 되도록 만들어 확률의 의미를 지킨다
    - e^z_i / (모든 뉴런 e^z 합)
    - 마지막 층 모든 뉴런의 z값을 알아야지만 출력을 구할 수 있다

    14.3 선형 함수
    - 비선형성은 은닉층에서 해결되기에 출력층에서 사용해도 무방하다
    - 회귀 문제를 풀 때 사용할 수 있다
    - z를 그대로 예측 값으로 사용

    14.4 정리
    - 이분적 분류 문제 -> 시그모이드 함수
    - 다중 분류 문제 -> softmax 함수
    - 회귀 문제 -> 선형 함수


15. 신경망 손실 함수
    15.1 회귀
    - 회귀에선 평균 제곱 오차를 손실 함수로 많이 이용한다
    - 오차를 제곱하여 예측에서 많이 벗어난 데이터를 더 강조할 수 있다
    - 이러한 특징은 머신 러닝 모델들이 학습하는 데 성능적으로 큰 도움을 준다

    15.2 이분적 분류
    - 로그 손실이나 cross entropy를 사용한다
    - 목표 변수를 0과 1로 두고 예측하는데 로그 손실을 이용하면 많이 벗어난 데이터를 더욱 강조할 수 있다

    15.3 다중 분류
    - 로그 손실 함수 사용
    - 19번 사진 참고


16. 다양한 경사 하강법
    16.1 배치(batch) 경사 하강법
    - 한 번 경사 하강을 할 때마다 모든 학습 데이터를 이용
    - 지금까지 사용한 방법
    - 시간이 너무 오래 걸림
    - 정확한 걸음을 오래 고려

    16.2 확률적 경사 하강법
    - 한 번 경사 하강을 할 때마다 임의의 데이터 하나를 이용
    - 짧게 고려하고 빠르게 이동
    - 확실한 방향이 아니기에 극소점 주위에서 계속 맴돌 수 있음

    16.3 미니 배치 경사 하강법
    - 확률적 경사 하강법 중 하나
    - 데이터를 임의 개수로 나누어 데이터 셋을 만든다
    - 한 번 경사 하강을 할 때, 작은 배치(나눈 데이터 셋 중 하나)를 학습 데이터로 사용
    - 신경망은 대부분 미니 배치 경사 하강법을 사용


17. 신경망 정규화
- 신경망의 경우, 가중치들이 너무 커지는 문제를 방지해서 과적합을 막는 방법
- 정규화를 적용할 때머신 러닝 모델의 손실 함수에 정규화 항을 더해준다(편향은 더하지 않는다)
    - L1이면 손실 함수 J에 모든 가중치의 절대값의 합을 더해준다
    - L2이면 모든 가중치의 제곱의 합을 더해준다
- 정규화 항 앞에 붙는 람다는 클수록 모델이 과소적합되고 작을수록 과적합된다


