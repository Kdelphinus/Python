1. 프로세스와 스레드의 차이
    1.1. 프로세스(Process)
    - 실행 중인 프로그램으로 디스크로부터 메모리에 적재되어 CPU의 할당을 받을 수 있는 것
    - 운영체제로부터 주소 공간, 파일, 메모리 등을 할당받으며 이것들을 총칭하여 프로세스라고 한다
    - 구체적으론 함수의 매개변수, 복귀 주소, 로컬 변수와 같은 임시 자료를 갖는 프로세스 스택과 전역 변수들을 수록하는 데이터 섹션을 포함한다
    - 또한 프로세스는 프로세스 실행 중에 동적으로 할당되는 메모리인 힙을 포함한다

        1.1.1. 프로세스 제어 블록(Process Control Block, PCB)
        - 특정 프로세스에 대한 중요한 정보를 저장하고 있는 운영체제의 자료구조
        - 운영체제는 프로세스를 관리하기 위해 프로세스의 생성과 동시에 고유한 PCB를 생성한다
        - 프로세스는 CPU를 할당받아 작업을 처리하다가도 프로세스 전환이 발생하면 진행하던 작업을 저장하고 CPU를 반환해야 한다, 이때 작업의 진행 상황을 모두 PCB에 저장하게 된다
        - 그리고 다시 CPU를 할당받게 되면 PCB에 저장되어있던 내용을 불러와 이전에 종료됐던 시점부터 다시 작업을 재개한다

        - PCB에 저장되는 정보
            - 프로세스 식별자(Process ID, PID): 프로세스 식별번호
            - 프로세스 상태: new, ready, running, waiting, terminated 등의 상태를 저장
            - 프로그램 카운터: 프로세스가 다음에 실행할 명령어의 주소
            - CPU 레지스터
            - CPU 스케쥴링 정보: 프로세스의 우선순위, 스케줄 큐에 대한 포인터 등
            - 메모리 관리 정보: 페이지 테이블 또는 세그먼트 테이블 등과 같은 정보
            - 입출력 상태 정보: 프로세스에 할당된 입출력 장치들과 열린 파일 목록
            - 어카운팅 정보: 사용된 CPU 시간, 시간제한, 계정번호 등

    1.2. 스레드(Thread)
    - 프로세스의 실행 단위라고 할 수 있다
    - 한 프로세스 내에서 동작되는 여러 실행 흐름으로 프로세스 내의 주소 공간이나 자원을 공유할 수 있다
    - 스레드는 스레드 ID, 프로그램 카운터, 레지스터 집합, 스택으로 구성된다
    - 같은 프로세스에 속한 다른 스레드, 코드, 데이터 섹션, 열린 파일, 신호와 같은 운영체제 자원들을 공유한다
    
    - 멀티스레딩: 하나의 프로세스를 다수의 실행 단위로 구분하여 자원을 공유하고 자원의 생성과 관리의 중복성을 최소화하여 수행 능력을 향상시키는 것
    - 멀티스레딩의 경우 각각의 스레드는 독립적인 작업을 수행해야 하기 때문에 각자의 스택과 PC 레지스터 값을 갖고 있다

        1.2.1. 스택을 스레드마다 독립적으로 할당하는 이유
        - 스택은 함수 호출 시, 전달되는 인자, 되돌아갈 주소값, 함수 내에서 선언하는 변수 등을 저장하기 위해 사용되는 메모리 공간이다
        - 그렇기에 스택 메모리 공간이 독립적이라는 것은 독립적인 함수 호출이 가능하단 것이고 이는 독립적인 실행 흐름이 추가되는 것이다
        - 따라서 스레드의 정의에 따라 독립적인 실행 흐름을 추가하기 위한 최소 조건으로 독립된 스택을 할당한다

        1.2.2. PC register를 스레드마다 독립적으로 할당하는 이유
        - PC값은 스레드가 명령어의 어디까지 수행하였는지 나타내게 된다
        - 스레드는 CPU를 할당받았다가 스케줄러에 의해 다시 선점당한다, 그렇기에 명령어가 연속적으로 수행되지 못하고 어느 부분까지 수행했는지 기억할 필요가 있다
        - 그렇기에 PC 레지스터를 독립적으로 할당한다


2. 멀티 스레드
    2.1. 장점
    - 프로세스를 이용하여 동시에 처리하던 일을 스레드로 구현할 경우, 메모리 공간과 시스템 자원 소모가 줄어들게 된다
    - 스레드 간의 통신이 필요한 경우에도 별도의 자원을 이용하지 않고 전역 변수의 공간 또는 동적으로 할당된 공간인 heap 영역을 이용하여 데이터를 주고 받을 수 있다
    - 그렇기에 프로세스 간, 통신 방법에 비해 스레드 간의 통신 방법이 훨씬 간단하다
    - 심지어 스레드의 context switch는 프로세스 context switch와는 달리 캐시 메모리를 비울 필요가 없어서 더 빠르다
    - 따라서 시스템의 throughput이 향상되고 자원 소모가 줄어들며 자연스럽게 프로그램의 응답 시간이 단축된다

    - 위 같은 장점 때문에 여러 프로세스로 할 수 잇는 작업들을 하나의 프로세스에서 스레드로 나눠 수행한다

    2.2. 문제점
    - 멀티 프로세스 기반의 프로그래밍은 프로세스 간 공유하는 자원이 없기에 동일한 자원에 동시 접근할 일이 없지만 멀티 스레딩을 기반으로 하면 이를 신경써야 한다
    - 서로 다른 스레드가 데이터와 힙 영역을 공유하기에 어떤 스레드가 다른 스레드에서 사용중인 변수나 자료구조에 접근하여 엉뚱한 값을 가져오거나 수정할 수 있다
    
    - 그렇기에 멀티 스레딩 환경에선 동기화 작업이 필요하다
    - 동기화를 통해 작업 처리 순서를 컨트롤하고 공유 자원에 대한 접근을 컨트롤 하는 것이다
    - 허나 이로 인해 병목현상이 발생하여 성능이 저하될 가능성이 있다
    - 그렇기에 과도한 락으로 인한 병목현상을 줄여야 한다

    2.3. 멀티 스레드 vs 멀티 프로세스
    - 동시에 여러 작업을 수행한다는 공통점
    - 시스템에 따라 적합한 동작 방식을 택하여 사용해야 함

    - 멀티 스레드
        - 적은 메모리 공간
        - 문맥 전환이 빠르다
        - 오류로 인해 하나의 스레드가 종료되면 전체 스레드가 종료될 수 있는 동기화 문제

    - 멀티 프로세스
        - 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 끼치지 않음
        - 많은 메모리 공간
        - CPU 시간을 차지함


3. 스케줄러
    3.1. 프로세스를 스케줄링 하기 위한 queue
        - Job queue: 현재 시스템 내에 있는 모든 프로세스의 집합
        - Ready queue: 현재 메모리 내에 있으면서 CPU를 잡아서 실행되기를 기다리는 프로세스의 집합
        - Device queue: Device I/O 작업을 대기하고 있는 프로세스의 집합

    3.2. queue에 프로세스들을 넣고 빼주는 스케줄러
    - 메모리에 프로그램이 너무 많이 올라가있거나 너무 적게 올라가있다면 성능이 좋지 않는 것이다
    - time sharing system에는 장기 스케줄러가 없다, 곧바로 메모리에 올라가 ready 상태가 된다

        3.2.1. 장기 스케줄러(Long-term scheduler or job scheduler)
        - 메모리는 한정되어 있는데 많은 프로세스들이 한꺼번에 메모리에 올라올 경우, 대용량 메모리(일반적으로 디스크)에 임시로 저장
        - 이 pool에 저장되어 있는 프로세스 중 어떤 프로세스에 메모리를 할당하여 ready queue로 보낼지 결정하는 역할

        - 메모리와 디스크 사이의 스케줄링 담당
        - 프로세스에 메모리(및 각종 리소스)를 할당(admit)
        - degree of multiprogramming 제어(실행 중인 프로세스의 수 제어)
        - 프로세스의 상태: new -> ready(in memory)

        3.2.2. 단기 스케줄러(Short-term scheduler or CPU scheduler)
        - CPU와 메모리 사이의 스케줄링을 담당
        - Ready queue에 존재하는 프로세스 중 어떤 프로세스를 running시킬 지 결정
        - 프로세스에 CPU를 할당(scheduler dispatch)
        - 프로세스의 상태: ready -> running -> wating -> ready

        3.2.3. 중기 스케줄러(Medium-term scheduler or Swapper)
        - 여유 공간 마련을 위해 프로세스를 통째로 메모리에서 디스크로 쫓아냄(swapping)
        - 프로세스에게서 메모리를 deallocate
        - degree of multiprogramming 제어
        - 현 시스템에서 메모리에 너무 많은 프로그램이 동시에 올라가는 것을 조절하는 스케줄러
        - 프로세스의 상태: ready -> suspended
            - suspended(stopped)
                - 외부적인 이유로 프로세스의 수행이 정지된 상태
                - 메모리에서 내려간 상태
                - 프로세스 전부 디스크로 swap out 된다
                - blocked 상태는 다른 I/O 작업을 기다리는 상태이기에 스스로 ready state로 돌아갈 수 있다
                - 하지만 suspended상태는 외부적인 이유이기에 스스로 돌아갈 수 없다


4. CPU 스케줄러
- 스케줄링 대상은 Ready queue에 있는 프로세스들이다

    4.1. FCFS(First Come First Served)
        4.1.1 특징
        - 먼저 온 고객을 먼저 서비스해주는 방식
        - 비선점형(non-preemptive) 스케쥴링: 일단 CPU를 잡으면 CPU burst가 완료될 때까지 CPU를 반환하지 않는다
        - 할당되었던 CPU가 반환될 때만 스케줄링이 이루어진다

        4.1.2. 문제점
        - convoy effect: 소요시간이 긴 프로세스가 먼저 도달하여 효율성을 낮추는 현상이 필요하다

    4.2. SJF(Shortest-Job-First)
        4.2.1. 특징
        - 다른 프로세스가 먼저 도착했어도 CPU burst time이 짧은 프로세스에게 선 할당
        - 비선점형 스케줄링

        4.2.2. 문제점
        - starvation
            - 효율성을 추구하야 하는 것이 가장 중요하나 특정 프로세스가 지나치게 차별받으면 안 된다
            - 이 스케줄링은 극단적으로 CPU 사용이 짧은 job을 선호한다
            - 그렇기에 사용 시간이 긴 프로세스는 거의 영원히 CPU를 할당받을 수 없다

    4.3. SRTF(Shortest Remaining Time First)
        4.3.1. 특징
        - 새로운 프로세스가 도착할 때마다 새로운 스케줄링이 이루어진다
        - 선점형(Preemptive) 스케줄링: 현재 수행중인 프로세스의 남은 burst time보다 더 짧은 CPU burst time을 갖는 새로운 프로세스가 도착하면 CPU를 뺏긴다
    
        4.3.2. 문제점
        - starvation
        - 새로운 프로세스가 도달할 때마다 스케줄링을 다시하기 때문에 CPU burst time(CPU 사용시간)을 측정할 수가 없다

    4.4. Priority Scheduling
        4.4.1. 특징
        - 우선순위가 가장 높은 프로세스에게 CPU를 할당하는 스케줄링
            - 우선순위란 정수로 표현하게 되고 작은 숫자가 우선순위가 높다
        - 선점형 스케줄링 방식: 더 높은 우선순위의 프로세스가 도착하면 실행중인 프로세스를 멈추고 CPU를 선점한다
        - 비선점형 스케줄링 방식: 더 높은 우선순위의 프로세스가 도착하면 Ready queue의 heap에 넣는다

        4.4.2. 문제점
        - starvation
        - 무기한 봉쇄(Indefinite blocking): 실행 준비는 되었으나 CPU를 사용못하는 프로세스를 CPU가 무기한 대기하는 상태

        4.4.3. 해결책
        - aging: 아무리 우선순위가 낮은 프로세스라도 오래 기다리면 우선순위를 높여준다

    
    4.5. Round Robin
        4.5.1. 특징
        - 현대적인 CPU 스케줄링
        - 각 프로세는 동일한 크기의 할당 시간(time quantum)을 갖게 된다
        - 할당 시간이 지나면 프로세스는 선점당하고 ready queue의 제일 뒤에 가서 다시 줄을 선다
        - Round Robin은 CPU 사용시간이 랜덤한 프로세스들이 섞여있을 경우 효율적
        - Round Robin이 가능한 이유는 프로세스의 context를 저장할 수 있기 때문

        4.5.2. 장점
        - 응답 시간이 빨라진다
        - n개의 프로세스가 ready queue에 있고 할당시간이 q(time quantum)인 경우 각 프로세스는 q단위로 CPU 시간의 1/n을 얻는다
        - 즉, 어떤 프로세스도 (n-1)q 이상 기다리지 않는다

        - 프로세스가 기다리는 시간이 CPU를 사용할만큼 증가한다
        - 공정한 스케줄링이다

        4.5.3. 주의할 점
        - 설정한 time quantum이 너무 커지면 FCFS와 같아진다
        - 너무 작아지면 스케줄링 알고리즘의 목적에는 이상적이나 잦은 context switch로 overhead가 발생한다
        - 그렇기에 적당한 time quantum을 설정하는 것이 중요하다


5. 동기와 비동기의 차이
    5.1. 비유를 통한 설명
    - 빨래, 설거지, 청소를 해야할 때
    - 동기적으로 처리: 빨래를 하고 설저기를 하고 청소를 한다
    - 비동기적으로 처리: 빨래업체에 빨래시킴, 설거지업체에 설거지시킴, 청소업체에 청소시킴
        - 일을 모두 마친 업체는 알려주기로 했으며 그동안 나는 다른 작업을 할 수 있다(백그라운드 스레드에서 해당 작업을 처리하는 경우의 비동기)
    - 무엇이 먼저 끝날진 알 수 없음

    5.2. Sync vs Async
    - 메소드를 실행시킴과 동시에 반환 값이 기대되는 경우를 동기, 그렇지 않으면 비동기라고 표현한다
    - 이때 동시라는 말은 실행되었을 때 값이 반환되기 전까지는 blocking되어 있다는 것을 의미
    - 비동기의 경우, blocking이 되지 않고 이벤트 큐에 넣거나 백그라운드 스레드에게 해당 task를 위임하고 바로 다음 코드를 실행하기 때문에 기대되는 값이 바로 반환되지 않는다


6. 프로세스 동기화
    6.1. Critical Section(임계영역)
    - 멀티스레딩에 문제점에서 나오듯, 동일한 자원을 동시에 접근하는 작업(공유하는 변수 사용, 동일 파일을 사용하는 등)을 실행하는 코드 영역을 임계영역이라고 한다
    
    6.2. Critical Section Problem(임계영역 문제)
    - 프로세스들이 임계영역을 함께 사용할 수 있는 프로토콜을 설계하는 것

        6.2.1. 해결을 위한 전제조건(requirements)
        - 상호 배제(Mutual Exclusion): 프로세스 P1이 임계영역에서 실행중이라면 다른 프로세스들은 그들이 가진 임계영역에서 실행될 수 없다
        - 진행(Progress): 임계영역에서 실행중인 프로세스가 없고 별도의 동작이 없는 프로세스들만 임계영역 진입 후보로서 참여될 수 있다
        - 한정된 대기(Bounded Waiting): P1이 임계영역에 진입 신청 후 부터 받아들여질 때까지 다른 프로세스들이 임계영역에 진입하는 횟수는 제한이 있어야 한다

    6.3 해결책
        6.3.1. Lock
        - 하드웨어 기반 해결책
        - 동시에 공유 자원에 접근하는 것을 막기 위해 임계영역에 진입하는 프로세스들은 Lock을 획득하고 임계영역을 빠져나올 때, Lock을 방출함으로써 동시에 접근이 되지 않도록 한다
        
        - 한계: 다중처리기 환경에서는 시간적인 효율성 측면에서 적용할 수 없다

        6.3.2. 세마포(Semaphores)
        - 소프트웨어상에서 임계영역 문제를 해결하기 위한 동기화 도구

            6.3.2.1. 종류
            - 카운팅 세마포
                - 가용한 개수를 가진 자원에 대한 접근 제어용으로 사용
                - 세마포는 가용한 자원의 개수로 초기화
                - 자원을 사용하면 세마포가 감소, 방출하면 세마포가 증가한다
            
            - 이진 세마포
                - MUTEX라고도 함(상호배제(Mutual Exclusion)의 머릿글자를 따옴)
                - 0과 1사이의 값만 가능
                - 다중 프로세스들 사이의 임계영역 문제를 해결하기 위해 사용

            6.3.2.2. 단점
            - 바쁜 대기(Busy Waiting)
                - spin lock이라고 불리는 세마포 초기 버전에서 임계영역에 진입해야하는 프로세스는 진입 코드를 계속 반복 실행하며 CPU 시간을 낭비하는 것
                - 이는 특수한 상황이 아니면 비효율적이다
                - 이를 해결하기 위해 일반적으론 세마포에서 임계영역에 진입을 시도했지만 실패한 프로세스에 대해 block시킨 뒤, 임계영역에 자리가 날 때 다시 깨우는 방식을 사용
            
            - 교착상태(Deadlock)
                - 세마포가 ready queue를 가지고 있고
                - 둘 이상의 프로세스가 임계영역 진입을 무한정 기다리고 있으며
                - 임계영역에서 실행되는 프로세스는 진입 대기 중인 프로세스가 실행되어야만 빠져나올 수 있는 상황
    
        6.3.3. 모니터
        - 고급 언어의 설계 구조물
        - 개발자의 코드를 상호배제하여 만든 추상화된 데이터 형태
        - 공유자원에 접근하기 위한 키 획득과 자원 사용 후 해제를 모두 처리한다
            - 세마포어는 직접 키 해제와 공유자원 접근 처리가 필요하다


7. 메몰 관리 전략
    7.1. 메모리 관리 배경
    - 각각의 프로세스는 독립된 메모리 공간을 갖고 운영체제 혹은 다른 프로세스의 메모리 공간에 접근할 수 없는 제한이 걸려있다
    - 단지 운영체제 만이 운영체제 메모리 영역과 사용자 메모리 영역의 접근제약이 없다

    - Swapping
        - 메모리의 관리를 위해 사용되는 기법
        - 표준 swapping 방식으로는 Round-Robin과 같은 스케줄링의 다중 프로그래밍 환경에서 CPU 할당 시간이 끝난 프로세스의 메모리를 보조 기억장치로 내보내고 다른 프로세스의 메모리를 불러들일 수 있다
        - swap-in: 주 기억장치(RAM)으로 불러오는 과정
        - swap_out: 보조 기억장치(하드디스크)로 내보내는 과정
        - swap에는 큰 디스크 전송시간이 필요하기에 현재는 메모리 공간이 부족할 때 swapping이 진행된다

    - 단편화(Fragmentation)
        - 프로세스들이 메모리에 적재되고 제거되는 일이 반복되면 프로세스들이 차지하는 메모리 틈 사이에 사용하지 못할 만큼의 작은 자유공간이 늘어나는 것
        - 외부 단편화
            - 메모리 공간 중 사용하지 못하게 되는 일부분
            - 물리 메모리(RAM)에서 사이사이 남는 공간들을 모두 합치면 충분한 공간이 되는 부분들이 분산되어 있을 때 발생
        - 내부 단편화
            - 프로세스가 사용하는 메모리 공간에 포함된 남는 부분
            - 예시: 메모리 분할 자유 공간이 10,000b있고 Process A가 9,998b 사용하게 되면 2b라는 차이가 존재하고 이 현상

    - 압축
        - 외부 단편화를 해소하기 위해 프로세스가 사용하는 공간들을 한쪽으로 몰아 자유공간을 확보하는 방법론
        - 그러나 효율이 좋지 않다
    
    7.2. 페이징(Paging)
    - 하나의 프로세스가 사용하는 메모리 공간이 연속적이어야 한다는 제약을 없애는 메모리 관리 방법
    - 외부 단편화와 압축 작업을 해소 하기 위해 생긴 방법론
    - 물리 메모리는 frame이라는 고정 크기로, 논리 메모리(프로세스가 점유하는)는 페이지라 불리는 고정 크기의 블록으로 분리된다
        - 페이지는 페이지 교체 알고리즘에 들어가는 페이지이다

    - 페이징 기법을 사용함으로 논리 메모리는 물리 메모리에 저장될 때, 연속되어 저장될 필요가 없고 물리 메모리의 남는 프레임에 적절히 배치됨으로 외부 단편화를 해결
    - 하나의 프로세스가 사용하는 공간은 여러개의 페이지로 나뉘어서 관리되고(논리 메모리에서), 개별 페이지는 순서 상관없이 물리 메모리에 있는 프레임에 mapping되어 저장된다

    - 단점
        - 내부 단편화 문제의 비중이 높아진다
        - 예시
            - 페이지 크기가 1,024b이고 프로세스 A가 3,172b의 메모리를 요구할 때
            - 3개의 페이지 프레임(1,024 * 3 = 3,072)하고도 100b가 남기때문에 총 4개의 페이지 프레임이 필요하다
            - 그렇기에 4번째 페이지 프레임에는 924b(1,024 - 100)의 여유공간이 남게 되어 내부 단편화 문제가 발생한다

    7.3. 세그멘테이션(Segmentation)
    - 페이징에서처럼 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌 서로 다른 크기의 논리적 단위인 세그먼트(Segment)로 분할
    - 사용자가 두 개의 주소로 지정(세그먼트 번호 + 변위)
    - 세그먼트 테이블에는 각 세그먼트의 기준(세그먼트의 시작 물리 주소)과 한계(세그먼트의 길이)를 저장

    - 단점: 서로 다른 크기의 세그먼트들이 메모리에 적재되고 제거되는 일이 반복되다 보면, 자유 공간들이 많은 수의 작은 조각들로 나누어져 못쓰게 될 수도 있다(외부 단편화)


8. 가상 메모리
- 다중 프로그래밍을 실현하기 위해선 많은 프로세스들을 동시에 메모리에 올려두어야 한다
- 가상메모리는 프로세스 전체가 메모리 내에 올라오지 않더라도 실행이 가능하도록 하는 기법
- 프로그램이 물리 메모리보다 커도 된다는 장점이 있음

    8.1. 개발 배경
    - 실행되는 코드의 전부를 물리 메모리에 존재시켜야 했고 메모리 용량보다는 큰 프로그램은 실행시킬 수 없었다
    - 그리고 여러 프로그램을 동시에 메모리에 올리기엔 용량의 한계와 페이지 교체 등의 성능 이슈가 발생했다
    - 또한 가끔만 사용되는 코드가 차지하는 메모리들을 확인할 수 있다는 점에서 불필요하게 전체의 프로그램이 메모리에 올라와 있어야 하는게 아니란 것을 알 수 있다

    - 만약 프로그램의 일부분만 메모리에 올릴 수 있다면
        - 물리 메모리 크기 제약이 없어진다
        - 더 많은 프로그램을 동시에 실행할 수 있게 된다
        - 이에 따라 응답시간은 유지되고 CPU 이용률과 처리율은 높아진다
        - swap에 필요한 입출력이 줄어들기 때문에 프로그램이 빠르게 실행된다
    
    8.2. 가상 메모리가 하는 일
    - 가상 메모리는 실제의 물리 메모리 개념과 사용자의 논리 메모리 개념을 분리한 것으로 정리할 수 있다
    - 이로써 작은 메모리를 가지고도 얼마든지 큰 가상 주소 공간을 프로그래머에게 제공할 수 있다

        8.2.1. 가상 주소 공간
        - 한 프로세스가 메모리에 저장되는 논리적인 모습을 가상메모리에 구현한 공간
        - 프로세스가 요구하는 메모리 공간을 가상메모리에 제공함으로서 현재 직접적으로 필요치 않은 메모리 공간은 실제 물리 메모리에 올리지 않는 것으로 물리 메모리를 절약할 수 있다
        -예시
            - 한 프로그램이 실행되며 논리 메모리로 100kb가 요구되었다
            - 하지만 실행까지 필요한 메모리 공간(heap 영역, stack 영역, 코드, 데이터)의 합이 40kb라면 실제 물리 메모리에는 40kb만 올라가 있다
            - 그리고 나머지 60kb만큼은 필요시에 물리 메모리에 요구하는 것이다
        
        8.2.2. 프로세스간의 페이지 공유
        - 가상메모리는 시스템 라이브러리가 여러 프로세스들 사이에 공유될 수 있도록 한다
        - 각 프로세스들은 공유 라이브러리를 자신의 가상 주소 공간에 두고 사용하는 것처럼 인식한다
        - 하지만 라이브러리가 올라가있는 물리 메모리 페이지들은 모든 프로세스에 공유되고 있다

        - 프로세스들이 메모리를 공유하는 것을 가능하게 하고 프로세스들은 공유 메모리를 통해 통신할 수 있다
        - 이또한 각 프로세스들은 각자 자신의 주소 공간처럼 인식하지만 실제 물리 메모리는 공유되고 있다

        - fork()를 통한 프로세스 생성 과정에서 페이지들이 공유되는 것을 가능하게 한다

    8.3. 요구 페이징(Demand Paging)
    - 프로그램 실행 시작 시에 프로그램 전체를 디스크에서 물리 메모리에 적재하는 대신, 초기에 필요한 것들만 적재하는 전략
    - 가상 메모리 시스템에서 많이 사용됨
    - 가상 메모리는 대개 페이지로 관리되며 요구 페이징을 사용하는 가상 메모리에서는 실행과정에서 필요해질 때 페이지들이 적재된다
    - 한 번도 접근되지 않은 페이지는 물리 메모리에 적재되지 않는다

    - 프로세스 내의 개별 페이지들은 페이저(pager)에 의해 관리된다
    - 페이저는 프로세스 실행에 실제 필요한 페이지들만 메모리로 읽어와 사용되지 않을 페이지를 가져오는 시간낭비와 메모리 낭비를 줄일 수 있다

    8.4. 페이지 교체
    - 프로그램 실행 시, 모든 항목이 물리 메모리에 올라오지 않기에 프로세스의 동작에 필요한 페이지를 요청하는 과정에서 페이지 부재(page fault)가 발생하게 되면 원하는 페이지를 보조저장장치에서 가져오게 된다
    - 하지만 만약 물리 메모리가 모두 사용중인 상황이라면 페이지 교체나 운영체제가 프로세스를 강제 종료시켜야 한다

        8.4.1. 기본적인 방법
        - 물리 메모리가 모두 사용중인 상황에서 메모리 교체 흐름이다
        
        - 디스크에서 필요한 페이지 위치를 찾는다
        - 빈 페이지 프레임을 찾는다
            - 페이지 교체 알고리즘을 통해 교체될 페이지(victim)를 고른다
            - 교체될 페이지를 디스크에 기록하고 관련 페이지 테이블을 수정한다
        - 새롭게 비워진 페이지 테이블 내 프레임에 새 페이지를 읽어오고, 프레임 테이블을 수정한다
        - 사용자 프로세스 재시작

        8.4.2. 페이지 교체 알고리즘
            8.4.2.1. FIFO 페이지 교체
            - 가장 간단한 페이지 교체 알고리즘
            - 먼저 물리 메모리에 들어온 페이지 순서대로 페이지 교체 시점에 먼저 나가게 되는 것
            - 장점
                - 이해하기 쉽다
                - 프로그램하기도 쉽다
            - 단점
                - 오래된 페이지가 필요한 정보를 가질 수 있다(초가 변수 등)
                - 처음부터 활발하게 사용되는 페이지를 교체해서 페이지 부재율을 높이는 부작용이 생길 수 있다
                - Belady의 모순: 페이지를 저장할 수 있는 페이지 프레임의 개수를 늘려도 되려 페이지 부재가 더 많이 발생하는 모순이 존재
            
            8.4.2.2. 최적 페이지 교체(Optimal Page Replacement)
            - Belady의 모순을 확인한 이후, 최적 교체 알고리즘에 대한 탐구가 진행
            - 모든 알고리즘보다 낮은 페이지 부재율을 보이며 Belady의 모순이 생기지 않는다
            - 핵심은 앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체하는 것
            - 주로 비교 연구 목적을 위해 사용됨

            - 장점: 알고리즘 중 가장 낮은 페이지 부재율을 보장한다
            - 단점: 구현의 어려움이 있다, 모든 프로세스의 메모리 참조 계획을 미리 파악할 방법이 없기 때문

            8.4.2.3. LRU 페이지 교체(LRU Page Replacement)
            - LRU: Least Recently Used
            - 최적 알고리즘의 근사 알고리즘으로 가장 오랫동안 사용되지 않은 페이지를 선택하여 교체한다
            - 대체적으로 FIFO 알고리즘보다 우수하고 OPT알고리즘보단 좋지 않다

            8.4.2.4. LFU 페이지 교체(LFU Page Replacement)
            - LFU: Least Frequently Used
            - 참조 횟수가 가장 적은 페이지를 교체하는 방법
            - 활발하게 사용되는 페이지는 참조 횟수가 많아질 거라는 가정에서 만들어진 알고리즘
            - 어떤 프로세스가 특정페이지를 집중적으로 사용하다 다른 기능을 사용하게되면 더 이상 사용하지 않아도 메모리에 머물게 되어 초기 가정이 어긋날 수 있다
            - OPT 알고리즘을 제대로 근사하지 못하기에 잘 쓰이지 않는다

            8.4.2.5. MFU 페이지 교체(MFU Page Replacement)
            - MFU: Most Frequently Used
            - 참조 횟수가 가장 작은 페이지가 최근에 메모리에 올라왔고 앞으로 계속 사용될 것이라는 가정에 기반한다
            - OPT알고리즘을 제대로 근사하지 못해 잘 쓰이지 않는다


9. 캐시의 지역성
    9.1. 캐시의 지역성 원리
    - 캐시 메모리는 속도가 빠른 장치와 느린 장치 간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리다
    - 이 역할을 수행하기 위해선 CPU가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다
    - 캐시의 성능은 작은 용량의 캐시 메모리에 CPU가 이후에 참조할 쓸모 있는 정보가 어느정도 있는지에 달렸기 때문이다

    - 이때 적중률(Hit rate)을 극대화시키기위해 데이터 지역성(Locality)의 원리를 사용한다
    - 지역성의 전제조건으로 프로그램은 모든 코드나 데이터를 균등하게 access하지 않는다는 특성을 기본으로 한다
    - 즉, Locality란 기억 장치 내의 정보를 균일하게 access하는 것이 아닌 어느 한 순간에 특정 부분을 집중적으로 참조하는 특성인 것이다
    - 데이터 지역성은 대표적으로 두 개로 나뉜다
        - 시간 지역성(Temporal Locality): 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성
        - 공간 지역성(Spatial Locality): 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성
    
    9.2. Caching line
    - 캐시는 프로세서 가까이 위치하며 빈번하게 사용되는 데이터를 놔두는 장소이다
    - 그러나 캐시가 아무리 가까이 있어도 찾고자 하는 데이터가 어디에 있는지 몰라 데이터를 순회한다면 캐시 목적이 희미해진다
    - 그렇기에 캐시의 목적을 유의미하게 만드려면 데이터가 저장된 지점으로 바로 접근하여 출력할 수 있어야 한다

    - 그렇기 때문에 캐시에 데이터를 저장할 때, 특정 자료구조를 사용하여 묶음으로 저장하게 되고 이를 캐싱 라인이라 한다
    - 프로세스는 다양한 주소에 있는 데이터를 사용하므로 빈번하게 사용하는 데이터의 주소 또한 흩어져 있다
    - 따라서 캐시에 저장하는 데이터에는 데이터의 메모리 주소 등을 기록해 둔 태그를 달아놓아야 한다
    - 이 태그들의 묶음이 캐싱 라인이 되며 메모리에서 가져올 때도 캐싱 라인을 기준으로 가져온다

    - 종류는 대표적으로 세 가지 방식이 존재한다
        - Full Associative
        - Set Associative
        - Direct Map