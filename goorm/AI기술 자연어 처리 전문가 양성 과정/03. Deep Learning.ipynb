{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Neural Networks\n",
    "2. Convolution Neural Network\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Training Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Sigmoid activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](_image/sigmoid.png)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "앞서 배웠듯이 0 ~ 1 사이 값을 가지는 함수입니다. 그러나 Neural Networks에서 사용하기엔 많은 문제점이 있습니다.\n",
    "\n",
    "- 기울기 소실\n",
    "- output의 중간값이 0이 아니다.\n",
    "- exp()의 연산이 너무 무겁다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Softmax activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_i = \\frac{e^{z_i}}{\\sum_{j = 1}^k e^{z_j}}$$\n",
    "\n",
    "sigmoid가 두 개의 값으로 분류한다면 softmax는 여러 가지 값으로 분류한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) tanh activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh](_image/tanh.png)\n",
    "\n",
    "$$tanh(x) = 2 \\times sigmoid(x) - 1$$\n",
    "\n",
    "-1 ~ 1의 범위를 갖고 있습니다. sigmoid의 문제점 중 하나인 output의 중간값을 0으로 만들었습니다. 그러나 기울기 소실 문제는 여전히 남아있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) ReLU(Rectified Linear Unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](_image/ReLU.png)\n",
    "\n",
    "$$computes \\; f(x) = max(0, x)$$\n",
    "\n",
    "학습이 굉장히 빠르고 기울기 소실 문제가 사라집니다. output의 중간값이 0은 아니지만 많이 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid나 tanh에서 중요한 것 중 하나는 기울기가 살아있는 범위에 input 데이터가 들어가야 유의미한 값이 나온다는 것입니다. 그걸 위해 사용하는 방법이 **Batch Normalization** 입니다. 이를 통해 기울기 소실 문제를 해결할 수 있습니다. \n",
    "\n",
    "$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)}]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$\n",
    "\n",
    "위 식을 통해 평균 0, 분산 1의 input data가 만들어집니다. \n",
    "\n",
    "그 후, Neural Network가 데이터에 말맞게 평균과 분산을 조절합니다.\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\n",
    "\n",
    "이를 통해 $\\gamma$는 표준편차를, $\\beta$는 평균을 학습시킨다. 이렇게 $\\gamma , \\beta$의 최적값을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization을 위한 방법은 여러가지가 있습니다. 왜냐하면 gradient descent를 그대로 사용하면 변동폭이 너무 크게 일어나기 때문입니다. 이제 하나씩 살펴보겠습니다.\n",
    "\n",
    "여러 방식들이 어떻게 이루어지는지 gif로 보려면 다음 링크로 접속하여 확인하면 됩니다.\n",
    "http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 5 # input data\n",
    "dx = 2 # gradient\n",
    "learning_rate = 0.1 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Gradient Descent(SGD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sgd](_image/sgd.png)\n",
    "\n",
    "일반적인 방법은 현재 자신의 위치에서 측정한 경사에 따라 움직입니다. 그렇기에 위 그림처럼 진동이 엄청나게 일어납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla gradient descent update\n",
    "x -= learning_rate * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Momentum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![momentum](_image/momentum.png)\n",
    "\n",
    "Momentum은 현재 위치에서 측정한 기울기만 사용하지 않습니다. 전에 지나온 기울기들을 계속해서 합산하여 사용합니다. 그렇기 때문에 진동이 큰 방향은 진동 크기를 줄이고 맞는 방향은 더욱 빠르게 가도록 해줍니다. \n",
    "\n",
    "기울기를 합산할 때, 예전 기울기의 영향력을 줄이기 위해서 0.8 등의 공비를 계속해서 곱해줍니다. \n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1000/1*X9SaxFM6_sBOAMY9TaGsKw.png\">\n",
    "\n",
    "이름이 momentum이듯, 관성처럼 local minimum에 머물지 않고 global minimum으로 가도록 만들어줍니다.\n",
    "\n",
    "이를 코드로 보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum update\n",
    "mu = 0.8 # 공비\n",
    "v = 0\n",
    "\n",
    "v = mu * v - learning_rate * dx\n",
    "x += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Adagrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기들을 합산할 때, 기울기의 크기가 많이 차이나지 않도록 기울기를 제곱해서 합을 구한다음 루트를 취하여 기울기에 나눠줍니다. 코드를 통해 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad update\n",
    "cache = 0\n",
    "\n",
    "cache += dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 1e-7을 더해주는 이유는 cache가 너무 작아져서 0이 되어 error가 나는 것을 방지하기 위해서입니다.\n",
    "\n",
    "Adagrad는 방향을 잘 찾아가지만 목표에 도달할수록 나누는 값이 너무 커집니다. 그렇기에 한 번의 가는 거리가 점점 작아지고 속도가 느려지게 됩니다.\n",
    "\n",
    "그렇기에 Adagrad를 직접 사용하지 않고 이를 활용하는 방법들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) RMSProp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad에서 decay_rate를추가하여 오래된 기울기들의 영향력을 제거하는 방식을 도입한 것이 RMSProp입니다. 코드로 구현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.1\n",
    "\n",
    "# RMSProp update\n",
    "cache += decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp와 Momentum 방식을 합쳐서 사용하는 방법이 Adam입니다. 기본적으로 beta1 = 0.9, beta2 = 0.999, eps = 1e-8을 추천됩니다. 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam upgrade\n",
    "m, v = 0, 0\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "m = beta1 * m + (1 - beta1) * dx\n",
    "v = beta2 * v + (1 - beta2) * (dx**2)\n",
    "x -= learning_rate * m / (np.sqrt(v) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate는 gradient descent에서 움직이는 보폭 비율이라고 생각하면 됩니다. 그렇기에 learning rate가 크면 크게크게 학습하고 작으면 차근차근 학습하게 됩니다. learning rate의 크기에 따라 학습 정도를 살펴보면 다음과 같습니다.\n",
    "\n",
    "![learning_rate](_image/learning_rate.png)\n",
    "\n",
    "하나씩 살펴보면 먼저 작은 값을 가지면 학습속도가 매우 느립니다. 만약 큰 값을 가진다면 학습속도가 빠르지만 어느 순간부터 학습이 진행되지 않습니다. 왜냐하면 현재 위치와 목표점의 거리보다 보폭이 더 크기 때문입니다. 그리고 만약 매우 큰 값을 가진다면 학습이 될 수 있지만 금방 발산해버립니다.\n",
    "\n",
    "그렇기 때문에 epoch가 진행될수록 learning rate를 줄이는 방법을 사용합니다.\n",
    "\n",
    "\n",
    "![learning_rate2](_image/learning_rate2.png)\n",
    "\n",
    "위 그림처럼 loss가 줄지 않을 때마다 learning rate를 줄여서 학습을 진행합니다. \n",
    "\n",
    "learning rate를 늘리거나 줄일 땐, 0.1, 1, 10, 100,... 등 10배씩 키우거나 줄이는 것이 일반적입니다. 또는 $\\sqrt{10} \\sim 3$을 이용해 3배씩 키우거나 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble](_image/ensemble.png)\n",
    "\n",
    "\n",
    "위 그림처럼 여러 가지 모델들의 결과를 합쳐서 하나의 평균값으로 예측하는 것을 Ensemble이라고 합니다. Ensemble은 대체적으로 2 ~ 3% 정도 정답률이 증가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) L1, L2 Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L1 : \\sum_k \\sum_l \\lvert W_{k, l} \\rvert \\quad (W : \\text{weight decay})$$\n",
    "$$L2 : \\sum_k \\sum_l (W_{k, l})^2$$\n",
    "$$Elastic net(\\text{L1 + L2}) : \\sum_k \\sum_l (\\beta W_{k, l}^2 + \\lvert W_{k, l} \\rvert)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 모델에 여러 뉴런이 있습니다. 이 중 임의로 몇 개의 뉴런을 무시하여 다양한 모델들을 사용하는 결과를 주는, 즉 ensemble 효과를 주는 방법을 Dropout이라고 합니다. \n",
    "\n",
    "![dropout](_image/dropout.png)\n",
    "\n",
    "위 그림처럼 임의의 뉴런을 무시하고 진행하는 방식입니다. 위 방식을 여러번 사용하여 ensemble처럼 모든 모델들의 예측값의 평균을 내서 결과를 예측합니다.\n",
    "\n",
    "주의할 점은 Dropout했던 모델을 테스트할 때는 모든 노드를 사용한다는 것입니다. 그렇기에 output이 학습할 때보다 크게 나오게 됩니다. 이를 방지하기 위해 학습 때 사용한 노드의 비중만큼만 output에서 가져옵니다.\n",
    "\n",
    "예를 들어 70%의 뉴런만 사용하여 학습을 진행했다면 실제 테스트할 때도 ouput의 70%를 실제 예측값으로 사용하게 됩니다.\n",
    "\n",
    "이를 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5 # probability of keeping a unit active, higher = less dropout\n",
    "\n",
    "\n",
    "def train_step(X, W, b):\n",
    "    \"\"\"X contains the data\"\"\"\n",
    "    \n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0])\n",
    "    U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "    H1 *= U1 # drop\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1])\n",
    "    U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "    H2 *= U2 # drop\n",
    "    out = np.dot(W[2], H2) + b[2]\n",
    "    \n",
    "    # backward pass: compute gradients...(not shown)\n",
    "    # perform parameter update...(not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    # ensemble forward pass\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0]) * p # scale the activations\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1]) * p # scale the activations\n",
    "    out = np.dot(W[2], H2) + b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가진 데이터들이 적을 때나 더 많이 필요할 때, 데이터들을 변형, 회전, 늘림 등의 과정을 통해 데이터를 늘리는 것을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Tensor operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 배열이나 행렬과 매우 유사한 자료구조입니다. PyTorch에서는 텐서를 사용하여 모델의 입력과 출력뿐만 아니라 모델의 파라미터를 나타냅니다.\n",
    "\n",
    "GPU나 다른 연산 가속을 위한 특수한 하드웨어에서 실행할 수 있다는 점을 제외하면, 텐서는 NumPy의 ndarray와 매우 유사합니다.\n",
    "\n",
    "이제 텐서에 대한 다양한 구현을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터로부터 직접 생성하기\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array로부터 생성하기\n",
    "np_array = np.array(data)\n",
    "x = torch.from_numpy(np_array)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor에서 numpy array로 변환하기\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0851, 0.0584],\n",
      "        [0.3296, 0.2676]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 텐서와 같은 모양의 텐서 초기화하기\n",
    "x_ones = torch.ones_like(x) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.4402, 0.1129, 0.1370, 0.3384],\n",
      "        [0.3500, 0.0489, 0.0349, 0.6645],\n",
      "        [0.6792, 0.5942, 0.9422, 0.8516]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 주어진 shape로 초기화하기\n",
    "shape = (3,4)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 속성은 텐서의 모양, 자료형 및 어느 장치에 저장되는지를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 노트북은 gpu가 없다\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "#tensor = tensor.to(device)\n",
    "#print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 텐서간의 연산도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 식의 인덱싱과 슬라이싱\n",
    "tensor = torch.ones(3, 4)\n",
    "tensor[:, 1] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 행으로 길어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 열로 이어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 곱하기\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산합니다\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "\n",
    "# 다른 문법:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 간 행렬 곱셈\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# 다른 문법:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Autograd**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. autograd를 통해 입력 X, 파라미터 W , 그리고 cross-entropy loss를 사용하는 logistic regression model의 gradient를 구하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[ 0.6696, -1.1055,  0.7509],\n",
      "        [ 0.2827,  0.7063,  1.1959],\n",
      "        [ 0.4497,  0.8117,  0.0219],\n",
      "        [ 0.3535,  0.1437,  1.7595],\n",
      "        [-0.3871,  0.4001, -1.1086]], requires_grad=True)\n",
      "tensor([-1.2913,  0.2003,  0.2606], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 입력 및 파라미터 초기화\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0770, 1.1566, 2.8803], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward\n",
    "z = torch.matmul(x,w)+b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 node를 크게 2가지 방법의 api를 활용해서 사용합니다.\n",
    "\n",
    "1. torch.nn\n",
    "2. torch.nn.functional\n",
    "\n",
    "torch.nn은 사전에 node를 초기화하고 해당 node에 텐서를 통과시켜 값을 받는 형태지만, torch.nn.functional은 사전에 초기화없이 바로 함수처럼 사용하는 방식입니다.\n",
    "\n",
    "코딩 스타일에 맞춰서 원하시는 api를 사용하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비용 함수\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에서 매개변수의 가중치를 최적화하려면 파라미터에 대한 loss function의 도함수(derivative)를 계산해야 합니다. \n",
    "이러한 도함수를 계산하기 위해, loss.backward() 를 호출한 다음 w.grad와 b.grad에서 값을 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156]])\n",
      "tensor([0.1731, 0.2536, 0.3156])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로, requires_grad=True인 모든 텐서들은 연산 기록을 추적하고 미분 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 forward 연산만 필요한 경우에는, 미분 연산을 위한 값들을 저장해두는 것이 속력 및 메모리의 저하를 가져올 수 있습니다. 연산 코드를 torch.no_grad() 블록으로 둘러싸서 미분 추적을 멈출 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. LR vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 MNIS dataset을 활용하여 logistic regression model과 MLP model을 구현해보고 학습 파이프라인을 익혀보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Preprocess Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist에 존재하는 각각의 사진은 28*28의 픽셀로 구성된 784차원짜리 벡터로 나타나져 있습니다. 각 픽셀은 0~255 사이의 값으로 흰색부터 검은색 사이의 값을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "X = X.values\n",
    "y = y.values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# scale\n",
    "X /= 255.0\n",
    "print(X.min(), X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(7000, 784)\n",
      "(7000,)\n",
      "(7000, 784)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "print(X_train.shape) # 80%\n",
    "print(y_train.shape)\n",
    "print(X_val.shape) # 10%\n",
    "print(y_val.shape)\n",
    "print(X_test.shape) # 10%\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dataset\n",
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)\n",
    "        \n",
    "\n",
    "plot_example(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) PyTorch Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 Custom Dataset을 사용하기 위해서는 torch.utils.data.Dataset의 형태로 dataset class를 정의해준 이후, torch.utils.data.DataLoader의 형태로 dataloader class를 정의하여 학습시에 model에 forwarding할 data를 sample해줍니다.\n",
    "\n",
    "(https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "\n",
    "\n",
    "가장 보편적으로 사용되는 map-style의 dataset class는 torch.utils.data.Dataset을 superclass로 받아 **getitem()** 과 **len()** 함수를 override해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "7000\n",
      "(7000, 784)\n",
      "7000\n",
      "(7000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(val_dataset))\n",
    "print(val_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader는 train 혹은 validation시 dataset에서 batch를 sampling하기 위한 API입니다 (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "필수적으로 사용하는 option들은 아래와 같습니다.\n",
    "- dataset: sampling할 dataset\n",
    "- batch_size: 한번에 sampling할 dataset의 개수\n",
    "- shuffle: 1 epoch를 기준으로 dataset을 shuffle할지\n",
    "\n",
    "더 자세한 option은 api를 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "110\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch에서 model을 선언할 때는 torch.nn.Module class를 superclass로 받아 __init__()함수와 forward() 함수를 작성해줍니다.\n",
    "\n",
    "__init__()함수에는 모델의 파라미터들을 선언하고, forward함수에는 해당 파라미터들을 이용하여 data를 model에 통과시켜줍니다.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression Model\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LR, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP Model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 선언한 model을 통해 학습을 진행하기 위해선 파라미터를 최적화할 optimizer가 필요합니다. 이번 실습에선 가장 보편적으로 사용되는 Adam optimizer를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, valloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        # 학습을 시작하기 위해 model을 train-mode로 변경\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                # optimizer는 예전 기울기도 계속 저장하기에 기울기를 초기화해준다.\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                # get output after passing through the network\n",
    "                outputs = self.model(inputs) \n",
    "                # compute model's score using the loss function\n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                # perform back-propagation from the loss\n",
    "                loss.backward() \n",
    "                # gradient descent를 통해 model의 output을 얻는다.\n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        val_acc = self.validate()\n",
    "        return val_acc\n",
    "\n",
    "    def validate(self):\n",
    "        # 현재 model이 train-mode일 수 있기에 eval-mode로 바꿔 validate를 수행할 수 있도록 변경\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.valloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.valloader.dataset)\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.testloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.166\n",
      "epoch: 2  loss: 1.194\n",
      "epoch: 3  loss: 1.261\n",
      "epoch: 4  loss: 1.297\n",
      "val_acc: 0.886\n",
      "epoch: 1  loss: 0.356\n",
      "epoch: 2  loss: 0.305\n",
      "epoch: 3  loss: 0.296\n",
      "epoch: 4  loss: 0.294\n",
      "val_acc: 0.919\n",
      "epoch: 1  loss: 0.555\n",
      "epoch: 2  loss: 0.325\n",
      "epoch: 3  loss: 0.295\n",
      "epoch: 4  loss: 0.281\n",
      "val_acc: 0.924\n",
      "epoch: 1  loss: 1.387\n",
      "epoch: 2  loss: 0.735\n",
      "epoch: 3  loss: 0.557\n",
      "epoch: 4  loss: 0.474\n",
      "val_acc: 0.893\n",
      "test_acc: 0.919\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = LR(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_logistic_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_logistic_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.930\n",
      "epoch: 2  loss: 0.956\n",
      "epoch: 3  loss: 1.004\n",
      "epoch: 4  loss: 1.003\n",
      "val_acc: 0.678\n",
      "epoch: 1  loss: 0.278\n",
      "epoch: 2  loss: 0.170\n",
      "epoch: 3  loss: 0.148\n",
      "epoch: 4  loss: 0.133\n",
      "val_acc: 0.956\n",
      "epoch: 1  loss: 0.472\n",
      "epoch: 2  loss: 0.254\n",
      "epoch: 3  loss: 0.206\n",
      "epoch: 4  loss: 0.173\n",
      "val_acc: 0.950\n",
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.553\n",
      "epoch: 3  loss: 0.414\n",
      "epoch: 4  loss: 0.358\n",
      "val_acc: 0.911\n",
      "test_acc: 0.953\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = MLP(input_dim=input_dim, \n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_MLP_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_MLP_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습3. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 activation function 중 가장 대표적으로 사용되는 sigmoid functio과 ReLU function을 사용해보고 비교해보겠습니다. 데이터는 실습 2에서 사용했던 MNIST를 사용합니다. 모델은 train과 test만 사용하겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfJBd9v9L_RgXGf8urNrYpb40zXU6gea)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- **activation: sigmoid or relu**\n",
    "- optimizer: sgd\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Prerequisite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oCnmrA9ltYs0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "X = mnist.data.astype('float32').values\n",
    "y = mnist.target.astype('int64').values\n",
    "X /= 255.0\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB-u3e9taDjT"
   },
   "source": [
    "#### **Split Dataset**\n",
    "\n",
    "학습과 평가를 위한 dataset으로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-HWWRcNjaLDi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(14000, 784)\n",
      "(14000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYnvqbdijWUQ"
   },
   "source": [
    "#### **Pytorch Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ypqp7zA-xRlB"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hTr4OWatzmaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "14000\n",
      "(14000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51PT-uPVzE8_"
   },
   "source": [
    "#### **DataLoader**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x2k3YVBoxRnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN65oTBk1d4T"
   },
   "source": [
    "#### **Trainer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OJqIwSltn9uY"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs) \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                loss.backward() \n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_acc = correct / len(self.testloader.dataset)\n",
    "        print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 2-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.187\n",
      "epoch: 2  loss: 1.817\n",
      "epoch: 3  loss: 1.386\n",
      "epoch: 4  loss: 1.079\n",
      "epoch: 5  loss: 0.882\n",
      "epoch: 6  loss: 0.754\n",
      "epoch: 7  loss: 0.666\n",
      "epoch: 8  loss: 0.602\n",
      "epoch: 9  loss: 0.553\n",
      "epoch: 10  loss: 0.515\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 2-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.527\n",
      "epoch: 3  loss: 0.409\n",
      "epoch: 4  loss: 0.364\n",
      "epoch: 5  loss: 0.340\n",
      "epoch: 6  loss: 0.323\n",
      "epoch: 7  loss: 0.310\n",
      "epoch: 8  loss: 0.299\n",
      "epoch: 9  loss: 0.290\n",
      "epoch: 10  loss: 0.282\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.917\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function에 따른 성능 차이가 보입니다. Sigmoid는 기울기 소실 문제가 발생하지만 그에 비해 ReLU는 기울기 소실 문제가 없기 때문입니다. 이제 층을 늘려보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.303\n",
      "epoch: 2  loss: 2.295\n",
      "epoch: 3  loss: 2.288\n",
      "epoch: 4  loss: 2.278\n",
      "epoch: 5  loss: 2.259\n",
      "epoch: 6  loss: 2.221\n",
      "epoch: 7  loss: 2.147\n",
      "epoch: 8  loss: 2.027\n",
      "epoch: 9  loss: 1.887\n",
      "epoch: 10  loss: 1.741\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.504\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 3-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.875\n",
      "epoch: 2  loss: 0.635\n",
      "epoch: 3  loss: 0.415\n",
      "epoch: 4  loss: 0.356\n",
      "epoch: 5  loss: 0.326\n",
      "epoch: 6  loss: 0.305\n",
      "epoch: 7  loss: 0.288\n",
      "epoch: 8  loss: 0.273\n",
      "epoch: 9  loss: 0.261\n",
      "epoch: 10  loss: 0.248\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.928\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 4가지 경우의 accuracy를 표로 정리하겠습니다.\n",
    "\n",
    "||Sigmoid|ReLU|\n",
    "|:---:|:---:|:---:|\n",
    "|2-layer|0.877|0.917|\n",
    "|3-layer|0.504|0.928|\n",
    "\n",
    "ReLU의 경우 어느 정도 비슷해보이지만 Sigmoid의 경우 accuracy가 거의 절반으로 떨어진 것을 볼 수 있습니다. \n",
    "\n",
    "Sigmoid의 경우 층이 추가되면서 기울기 소실이 일어난 것으로 보입니다. 그에 반해 ReLU는 기울기 소실이 발생하지 않기에 층이 쌓여도 정확도가 떨어지지는 않습니다. \n",
    "\n",
    "만약 activation function이 없다면 가중치와 입력값의 곱으로만 뉴런이 이루어집니다. 이는 non-linear한 데이터를 표현할 수 없음을 의미합니다. 그리고 층을 여러개 쌓일 필요도 없어집니다. 그렇기에 non-linear한 데이터도 잘 표현하기 위해서 activation function은 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습4. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 3에 이어서 이번 실습에선 sgd, momentun, Adam 등의 optimizer를 사용해보고 성능을 비교해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfCTx8xj4zoaombrK2bSN9nv0Z3r95jp)\n",
    "\n",
    "- input: 784\n",
    "- hidden: (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- **optimizer: sgd** or **momentum** or **adam**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 3-layer Network + ReLU + SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.749\n",
      "epoch: 2  loss: 0.618\n",
      "epoch: 3  loss: 0.428\n",
      "epoch: 4  loss: 0.370\n",
      "epoch: 5  loss: 0.338\n",
      "epoch: 6  loss: 0.316\n",
      "epoch: 7  loss: 0.298\n",
      "epoch: 8  loss: 0.282\n",
      "epoch: 9  loss: 0.268\n",
      "epoch: 10  loss: 0.256\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.923\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 3-layer Network + ReLU + Momentum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.99)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.600\n",
      "epoch: 2  loss: 0.255\n",
      "epoch: 3  loss: 0.210\n",
      "epoch: 4  loss: 0.199\n",
      "epoch: 5  loss: 0.165\n",
      "epoch: 6  loss: 0.150\n",
      "epoch: 7  loss: 0.151\n",
      "epoch: 8  loss: 0.149\n",
      "epoch: 9  loss: 0.140\n",
      "epoch: 10  loss: 0.136\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + ReLU + Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.295\n",
      "epoch: 2  loss: 0.174\n",
      "epoch: 3  loss: 0.156\n",
      "epoch: 4  loss: 0.141\n",
      "epoch: 5  loss: 0.132\n",
      "epoch: 6  loss: 0.128\n",
      "epoch: 7  loss: 0.122\n",
      "epoch: 8  loss: 0.118\n",
      "epoch: 9  loss: 0.116\n",
      "epoch: 10  loss: 0.113\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer에 따른 정확도를 표로 나타내면 다음과 같다.\n",
    "\n",
    "|SGD|Momentum|Adam|\n",
    "|:---:|:---:|:---:|\n",
    "|0.923|0.947|0.958|\n",
    "\n",
    "Adam > Momentum > SGD 순임을 알 수 있습니다. 평균적으로 학습 속도도 정확도가 높을수록 짧습니다. SGD는 현재 위치의 기울기만을 가지고 경사 하강을 하기에 진동이 심합니다. 이에 반해 Momentum은 관성의 개념을 이용하여 진동을 줄이고 더욱 빠르게 저점으로 수렴하게 만듭니다. 그렇기에 SGD보다 Momentum이 더 빠르게 학습합니다. Adam은 Momentum의 방법에 이전 기울기까지 고려하여 이동합니다. 그렇기에 Momentum보다 더 빠르게 수렴하고 학습속도도 더 빠릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습5. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 image data에서 주로 사용되는 batch-normalization까지 추가해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xZSWZiSxuGZAsonghidhTSfUEYiuxRtN)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- optimizer: adam\n",
    "- **regularizer: batch_norm**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-layer Network + ReLU + Adam + batch_norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.260\n",
      "epoch: 2  loss: 0.146\n",
      "epoch: 3  loss: 0.121\n",
      "epoch: 4  loss: 0.101\n",
      "epoch: 5  loss: 0.092\n",
      "epoch: 6  loss: 0.085\n",
      "epoch: 7  loss: 0.080\n",
      "epoch: 8  loss: 0.073\n",
      "epoch: 9  loss: 0.070\n",
      "epoch: 10  loss: 0.068\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.972\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26634"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-normalization을 쓴 것과 쓰지 않은 것을 비교해보면 다음과 같습니다.\n",
    "\n",
    "|Batch-normalization O|Batch-normalization X|\n",
    "|:---:|:---:|\n",
    "|0.972|0.958|\n",
    "\n",
    "Batch-normalization을 썼을 때, 정확도가 더 증가한 것을 확인할 수 있습니다. Regularization은 feature들의 영향력을 조절하여 overfitting을 방지하는 역할을 합니다. 우리의 모델은 feature가 많음을 알 수 있습니다. 그렇기에 batch-normalization으로 overfitting을 방지하여 더 일반적이고 성능이 좋은 학습이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Convolution Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/62306 의 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 합성곱과 풀링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**합성곱 신경망(Convolutional Neural Network)** 은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. 합성곱 신경망은 크게 **합성곱층(Convolution layer)** 과 **풀링층(Pooling layer)** 으로 구성됩니다. 아래의 그림은 합성곱 신경망의 일반적인 예를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/convpooling.PNG\">\n",
    "\n",
    "위 그림에서 CONV는 합성곱 연산을 의미하고, 합성곱 연산의 결과가 활성화 함수 ReLU를 지납니다. 이 두 과정을 합성곱층이라고 합니다. 그 후에 POOL이라는 구간을 지나는데 이는 풀링 연산을 의미하여 풀링층이라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 합성곱 신경망의 대두"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다. 예를 들어, 알파벳 손글씨를 분류하는 어떤 문제가 있습니다. 알파벳 y를 손글씨로 쓴 두 가지 예시를 행렬로 표현한 것이 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv0.png\">\n",
    "\n",
    "사람이 보기에 두 그림은 모두 y로 보이지만 기계가 보기에 각 픽셀마다 가진 값이 거의 상이하므로 완전히 다른 입력으로 받아들입니다. 그리고 두 이미지 외에도 휘어지거나, 이동되거나 방향이 뒤틀리는 등 다양한 변형이 존재합니다. 다층 퍼셉트론은 몇 가지 픽셀만 값이 달라져도 민감하게 받아들이기에 적합하지 않습니다. \n",
    "\n",
    "이를 더 자세히 살펴봅시다. 만약 위 이미지를 다층 퍼셉트론으로 분류한다면 이미지를 1차원 텐서인 벡터로 변환하여 입력층으로 사용해야 합니다. 이를 전환하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv1.png\">\n",
    "\n",
    "1차원으로 변환된 결과는 사람이 보기에 원래 이미지를 유추하기 매우 어렵습니다. 이는 기계 역시 마찬가집니다. 위와 같은 변환은 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태입니다. 그렇기에 다층 퍼셉트론으로 이미지를 분류하기엔 어렵습니다. 그렇기에 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고 이를 위해 사용하는 것이 합성곱 신경망입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 채널(Channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리의 기본적인 용어인 채널에 대해서 간단히 정의하겠습니다.\n",
    "\n",
    "기계는 글자나 이미지보다, 텐서를 더 잘 처리할 수 있습니다. 이미지는 **(높이, 너비, 채널)**이라는 3차원 텐서입니다. 여기서 높이는 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지는 채널 수가 1이며 각 픽셀은 0부터 255 사이의 값을 가집니다. 아래는 28 x 28 픽셀의 손글씨 데이터를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv2.png\">\n",
    "\n",
    "위 손글씨 데이터는 흑백 이미지이므로 채널 수가 1입니다. 그렇기에 위 이미지는 (28 x 28 x 1)의 크기를 가지는 3차원 텐서입니다. 만약 컬러 이미지라면 RGB가 각각 채널이 1개씩 총 3개를 가집니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv3.png\">\n",
    "\n",
    "하나의 픽셀은 삼원색의 조합으로 이루어집니다. 만약, 높이와 너비가 28인 컬러 이미지가 있다면 이 이미지의 텐서는 (28 x 28 x 3)의 크기를 가지는 3차원 텐서입니다. 채널은 떄로는 깊이(depth)라고도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 합성곱 연산(Convolution operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출하는 역할을 합니다. 합성곱은 kernel 또는 filter라는 n x m크기의 행렬로 height x width 크기의 이미지를 처음부터 끝까지 겹치고 훓으면서 n x m 크기의 겹쳐지는 부분의 각 이미지와 kernel의 원소의 값을 곱합니다. 그리고 이를 모두 더하여 값으로 출력하는 것을 말합니다. \n",
    "\n",
    "- kernel은 일반적으로 3 x 3, 5 x 5를 많이 사용합니다.\n",
    "\n",
    "예시를 보겠습니다. 아래는 3 x 3크기의 커널로 5 x 5 이미지 행렬에 합성곱 연산을 수행하는 과정을 보여줍니다. 한 번의 연산을 1 step이라고 했을 때, 합성곱 연산의 네번째 스텝까지 이미지와 식으로 보겠습니다. \n",
    "\n",
    "1) 첫번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv4.png\">\n",
    "\n",
    "$(1 \\times 1) + (2 \\times 0) + (3 \\times 1) + (2 \\times 1) + (1 \\times 0)+ (0 \\times 1)+ (3 \\times 0)+ (0 \\times 1) + (1 \\times 0) = 6$\n",
    "\n",
    "\n",
    "2) 두번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv5.png\">\n",
    "\n",
    "$(2 \\times 1) + (3 \\times 0) + (4 \\times 1) + (1 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (0 \\times 0)+ (1 \\times 1) + (1 \\times 0) = 9$\n",
    "\n",
    "\n",
    "3) 세번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv6.png\">\n",
    "\n",
    "$(3 \\times 1) + (4 \\times 0) + (5 \\times 1) + (0 \\times 1) + (1 \\times 0)+ (2 \\times 1)+ (1 \\times 0)+ (1 \\times 1) + (0 \\times 0) = 11$\n",
    "\n",
    "\n",
    "4) 네번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv7.png\">\n",
    "\n",
    "$(2 \\times 1) + (1 \\times 0) + (0 \\times 1) + (3 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (1 \\times 0)+ (4 \\times 1) + (1 \\times 0) = 10$\n",
    "\n",
    "\n",
    "총 9번의 스텝을 했을 때 최종 결과는 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv8.png\">\n",
    "\n",
    "위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 **특성 맵(feature map)** 이라고 합니다.\n",
    "\n",
    "위의 예제에선 커널의 크기 3 x 3이었지만, 커널의 크기는 사용자가 지정할 수 있습니다. 또한 커널의 이동 범위가 위의 예제에서는 한 칸이었지만, 이 또한 사용자가 정할 수 있습니다. 이러한 이동 범위를 **스트라이드(stride)** 라고 합니다.\n",
    "\n",
    "아래의 예제는 스트라이드가 2일 때, 5 x 5 이미지에 합성곱 연산을 수행하는 3 x 3 커널의 움직임을 보여줍니다. 최종적으로 2 x 2 크기의 특성 맵을 얻습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 패딩(Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예시처럼 합성곱 연산으로 얻은 특성 맵은 입력보다 크기가 작아지는 특징이 있습니다. 만약, 합성곱 층을 여러개 쌓았다면 최종적으로 얻는 특성 맵의 크기는 입력보다 매우 작아진 상태가 됩니다. 만약 합성곱 연산 이후에도 특성 맵의 크기가 입력 크기와 동일하게 유지되도록 하고 싶다면 패딩(padding)을 사용합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv10.png\">\n",
    "\n",
    "패딩은 합성곱 연산을 하기 전에 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가하는 것을 말합니다. 다시 말해 지정된 개수의 폭만큼 테두리를 추가합니다. 주로 0으로 채우는 제로 패딩(zeor padding)을 사용합니다. 위의 그림은 5 x 5 이미지에 1폭짜리 제로 패딩을 사용한 모습입니다.\n",
    "\n",
    "만약 스트라이드가 1일 때, 3 x 3 크기의 커널을 사용한다면 1폭짜리 제로 패딩을 사용하고, 5 x 5 크기의 커널을 사용한다면 2폭짜리 제로 패딩을 사용하여 크기를 보존할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 가중치와 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 신경망에서의 가중치와 편향을 이해하기 위해 먼저 다층 퍼셉트론을 복습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 합성곱 신경망의 가중치**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 퍼셉트론으로 3 x 3 이미지를 처리한다고 가정하겠습니다. 우선 이미지를 1차원 텐서로 만들면 입력층은 9개의 뉴론을 가집니다. 그리고 4개의 뉴론을 가지는 은닉층을 추가한다면 아래의 그림과 같아집니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv11.png\">\n",
    "\n",
    "위에서 각 연결선은 가중치를 의미하므로 위의 그림에서는 36(=9 x 4)개의 가중치를 가집니다.\n",
    "\n",
    "같은 이미지를 합성곱 신경망으로 처리해보겠습니다. 2 x 2 커널을 사용하고 스트라이드는 1로 하겠습니다. (*는 합성곱 연산을 의미합니다.)\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv12.png\">\n",
    "\n",
    "합성곱 신경망에서 가중치는 커널 행렬의 원소들입니다. 이를 인공 신경망으로 표현하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv13.png\">\n",
    "\n",
    "최종적으로 특성 맵을 얻기 위해서 동일한 커널로 이미지 전체를 훑으면서 합성곱 연산을 진행합니다. 결국 이미지 전체를 훑으면서 사용되는 가중치는 $w_0, w_1, w_2, w_3$ 4개 뿐입니다. 그리고 각 합성곱 연산마다 이미지의 모든 픽셀을 사용하는 것이 아니라 커널과 맵핑되는 픽셀만을 입력으로 사용하는 것을 볼 수 있습니다. 결국 합성곱 신경망은 다층 퍼셉트론보다 훨씬 적은 가중치를 사용하며 공간적 구조 정보를 보존하는 특징을 가집니다.\n",
    "\n",
    "다층 퍼셉트론의 은닉층에서 가중치 연산 이후, 비선형성을 위해 활성화 함수를 통과시켰듯, 합성곱 신경망에서도 합성곱 연산을 통해 얻은 특성 맵을 활성화 함수를 통과시켜 비선형성을 갖게 만듭니다. 이때 활성화 함수로 ReLU나 그 변형들이 주로 사용됩니다. 이와 같이 합성곱 연산을 통해 특성 맵을 얻고, 활성화 함수를 지나는 연산을 하는 합성곱 신경망의 은닉층을 **합성곱 층(convolution layer)** 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 합성곱 신경망의 편향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/64066/conv14.png\">\n",
    "\n",
    "합성곱 신경망에도 편향을 추가할 수 있습니다. 만약, 편향을 사용한다면 커널을 적용한 뒤에 더해집니다. 편향은 하나의 값만 존재하며 커널이 적용된 결과의 모든 원소에 더해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 특성 맵의 크기 계산 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산의 notation을 먼저 확인하겠습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $S$: 스트라이드\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "\n",
    "이에 따라 특성 맵의 높이와 너비는 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w}{S} + 1)$$\n",
    "\n",
    "여기서 $floor$ 함수는 소수점 발생 시, 소수점 이하를 버리는 역할을 합니다. \n",
    "\n",
    "예를 들어 5 x 5 크기의 이미지에 3 x 3 커널을 사용하고 스트라이드 1로 합성곱 연산을 한 경우, 특성 맵의 크기는 (5 - 3 + 1) x (5 - 3 + 1) = 3 x 3임을 알 수 있습니다. 이는 또한 9번의 스텝이 필요함을 의미하기도 합니다.\n",
    "\n",
    "패딩의 폭을 $P$라고 할=고, 패딩까지 고려한 식은 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h + 2P}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w + 2P}{S} + 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 채널 또는 깊이를 고려하지 않고, 2차원 텐서를 가정하고 설명했습니다. 하지만 실제로 합성곱 연산의 입력은 '다수의 채널을 가진' 이미지 또는 이전 연산의 결과로 나온 특성 맵일 수 있습니다. 만약, 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다고 하면 커널의 채널 수도 입력의 채널 수만큼 존재해야 합니다. 다시 말해 입력 데이터의 채널 수와 커널의 채널 수는 같아야 합니다. 채널 수가 같으므로 합성곱 연산을 채널마다 수행합니다. 그리고 그 결과를 모두 더하여 최종 특성 맵을 얻습니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv15.png\">\n",
    "\n",
    "위 그림은 3개의 채널을 가진 입력 데이터와 3개의 채널을 가진 커널의 합성곱 연산을 보여줍니다. 커널의 각 채널끼리의 크기는 같아야 합니다. 각 채널간 합성곱 연산을 마치고, 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵을 만듭니다. 주의할 점은 위의 연산에서 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 것입니다.\n",
    "\n",
    "위 그림은 높이 3, 너비 3, 채널 3의 입력이 높이 2, 너비 2, 채널 3의 커널과 합성곱 연산을 하여 높이 2, 너비 2, 채널 1의 특성 맵을 얻는다는 의미입니다. 합성곱 연산의 결과로 얻은 특성 맵의 태널 차원은 RGB 채널 등과 같은 컬러의 의미를 담고 있지 않습니다.\n",
    "\n",
    "이제 이 연산에서 각 차원을 변수로 두고 좀 더 일반화시켜보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 3차원 텐서의 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화를 위해 사용하는 변수들은 다음과 같습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "- $C_i$: 입력 데이터의 채널\n",
    "\n",
    "다음은 3차원 텐서의 합성곱 연산을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv16_final.png\">\n",
    "\n",
    "높이 $I_h$, 너비 $I_w$, 채널 $C_i$의 입력 데이터는 동일한 채널 수 $C_i$를 가지는 높이 $K_h$, 너비 $K_w$의 커널과 합성곱 연산을 하여 높이 $O_h$, 너비 $O_w$, 채널 1의 특성 맵을 얻습니다. 그런데 하나의 입력에 여러 개의 커널을 사용하는 합성곱 연산을 할 수도 있습니다. \n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 특성 맵의 크기가 어떻게 바뀌는지 봅시다. 다음은 $C_o$를 합성곱 연산에 사용하는 커널의 수라고 했을 때의 합성곱 연산 과정을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv17_final_final.PNG\">\n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 사용한 커널 수는 합성곱 연산의 결과로 나오는 특성 맵의 채널 수가 됩니다.\n",
    "\n",
    "이를 이해했다면 커널의 크기와 입력 데이터의 채널 수 $C_i$와 특성 맵(출력 데이터)의 채널 수 $C_o$가 주어졌을 때, 가중치 매개변수의 총 개수를 구할 수 있습니다. 가중치는 커널의 원소들이므로 하나의 커널의 하나의 채널은 $K_i \\times K_o$개의 매개변수를 가지고 있습니다. 그런데 합성곱 연산을 하려면 커널은 입력 데이터의 채널 수와 동일한 채널 수를 가져야 합니다. 이에 따라 하나의 커널이 가지는 매개변수의 수는 $K_i \\times K_o \\times C_i$입니다. 그런데 이러한 커널이 총 $C_o$개가 있어야 하므로 가중치 매개변수의 총 수는 다음과 같습니다.\n",
    "\n",
    "가중치 매개변수의 총 수: $K_i \\times K_o \\times C_i \\times C_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 풀링(Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적입니다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어집니다. 풀링 연산에는 일반적으로 최대 풀링(max pooling)과 평균 풀링(average pooling)이 사용됩니다. 우선 최대 풀링을 통해 풀링 연산을 보겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/maxpooling.PNG\">\n",
    "\n",
    "풀링 연산에서도 합성곱 연산과 마찬가지로 커널과 스트라이드의 개념을 가집니다. 위의 그림은 스트라이드가 2일 때, 2 x 2 크기 커널로 맥스 풀링 연산과정입니다. 특성맵이 절반의 크기로 다운샘플링되는 것을 볼 수 있습니다. 맥스풀링은 커널과 겹치는 영역 안에서 최대값을 추출하는 방식으로 다운샘플링합니다.\n",
    "\n",
    "평균 풀링은 최대값대신 평균값을 추출하는 연산이 됩니다. 풀링 연산은 커널가 스트라이드 개념이 존재한다는 점에서 합성곱 연산과 유사하지만 합성곱 연산과 차이점은 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 점입니다. \n",
    "\n",
    "풀링을 사용하면, 특성맵의 크기가 줄어드므로 특성맵의 가중치 개수를 줄여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. Fully-Connected Layer vs Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1의 실습들을 통해 model의 다양한 node를 바꿔가며 mnist의 성능 변화를 확인해보았습니다. 비록, fully-connected network가 mnist 데이터에서 높은 성능을 내는데 문제가 없었지만, 모든 layer를 fully-connected layer로 만드는 것은 엄청난 파라미터와 연산량을 필요로 합니다. 그렇기에 큰 고화질의 이미지 데이터를 처리하는데 적합합지 않습니다.\n",
    "\n",
    "따라서, 이번 실습은 이미지 데이터 처리에 주로 사용되는 convolution layer를 사용하여 파라미터 수와 성능이 어떻게 변화하는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1xdjTf4ab0P8qfu_TaLJ4TZzt5sk3twS6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.204\n",
      "epoch: 2  loss: 0.098\n",
      "epoch: 3  loss: 0.089\n",
      "epoch: 4  loss: 0.086\n",
      "epoch: 5  loss: 0.078\n",
      "epoch: 6  loss: 0.081\n",
      "epoch: 7  loss: 0.079\n",
      "epoch: 8  loss: 0.076\n",
      "epoch: 9  loss: 0.079\n",
      "epoch: 10  loss: 0.074\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.973\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4274"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|구분|Fully-connected layer|Covolution operation|\n",
    "|:---:|:---:|:---:|\n",
    "|정확도|0.972|0.973|\n",
    "|parameter 개수|26634|4274|\n",
    "\n",
    "챕터1에서 0.972가 나온 것을 보면 0.001의 차이는 크진 않습니다. 그러나 parameter의 개수가 6배 정도 차이납니다. 이는 연산량을 줄일 수 있고 큰 데이터를 다루기에 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. 나만의 model 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1, 2의 실습을 바탕으로 20,000개 이하의 파라미터를 가지며 98%이상의 정확도를 갖는 모델을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=4,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel()\n",
    "if count_parameters(model) > 20000:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(4, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.240\n",
      "epoch: 2  loss: 0.112\n",
      "epoch: 3  loss: 0.099\n",
      "epoch: 4  loss: 0.093\n",
      "epoch: 5  loss: 0.094\n",
      "epoch: 6  loss: 0.093\n",
      "epoch: 7  loss: 0.090\n",
      "epoch: 8  loss: 0.085\n",
      "epoch: 9  loss: 0.084\n",
      "epoch: 10  loss: 0.086\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.975\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
