{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Neural Networks\n",
    "2. Convolution Neural Network\n",
    "3. CNN - Case Study\n",
    "4. Recurrent Neural Networks\n",
    "5. Attention Models\n",
    "6. Generative Adversarial Network\n",
    "7. Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Training Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Sigmoid activation**"
   ]
  },
  {
   "attachments": {
    "sigmoid.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid.PNG](_image/sigmoid.PNG)  \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "앞서 배웠듯이 0 ~ 1 사이 값을 가지는 함수입니다. 그러나 Neural Networks에서 사용하기엔 많은 문제점이 있습니다.\n",
    "\n",
    "- 기울기 소실\n",
    "- output의 중간값이 0이 아니다.\n",
    "- exp()의 연산이 너무 무겁다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Softmax activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_i = \\frac{e^{z_i}}{\\sum_{j = 1}^k e^{z_j}}$$\n",
    "\n",
    "sigmoid가 두 개의 값으로 분류한다면 softmax는 여러 가지 값으로 분류한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) tanh activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh](_image/tanh.PNG)\n",
    "\n",
    "$$tanh(x) = 2 \\times sigmoid(x) - 1$$\n",
    "\n",
    "-1 ~ 1의 범위를 갖고 있습니다. sigmoid의 문제점 중 하나인 output의 중간값을 0으로 만들었습니다. 그러나 기울기 소실 문제는 여전히 남아있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) ReLU(Rectified Linear Unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](_image/ReLU.PNG)\n",
    "\n",
    "$$computes \\; f(x) = max(0, x)$$\n",
    "\n",
    "학습이 굉장히 빠르고 기울기 소실 문제가 사라집니다. output의 중간값이 0은 아니지만 많이 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid나 tanh에서 중요한 것 중 하나는 기울기가 살아있는 범위에 input 데이터가 들어가야 유의미한 값이 나온다는 것입니다. 그걸 위해 사용하는 방법이 **Batch Normalization** 입니다. 이를 통해 기울기 소실 문제를 해결할 수 있습니다. \n",
    "\n",
    "$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\text{E}[x^{(k)}]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$\n",
    "\n",
    "위 식을 통해 평균 0, 분산 1의 input data가 만들어집니다. \n",
    "\n",
    "그 후, Neural Network가 데이터에 말맞게 평균과 분산을 조절합니다.\n",
    "\n",
    "$$y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\n",
    "\n",
    "이를 통해 $\\gamma$는 표준편차를, $\\beta$는 평균을 학습시킨다. 이렇게 $\\gamma , \\beta$의 최적값을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization을 위한 방법은 여러가지가 있습니다. 왜냐하면 gradient descent를 그대로 사용하면 변동폭이 너무 크게 일어나기 때문입니다. 이제 하나씩 살펴보겠습니다.\n",
    "\n",
    "여러 방식들이 어떻게 이루어지는지 gif로 보려면 다음 링크로 접속하여 확인하면 됩니다.\n",
    "http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 5 # input data\n",
    "dx = 2 # gradient\n",
    "learning_rate = 0.1 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Gradient Descent(SGD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sgd](_image/sgd.PNG)\n",
    "\n",
    "일반적인 방법은 현재 자신의 위치에서 측정한 경사에 따라 움직입니다. 그렇기에 위 그림처럼 진동이 엄청나게 일어납니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla gradient descent update\n",
    "x -= learning_rate * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Momentum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![momentum](_image/momentum.PNG)\n",
    "\n",
    "Momentum은 현재 위치에서 측정한 기울기만 사용하지 않습니다. 전에 지나온 기울기들을 계속해서 합산하여 사용합니다. 그렇기 때문에 진동이 큰 방향은 진동 크기를 줄이고 맞는 방향은 더욱 빠르게 가도록 해줍니다. \n",
    "\n",
    "기울기를 합산할 때, 예전 기울기의 영향력을 줄이기 위해서 0.8 등의 공비를 계속해서 곱해줍니다. \n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1000/1*X9SaxFM6_sBOAMY9TaGsKw.png\">\n",
    "\n",
    "이름이 momentum이듯, 관성처럼 local minimum에 머물지 않고 global minimum으로 가도록 만들어줍니다.\n",
    "\n",
    "이를 코드로 보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum update\n",
    "mu = 0.8 # 공비\n",
    "v = 0\n",
    "\n",
    "v = mu * v - learning_rate * dx\n",
    "x += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Adagrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기들을 합산할 때, 기울기의 크기가 많이 차이나지 않도록 기울기를 제곱해서 합을 구한다음 루트를 취하여 기울기에 나눠줍니다. 코드를 통해 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad update\n",
    "cache = 0\n",
    "\n",
    "cache += dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 1e-7을 더해주는 이유는 cache가 너무 작아져서 0이 되어 error가 나는 것을 방지하기 위해서입니다.\n",
    "\n",
    "Adagrad는 방향을 잘 찾아가지만 목표에 도달할수록 나누는 값이 너무 커집니다. 그렇기에 한 번의 가는 거리가 점점 작아지고 속도가 느려지게 됩니다.\n",
    "\n",
    "그렇기에 Adagrad를 직접 사용하지 않고 이를 활용하는 방법들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) RMSProp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad에서 decay_rate를추가하여 오래된 기울기들의 영향력을 제거하는 방식을 도입한 것이 RMSProp입니다. 코드로 구현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.1\n",
    "\n",
    "# RMSProp update\n",
    "cache += decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp와 Momentum 방식을 합쳐서 사용하는 방법이 Adam입니다. 기본적으로 beta1 = 0.9, beta2 = 0.999, eps = 1e-8을 추천됩니다. 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam upgrade\n",
    "m, v = 0, 0\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "m = beta1 * m + (1 - beta1) * dx\n",
    "v = beta2 * v + (1 - beta2) * (dx**2)\n",
    "x -= learning_rate * m / (np.sqrt(v) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate는 gradient descent에서 움직이는 보폭 비율이라고 생각하면 됩니다. 그렇기에 learning rate가 크면 크게크게 학습하고 작으면 차근차근 학습하게 됩니다. learning rate의 크기에 따라 학습 정도를 살펴보면 다음과 같습니다.\n",
    "\n",
    "![learning_rate](_image/learning_rate.PNG)\n",
    "\n",
    "하나씩 살펴보면 먼저 작은 값을 가지면 학습속도가 매우 느립니다. 만약 큰 값을 가진다면 학습속도가 빠르지만 어느 순간부터 학습이 진행되지 않습니다. 왜냐하면 현재 위치와 목표점의 거리보다 보폭이 더 크기 때문입니다. 그리고 만약 매우 큰 값을 가진다면 학습이 될 수 있지만 금방 발산해버립니다.\n",
    "\n",
    "그렇기 때문에 epoch가 진행될수록 learning rate를 줄이는 방법을 사용합니다.\n",
    "\n",
    "\n",
    "![learning_rate2](_image/learning_rate2.PNG)\n",
    "\n",
    "위 그림처럼 loss가 줄지 않을 때마다 learning rate를 줄여서 학습을 진행합니다. \n",
    "\n",
    "learning rate를 늘리거나 줄일 땐, 0.1, 1, 10, 100,... 등 10배씩 키우거나 줄이는 것이 일반적입니다. 또는 $\\sqrt{10} \\sim 3$을 이용해 3배씩 키우거나 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble](_image/ensemble.PNG)\n",
    "\n",
    "\n",
    "위 그림처럼 여러 가지 모델들의 결과를 합쳐서 하나의 평균값으로 예측하는 것을 Ensemble이라고 합니다. Ensemble은 대체적으로 2 ~ 3% 정도 정답률이 증가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) L1, L2 Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L1 : \\sum_k \\sum_l \\lvert W_{k, l} \\rvert \\quad (W : \\text{weight decay})$$\n",
    "$$L2 : \\sum_k \\sum_l (W_{k, l})^2$$\n",
    "$$Elastic net(\\text{L1 + L2}) : \\sum_k \\sum_l (\\beta W_{k, l}^2 + \\lvert W_{k, l} \\rvert)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 모델에 여러 뉴런이 있습니다. 이 중 임의로 몇 개의 뉴런을 무시하여 다양한 모델들을 사용하는 결과를 주는, 즉 ensemble 효과를 주는 방법을 Dropout이라고 합니다. \n",
    "\n",
    "![dropout](_image/dropout.PNG)\n",
    "\n",
    "위 그림처럼 임의의 뉴런을 무시하고 진행하는 방식입니다. 위 방식을 여러번 사용하여 ensemble처럼 모든 모델들의 예측값의 평균을 내서 결과를 예측합니다.\n",
    "\n",
    "주의할 점은 Dropout했던 모델을 테스트할 때는 모든 노드를 사용한다는 것입니다. 그렇기에 output이 학습할 때보다 크게 나오게 됩니다. 이를 방지하기 위해 학습 때 사용한 노드의 비중만큼만 output에서 가져옵니다.\n",
    "\n",
    "예를 들어 70%의 뉴런만 사용하여 학습을 진행했다면 실제 테스트할 때도 ouput의 70%를 실제 예측값으로 사용하게 됩니다.\n",
    "\n",
    "이를 코드로 나타내면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5 # probability of keeping a unit active, higher = less dropout\n",
    "\n",
    "\n",
    "def train_step(X, W, b):\n",
    "    \"\"\"X contains the data\"\"\"\n",
    "    \n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0])\n",
    "    U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "    H1 *= U1 # drop\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1])\n",
    "    U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "    H2 *= U2 # drop\n",
    "    out = np.dot(W[2], H2) + b[2]\n",
    "    \n",
    "    # backward pass: compute gradients...(not shown)\n",
    "    # perform parameter update...(not shown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    # ensemble forward pass\n",
    "    H1 = np.maximum(0, np.dot(W[0], X) + b[0]) * p # scale the activations\n",
    "    H2 = np.maximum(0, np.dot(W[1], H1) + b[1]) * p # scale the activations\n",
    "    out = np.dot(W[2], H2) + b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가진 데이터들이 적을 때나 더 많이 필요할 때, 데이터들을 변형, 회전, 늘림 등의 과정을 통해 데이터를 늘리는 것을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Tensor operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서는 배열이나 행렬과 매우 유사한 자료구조입니다. PyTorch에서는 텐서를 사용하여 모델의 입력과 출력뿐만 아니라 모델의 파라미터를 나타냅니다.\n",
    "\n",
    "GPU나 다른 연산 가속을 위한 특수한 하드웨어에서 실행할 수 있다는 점을 제외하면, 텐서는 NumPy의 ndarray와 매우 유사합니다.\n",
    "\n",
    "이제 텐서에 대한 다양한 구현을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터로부터 직접 생성하기\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array로부터 생성하기\n",
    "np_array = np.array(data)\n",
    "x = torch.from_numpy(np_array)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor에서 numpy array로 변환하기\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0851, 0.0584],\n",
      "        [0.3296, 0.2676]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 텐서와 같은 모양의 텐서 초기화하기\n",
    "x_ones = torch.ones_like(x) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.4402, 0.1129, 0.1370, 0.3384],\n",
      "        [0.3500, 0.0489, 0.0349, 0.6645],\n",
      "        [0.6792, 0.5942, 0.9422, 0.8516]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 주어진 shape로 초기화하기\n",
    "shape = (3,4)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 속성은 텐서의 모양, 자료형 및 어느 장치에 저장되는지를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 노트북은 gpu가 없다\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "#tensor = tensor.to(device)\n",
    "#print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 텐서간의 연산도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 식의 인덱싱과 슬라이싱\n",
    "tensor = torch.ones(3, 4)\n",
    "tensor[:, 1] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 행으로 길어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기, 열로 이어지도록\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 곱하기\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산합니다\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "\n",
    "# 다른 문법:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 간 행렬 곱셈\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# 다른 문법:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Autograd**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. autograd를 통해 입력 X, 파라미터 W , 그리고 cross-entropy loss를 사용하는 logistic regression model의 gradient를 구하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[ 0.6696, -1.1055,  0.7509],\n",
      "        [ 0.2827,  0.7063,  1.1959],\n",
      "        [ 0.4497,  0.8117,  0.0219],\n",
      "        [ 0.3535,  0.1437,  1.7595],\n",
      "        [-0.3871,  0.4001, -1.1086]], requires_grad=True)\n",
      "tensor([-1.2913,  0.2003,  0.2606], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 입력 및 파라미터 초기화\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0770, 1.1566, 2.8803], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward\n",
    "z = torch.matmul(x,w)+b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 node를 크게 2가지 방법의 api를 활용해서 사용합니다.\n",
    "\n",
    "1. torch.nn\n",
    "2. torch.nn.functional\n",
    "\n",
    "torch.nn은 사전에 node를 초기화하고 해당 node에 텐서를 통과시켜 값을 받는 형태지만, torch.nn.functional은 사전에 초기화없이 바로 함수처럼 사용하는 방식입니다.\n",
    "\n",
    "코딩 스타일에 맞춰서 원하시는 api를 사용하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비용 함수\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에서 매개변수의 가중치를 최적화하려면 파라미터에 대한 loss function의 도함수(derivative)를 계산해야 합니다. \n",
    "이러한 도함수를 계산하기 위해, loss.backward() 를 호출한 다음 w.grad와 b.grad에서 값을 가져옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156],\n",
      "        [0.1731, 0.2536, 0.3156]])\n",
      "tensor([0.1731, 0.2536, 0.3156])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로, requires_grad=True인 모든 텐서들은 연산 기록을 추적하고 미분 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 forward 연산만 필요한 경우에는, 미분 연산을 위한 값들을 저장해두는 것이 속력 및 메모리의 저하를 가져올 수 있습니다. 연산 코드를 torch.no_grad() 블록으로 둘러싸서 미분 추적을 멈출 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. LR vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 MNIS dataset을 활용하여 logistic regression model과 MLP model을 구현해보고 학습 파이프라인을 익혀보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Preprocess Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist에 존재하는 각각의 사진은 28*28의 픽셀로 구성된 784차원짜리 벡터로 나타나져 있습니다. 각 픽셀은 0~255 사이의 값으로 흰색부터 검은색 사이의 값을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "X = X.values\n",
    "y = y.values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# scale\n",
    "X /= 255.0\n",
    "print(X.min(), X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(7000, 784)\n",
      "(7000,)\n",
      "(7000, 784)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "print(X_train.shape) # 80%\n",
    "print(y_train.shape)\n",
    "print(X_val.shape) # 10%\n",
    "print(y_val.shape)\n",
    "print(X_test.shape) # 10%\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dataset\n",
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)\n",
    "        \n",
    "\n",
    "plot_example(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) PyTorch Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 Custom Dataset을 사용하기 위해서는 torch.utils.data.Dataset의 형태로 dataset class를 정의해준 이후, torch.utils.data.DataLoader의 형태로 dataloader class를 정의하여 학습시에 model에 forwarding할 data를 sample해줍니다.\n",
    "\n",
    "(https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "\n",
    "\n",
    "가장 보편적으로 사용되는 map-style의 dataset class는 torch.utils.data.Dataset을 superclass로 받아 **getitem()** 과 **len()** 함수를 override해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "7000\n",
      "(7000, 784)\n",
      "7000\n",
      "(7000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(val_dataset))\n",
    "print(val_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader는 train 혹은 validation시 dataset에서 batch를 sampling하기 위한 API입니다 (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "필수적으로 사용하는 option들은 아래와 같습니다.\n",
    "- dataset: sampling할 dataset\n",
    "- batch_size: 한번에 sampling할 dataset의 개수\n",
    "- shuffle: 1 epoch를 기준으로 dataset을 shuffle할지\n",
    "\n",
    "더 자세한 option은 api를 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "110\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch에서 model을 선언할 때는 torch.nn.Module class를 superclass로 받아 __init__()함수와 forward() 함수를 작성해줍니다.\n",
    "\n",
    "__init__()함수에는 모델의 파라미터들을 선언하고, forward함수에는 해당 파라미터들을 이용하여 data를 model에 통과시켜줍니다.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression Model\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LR, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP Model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 선언한 model을 통해 학습을 진행하기 위해선 파라미터를 최적화할 optimizer가 필요합니다. 이번 실습에선 가장 보편적으로 사용되는 Adam optimizer를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class Trainer():\n",
    "    def __init__(self, trainloader, valloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        # 학습을 시작하기 위해 model을 train-mode로 변경\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                # optimizer는 예전 기울기도 계속 저장하기에 기울기를 초기화해준다.\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                # get output after passing through the network\n",
    "                outputs = self.model(inputs) \n",
    "                # compute model's score using the loss function\n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                # perform back-propagation from the loss\n",
    "                loss.backward() \n",
    "                # gradient descent를 통해 model의 output을 얻는다.\n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        val_acc = self.validate()\n",
    "        return val_acc\n",
    "\n",
    "    def validate(self):\n",
    "        # 현재 model이 train-mode일 수 있기에 eval-mode로 바꿔 validate를 수행할 수 있도록 변경\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.valloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.valloader.dataset)\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        return correct / len(self.testloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.166\n",
      "epoch: 2  loss: 1.194\n",
      "epoch: 3  loss: 1.261\n",
      "epoch: 4  loss: 1.297\n",
      "val_acc: 0.886\n",
      "epoch: 1  loss: 0.356\n",
      "epoch: 2  loss: 0.305\n",
      "epoch: 3  loss: 0.296\n",
      "epoch: 4  loss: 0.294\n",
      "val_acc: 0.919\n",
      "epoch: 1  loss: 0.555\n",
      "epoch: 2  loss: 0.325\n",
      "epoch: 3  loss: 0.295\n",
      "epoch: 4  loss: 0.281\n",
      "val_acc: 0.924\n",
      "epoch: 1  loss: 1.387\n",
      "epoch: 2  loss: 0.735\n",
      "epoch: 3  loss: 0.557\n",
      "epoch: 4  loss: 0.474\n",
      "val_acc: 0.893\n",
      "test_acc: 0.919\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = LR(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_logistic_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_logistic_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.930\n",
      "epoch: 2  loss: 0.956\n",
      "epoch: 3  loss: 1.004\n",
      "epoch: 4  loss: 1.003\n",
      "val_acc: 0.678\n",
      "epoch: 1  loss: 0.278\n",
      "epoch: 2  loss: 0.170\n",
      "epoch: 3  loss: 0.148\n",
      "epoch: 4  loss: 0.133\n",
      "val_acc: 0.956\n",
      "epoch: 1  loss: 0.472\n",
      "epoch: 2  loss: 0.254\n",
      "epoch: 3  loss: 0.206\n",
      "epoch: 4  loss: 0.173\n",
      "val_acc: 0.950\n",
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.553\n",
      "epoch: 3  loss: 0.414\n",
      "epoch: 4  loss: 0.358\n",
      "val_acc: 0.911\n",
      "test_acc: 0.953\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "epoch = 4\n",
    "\n",
    "best_acc = 0.0\n",
    "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "for lr in lrs:\n",
    "    model = MLP(input_dim=input_dim, \n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)\n",
    "    val_acc = trainer.train(epoch = epoch)\n",
    "    print('val_acc: %.3f' %(val_acc))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/3-1_MLP_model')\n",
    "\n",
    "trainer.model.load_state_dict(torch.load('models/3-1_MLP_model'))\n",
    "test_acc = trainer.test()\n",
    "print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습3. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 activation function 중 가장 대표적으로 사용되는 sigmoid functio과 ReLU function을 사용해보고 비교해보겠습니다. 데이터는 실습 2에서 사용했던 MNIST를 사용합니다. 모델은 train과 test만 사용하겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfJBd9v9L_RgXGf8urNrYpb40zXU6gea)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- **activation: sigmoid or relu**\n",
    "- optimizer: sgd\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Prerequisite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available is True else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oCnmrA9ltYs0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "X = mnist.data.astype('float32').values\n",
    "y = mnist.target.astype('int64').values\n",
    "X /= 255.0\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB-u3e9taDjT"
   },
   "source": [
    "#### **Split Dataset**\n",
    "\n",
    "학습과 평가를 위한 dataset으로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-HWWRcNjaLDi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(56000,)\n",
      "(14000, 784)\n",
      "(14000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYnvqbdijWUQ"
   },
   "source": [
    "#### **Pytorch Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ypqp7zA-xRlB"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(np.array(y)).long()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hTr4OWatzmaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000\n",
      "(56000, 784)\n",
      "14000\n",
      "(14000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.X.shape)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51PT-uPVzE8_"
   },
   "source": [
    "#### **DataLoader**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x2k3YVBoxRnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n",
      "219\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# shuffle the train data\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# do not shuffle the val & test data\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# dataset size // batch_size\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN65oTBk1d4T"
   },
   "source": [
    "#### **Trainer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OJqIwSltn9uY"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        self.model.train()\n",
    "        for e in range(epoch):\n",
    "            running_loss = 0.0  \n",
    "            for i, data in enumerate(self.trainloader, 0): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs) \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                loss.backward() \n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_acc = correct / len(self.testloader.dataset)\n",
    "        print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 2-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.187\n",
      "epoch: 2  loss: 1.817\n",
      "epoch: 3  loss: 1.386\n",
      "epoch: 4  loss: 1.079\n",
      "epoch: 5  loss: 0.882\n",
      "epoch: 6  loss: 0.754\n",
      "epoch: 7  loss: 0.666\n",
      "epoch: 8  loss: 0.602\n",
      "epoch: 9  loss: 0.553\n",
      "epoch: 10  loss: 0.515\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.877\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 2-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=32, \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.298\n",
      "epoch: 2  loss: 0.527\n",
      "epoch: 3  loss: 0.409\n",
      "epoch: 4  loss: 0.364\n",
      "epoch: 5  loss: 0.340\n",
      "epoch: 6  loss: 0.323\n",
      "epoch: 7  loss: 0.310\n",
      "epoch: 8  loss: 0.299\n",
      "epoch: 9  loss: 0.290\n",
      "epoch: 10  loss: 0.282\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.917\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function에 따른 성능 차이가 보입니다. Sigmoid는 기울기 소실 문제가 발생하지만 그에 비해 ReLU는 기울기 소실 문제가 없기 때문입니다. 이제 층을 늘려보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + Sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 2.303\n",
      "epoch: 2  loss: 2.295\n",
      "epoch: 3  loss: 2.288\n",
      "epoch: 4  loss: 2.278\n",
      "epoch: 5  loss: 2.259\n",
      "epoch: 6  loss: 2.221\n",
      "epoch: 7  loss: 2.147\n",
      "epoch: 8  loss: 2.027\n",
      "epoch: 9  loss: 1.887\n",
      "epoch: 10  loss: 1.741\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.504\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 3-layer Network + ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.875\n",
      "epoch: 2  loss: 0.635\n",
      "epoch: 3  loss: 0.415\n",
      "epoch: 4  loss: 0.356\n",
      "epoch: 5  loss: 0.326\n",
      "epoch: 6  loss: 0.305\n",
      "epoch: 7  loss: 0.288\n",
      "epoch: 8  loss: 0.273\n",
      "epoch: 9  loss: 0.261\n",
      "epoch: 10  loss: 0.248\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.928\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 4가지 경우의 accuracy를 표로 정리하겠습니다.\n",
    "\n",
    "||Sigmoid|ReLU|\n",
    "|:---:|:---:|:---:|\n",
    "|2-layer|0.877|0.917|\n",
    "|3-layer|0.504|0.928|\n",
    "\n",
    "ReLU의 경우 어느 정도 비슷해보이지만 Sigmoid의 경우 accuracy가 거의 절반으로 떨어진 것을 볼 수 있습니다. \n",
    "\n",
    "Sigmoid의 경우 층이 추가되면서 기울기 소실이 일어난 것으로 보입니다. 그에 반해 ReLU는 기울기 소실이 발생하지 않기에 층이 쌓여도 정확도가 떨어지지는 않습니다. \n",
    "\n",
    "만약 activation function이 없다면 가중치와 입력값의 곱으로만 뉴런이 이루어집니다. 이는 non-linear한 데이터를 표현할 수 없음을 의미합니다. 그리고 층을 여러개 쌓일 필요도 없어집니다. 그렇기에 non-linear한 데이터도 잘 표현하기 위해서 activation function은 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습4. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 3에 이어서 이번 실습에선 sgd, momentun, Adam 등의 optimizer를 사용해보고 성능을 비교해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfCTx8xj4zoaombrK2bSN9nv0Z3r95jp)\n",
    "\n",
    "- input: 784\n",
    "- hidden: (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- **optimizer: sgd** or **momentum** or **adam**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 3-layer Network + ReLU + SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.749\n",
      "epoch: 2  loss: 0.618\n",
      "epoch: 3  loss: 0.428\n",
      "epoch: 4  loss: 0.370\n",
      "epoch: 5  loss: 0.338\n",
      "epoch: 6  loss: 0.316\n",
      "epoch: 7  loss: 0.298\n",
      "epoch: 8  loss: 0.282\n",
      "epoch: 9  loss: 0.268\n",
      "epoch: 10  loss: 0.256\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.923\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 3-layer Network + ReLU + Momentum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.99)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.600\n",
      "epoch: 2  loss: 0.255\n",
      "epoch: 3  loss: 0.210\n",
      "epoch: 4  loss: 0.199\n",
      "epoch: 5  loss: 0.165\n",
      "epoch: 6  loss: 0.150\n",
      "epoch: 7  loss: 0.151\n",
      "epoch: 8  loss: 0.149\n",
      "epoch: 9  loss: 0.140\n",
      "epoch: 10  loss: 0.136\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 3-layer Network + ReLU + Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.295\n",
      "epoch: 2  loss: 0.174\n",
      "epoch: 3  loss: 0.156\n",
      "epoch: 4  loss: 0.141\n",
      "epoch: 5  loss: 0.132\n",
      "epoch: 6  loss: 0.128\n",
      "epoch: 7  loss: 0.122\n",
      "epoch: 8  loss: 0.118\n",
      "epoch: 9  loss: 0.116\n",
      "epoch: 10  loss: 0.113\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer에 따른 정확도를 표로 나타내면 다음과 같다.\n",
    "\n",
    "|SGD|Momentum|Adam|\n",
    "|:---:|:---:|:---:|\n",
    "|0.923|0.947|0.958|\n",
    "\n",
    "Adam > Momentum > SGD 순임을 알 수 있습니다. 평균적으로 학습 속도도 정확도가 높을수록 짧습니다. SGD는 현재 위치의 기울기만을 가지고 경사 하강을 하기에 진동이 심합니다. 이에 반해 Momentum은 관성의 개념을 이용하여 진동을 줄이고 더욱 빠르게 저점으로 수렴하게 만듭니다. 그렇기에 SGD보다 Momentum이 더 빠르게 학습합니다. Adam은 Momentum의 방법에 이전 기울기까지 고려하여 이동합니다. 그렇기에 Momentum보다 더 빠르게 수렴하고 학습속도도 더 빠릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습5. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 image data에서 주로 사용되는 batch-normalization까지 추가해보겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xZSWZiSxuGZAsonghidhTSfUEYiuxRtN)\n",
    "\n",
    "- input: 784\n",
    "- hidden: 32 or (32, 32)\n",
    "- output: 10\n",
    "- activation: relu\n",
    "- optimizer: adam\n",
    "- **regularizer: batch_norm**\n",
    "- loss: cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-layer Network + ReLU + Adam + batch_norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                hidden_dim=(32,32), \n",
    "                output_dim=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.260\n",
      "epoch: 2  loss: 0.146\n",
      "epoch: 3  loss: 0.121\n",
      "epoch: 4  loss: 0.101\n",
      "epoch: 5  loss: 0.092\n",
      "epoch: 6  loss: 0.085\n",
      "epoch: 7  loss: 0.080\n",
      "epoch: 8  loss: 0.073\n",
      "epoch: 9  loss: 0.070\n",
      "epoch: 10  loss: 0.068\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.972\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26634"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **결론**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-normalization을 쓴 것과 쓰지 않은 것을 비교해보면 다음과 같습니다.\n",
    "\n",
    "|Batch-normalization O|Batch-normalization X|\n",
    "|:---:|:---:|\n",
    "|0.972|0.958|\n",
    "\n",
    "Batch-normalization을 썼을 때, 정확도가 더 증가한 것을 확인할 수 있습니다. Regularization은 feature들의 영향력을 조절하여 overfitting을 방지하는 역할을 합니다. 우리의 모델은 feature가 많음을 알 수 있습니다. 그렇기에 batch-normalization으로 overfitting을 방지하여 더 일반적이고 성능이 좋은 학습이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Convolution Neural Network(CNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/62306 의 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 합성곱과 풀링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**합성곱 신경망(Convolutional Neural Network)** 은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. 합성곱 신경망은 크게 **합성곱층(Convolution layer)** 과 **풀링층(Pooling layer)** 으로 구성됩니다. 아래의 그림은 합성곱 신경망의 일반적인 예를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/convpooling.PNG\">\n",
    "\n",
    "위 그림에서 CONV는 합성곱 연산을 의미하고, 합성곱 연산의 결과가 활성화 함수 ReLU를 지납니다. 이 두 과정을 합성곱층이라고 합니다. 그 후에 POOL이라는 구간을 지나는데 이는 풀링 연산을 의미하여 풀링층이라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 합성곱 신경망의 대두"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다. 예를 들어, 알파벳 손글씨를 분류하는 어떤 문제가 있습니다. 알파벳 y를 손글씨로 쓴 두 가지 예시를 행렬로 표현한 것이 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv0.png\">\n",
    "\n",
    "사람이 보기에 두 그림은 모두 y로 보이지만 기계가 보기에 각 픽셀마다 가진 값이 거의 상이하므로 완전히 다른 입력으로 받아들입니다. 그리고 두 이미지 외에도 휘어지거나, 이동되거나 방향이 뒤틀리는 등 다양한 변형이 존재합니다. 다층 퍼셉트론은 몇 가지 픽셀만 값이 달라져도 민감하게 받아들이기에 적합하지 않습니다. \n",
    "\n",
    "이를 더 자세히 살펴봅시다. 만약 위 이미지를 다층 퍼셉트론으로 분류한다면 이미지를 1차원 텐서인 벡터로 변환하여 입력층으로 사용해야 합니다. 이를 전환하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv1.png\">\n",
    "\n",
    "1차원으로 변환된 결과는 사람이 보기에 원래 이미지를 유추하기 매우 어렵습니다. 이는 기계 역시 마찬가집니다. 위와 같은 변환은 전에 가지고 있던 공간적인 구조(spatial structure) 정보가 유실된 상태입니다. 그렇기에 다층 퍼셉트론으로 이미지를 분류하기엔 어렵습니다. 그렇기에 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고 이를 위해 사용하는 것이 합성곱 신경망입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 채널(Channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리의 기본적인 용어인 채널에 대해서 간단히 정의하겠습니다.\n",
    "\n",
    "기계는 글자나 이미지보다, 텐서를 더 잘 처리할 수 있습니다. 이미지는 **(높이, 너비, 채널)** 이라는 3차원 텐서입니다. 여기서 높이는 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지는 채널 수가 1이며 각 픽셀은 0부터 255 사이의 값을 가집니다. 아래는 28 x 28 픽셀의 손글씨 데이터를 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv2.png\">\n",
    "\n",
    "위 손글씨 데이터는 흑백 이미지이므로 채널 수가 1입니다. 그렇기에 위 이미지는 (28 x 28 x 1)의 크기를 가지는 3차원 텐서입니다. 만약 컬러 이미지라면 RGB가 각각 채널이 1개씩 총 3개를 가집니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv3.png\">\n",
    "\n",
    "하나의 픽셀은 삼원색의 조합으로 이루어집니다. 만약, 높이와 너비가 28인 컬러 이미지가 있다면 이 이미지의 텐서는 (28 x 28 x 3)의 크기를 가지는 3차원 텐서입니다. 채널은 떄로는 깊이(depth)라고도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 합성곱 연산(Convolution operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출하는 역할을 합니다. 합성곱은 kernel 또는 filter라는 n x m크기의 행렬로 height x width 크기의 이미지를 처음부터 끝까지 겹치고 훓으면서 n x m 크기의 겹쳐지는 부분의 각 이미지와 kernel의 원소의 값을 곱합니다. 그리고 이를 모두 더하여 값으로 출력하는 것을 말합니다. \n",
    "\n",
    "- kernel은 일반적으로 3 x 3, 5 x 5를 많이 사용합니다.\n",
    "\n",
    "예시를 보겠습니다. 아래는 3 x 3크기의 커널로 5 x 5 이미지 행렬에 합성곱 연산을 수행하는 과정을 보여줍니다. 한 번의 연산을 1 step이라고 했을 때, 합성곱 연산의 네번째 스텝까지 이미지와 식으로 보겠습니다. \n",
    "\n",
    "1) 첫번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv4.png\">\n",
    "\n",
    "$(1 \\times 1) + (2 \\times 0) + (3 \\times 1) + (2 \\times 1) + (1 \\times 0)+ (0 \\times 1)+ (3 \\times 0)+ (0 \\times 1) + (1 \\times 0) = 6$\n",
    "\n",
    "\n",
    "2) 두번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv5.png\">\n",
    "\n",
    "$(2 \\times 1) + (3 \\times 0) + (4 \\times 1) + (1 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (0 \\times 0)+ (1 \\times 1) + (1 \\times 0) = 9$\n",
    "\n",
    "\n",
    "3) 세번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv6.png\">\n",
    "\n",
    "$(3 \\times 1) + (4 \\times 0) + (5 \\times 1) + (0 \\times 1) + (1 \\times 0)+ (2 \\times 1)+ (1 \\times 0)+ (1 \\times 1) + (0 \\times 0) = 11$\n",
    "\n",
    "\n",
    "4) 네번째 스텝  \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv7.png\">\n",
    "\n",
    "$(2 \\times 1) + (1 \\times 0) + (0 \\times 1) + (3 \\times 1) + (0 \\times 0)+ (1 \\times 1)+ (1 \\times 0)+ (4 \\times 1) + (1 \\times 0) = 10$\n",
    "\n",
    "\n",
    "총 9번의 스텝을 했을 때 최종 결과는 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv8.png\">\n",
    "\n",
    "위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 **특성 맵(feature map)** 이라고 합니다.\n",
    "\n",
    "위의 예제에선 커널의 크기 3 x 3이었지만, 커널의 크기는 사용자가 지정할 수 있습니다. 또한 커널의 이동 범위가 위의 예제에서는 한 칸이었지만, 이 또한 사용자가 정할 수 있습니다. 이러한 이동 범위를 **스트라이드(stride)** 라고 합니다.\n",
    "\n",
    "아래의 예제는 스트라이드가 2일 때, 5 x 5 이미지에 합성곱 연산을 수행하는 3 x 3 커널의 움직임을 보여줍니다. 최종적으로 2 x 2 크기의 특성 맵을 얻습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 패딩(Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예시처럼 합성곱 연산으로 얻은 특성 맵은 입력보다 크기가 작아지는 특징이 있습니다. 만약, 합성곱 층을 여러개 쌓았다면 최종적으로 얻는 특성 맵의 크기는 입력보다 매우 작아진 상태가 됩니다. 만약 합성곱 연산 이후에도 특성 맵의 크기가 입력 크기와 동일하게 유지되도록 하고 싶다면 패딩(padding)을 사용합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv10.png\">\n",
    "\n",
    "패딩은 합성곱 연산을 하기 전에 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가하는 것을 말합니다. 다시 말해 지정된 개수의 폭만큼 테두리를 추가합니다. 주로 0으로 채우는 제로 패딩(zeor padding)을 사용합니다. 위의 그림은 5 x 5 이미지에 1폭짜리 제로 패딩을 사용한 모습입니다.\n",
    "\n",
    "만약 스트라이드가 1일 때, 3 x 3 크기의 커널을 사용한다면 1폭짜리 제로 패딩을 사용하고, 5 x 5 크기의 커널을 사용한다면 2폭짜리 제로 패딩을 사용하여 크기를 보존할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 가중치와 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 신경망에서의 가중치와 편향을 이해하기 위해 먼저 다층 퍼셉트론을 복습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 합성곱 신경망의 가중치**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 퍼셉트론으로 3 x 3 이미지를 처리한다고 가정하겠습니다. 우선 이미지를 1차원 텐서로 만들면 입력층은 9개의 뉴론을 가집니다. 그리고 4개의 뉴론을 가지는 은닉층을 추가한다면 아래의 그림과 같아집니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv11.png\">\n",
    "\n",
    "위에서 각 연결선은 가중치를 의미하므로 위의 그림에서는 36(=9 x 4)개의 가중치를 가집니다.\n",
    "\n",
    "같은 이미지를 합성곱 신경망으로 처리해보겠습니다. 2 x 2 커널을 사용하고 스트라이드는 1로 하겠습니다. (*는 합성곱 연산을 의미합니다.)\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv12.png\">\n",
    "\n",
    "합성곱 신경망에서 가중치는 커널 행렬의 원소들입니다. 이를 인공 신경망으로 표현하면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv13.png\">\n",
    "\n",
    "최종적으로 특성 맵을 얻기 위해서 동일한 커널로 이미지 전체를 훑으면서 합성곱 연산을 진행합니다. 결국 이미지 전체를 훑으면서 사용되는 가중치는 $w_0, w_1, w_2, w_3$ 4개 뿐입니다. 그리고 각 합성곱 연산마다 이미지의 모든 픽셀을 사용하는 것이 아니라 커널과 맵핑되는 픽셀만을 입력으로 사용하는 것을 볼 수 있습니다. 결국 합성곱 신경망은 다층 퍼셉트론보다 훨씬 적은 가중치를 사용하며 공간적 구조 정보를 보존하는 특징을 가집니다.\n",
    "\n",
    "다층 퍼셉트론의 은닉층에서 가중치 연산 이후, 비선형성을 위해 활성화 함수를 통과시켰듯, 합성곱 신경망에서도 합성곱 연산을 통해 얻은 특성 맵을 활성화 함수를 통과시켜 비선형성을 갖게 만듭니다. 이때 활성화 함수로 ReLU나 그 변형들이 주로 사용됩니다. 이와 같이 합성곱 연산을 통해 특성 맵을 얻고, 활성화 함수를 지나는 연산을 하는 합성곱 신경망의 은닉층을 **합성곱 층(convolution layer)** 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 합성곱 신경망의 편향**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/64066/conv14.png\">\n",
    "\n",
    "합성곱 신경망에도 편향을 추가할 수 있습니다. 만약, 편향을 사용한다면 커널을 적용한 뒤에 더해집니다. 편향은 하나의 값만 존재하며 커널이 적용된 결과의 모든 원소에 더해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 특성 맵의 크기 계산 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산의 notation을 먼저 확인하겠습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $S$: 스트라이드\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "\n",
    "이에 따라 특성 맵의 높이와 너비는 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w}{S} + 1)$$\n",
    "\n",
    "여기서 $floor$ 함수는 소수점 발생 시, 소수점 이하를 버리는 역할을 합니다. \n",
    "\n",
    "예를 들어 5 x 5 크기의 이미지에 3 x 3 커널을 사용하고 스트라이드 1로 합성곱 연산을 한 경우, 특성 맵의 크기는 (5 - 3 + 1) x (5 - 3 + 1) = 3 x 3임을 알 수 있습니다. 이는 또한 9번의 스텝이 필요함을 의미하기도 합니다.\n",
    "\n",
    "패딩의 폭을 $P$라고 할=고, 패딩까지 고려한 식은 다음과 같습니다.\n",
    "\n",
    "$$O_h = floor(\\frac{I_h - K_h + 2P}{S} + 1)$$\n",
    "\n",
    "$$O_w = floor(\\frac{I_w - K_w + 2P}{S} + 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 채널 또는 깊이를 고려하지 않고, 2차원 텐서를 가정하고 설명했습니다. 하지만 실제로 합성곱 연산의 입력은 '다수의 채널을 가진' 이미지 또는 이전 연산의 결과로 나온 특성 맵일 수 있습니다. 만약, 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다고 하면 커널의 채널 수도 입력의 채널 수만큼 존재해야 합니다. 다시 말해 입력 데이터의 채널 수와 커널의 채널 수는 같아야 합니다. 채널 수가 같으므로 합성곱 연산을 채널마다 수행합니다. 그리고 그 결과를 모두 더하여 최종 특성 맵을 얻습니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv15.png\">\n",
    "\n",
    "위 그림은 3개의 채널을 가진 입력 데이터와 3개의 채널을 가진 커널의 합성곱 연산을 보여줍니다. 커널의 각 채널끼리의 크기는 같아야 합니다. 각 채널간 합성곱 연산을 마치고, 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵을 만듭니다. 주의할 점은 위의 연산에서 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 것입니다.\n",
    "\n",
    "위 그림은 높이 3, 너비 3, 채널 3의 입력이 높이 2, 너비 2, 채널 3의 커널과 합성곱 연산을 하여 높이 2, 너비 2, 채널 1의 특성 맵을 얻는다는 의미입니다. 합성곱 연산의 결과로 얻은 특성 맵의 태널 차원은 RGB 채널 등과 같은 컬러의 의미를 담고 있지 않습니다.\n",
    "\n",
    "이제 이 연산에서 각 차원을 변수로 두고 좀 더 일반화시켜보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 3차원 텐서의 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화를 위해 사용하는 변수들은 다음과 같습니다.\n",
    "\n",
    "- $I_h$: 입력의 높이\n",
    "- $I_w$: 입력의 너비\n",
    "- $K_h$: 커널의 높이\n",
    "- $K_w$: 커널의 너비\n",
    "- $O_h$: 특성 맵의 높이\n",
    "- $O_w$: 특성 맵의 너비\n",
    "- $C_i$: 입력 데이터의 채널\n",
    "\n",
    "다음은 3차원 텐서의 합성곱 연산을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv16_final.png\">\n",
    "\n",
    "높이 $I_h$, 너비 $I_w$, 채널 $C_i$의 입력 데이터는 동일한 채널 수 $C_i$를 가지는 높이 $K_h$, 너비 $K_w$의 커널과 합성곱 연산을 하여 높이 $O_h$, 너비 $O_w$, 채널 1의 특성 맵을 얻습니다. 그런데 하나의 입력에 여러 개의 커널을 사용하는 합성곱 연산을 할 수도 있습니다. \n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 특성 맵의 크기가 어떻게 바뀌는지 봅시다. 다음은 $C_o$를 합성곱 연산에 사용하는 커널의 수라고 했을 때의 합성곱 연산 과정을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/64066/conv17_final_final.PNG\">\n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 사용한 커널 수는 합성곱 연산의 결과로 나오는 특성 맵의 채널 수가 됩니다.\n",
    "\n",
    "이를 이해했다면 커널의 크기와 입력 데이터의 채널 수 $C_i$와 특성 맵(출력 데이터)의 채널 수 $C_o$가 주어졌을 때, 가중치 매개변수의 총 개수를 구할 수 있습니다. 가중치는 커널의 원소들이므로 하나의 커널의 하나의 채널은 $K_i \\times K_o$개의 매개변수를 가지고 있습니다. 그런데 합성곱 연산을 하려면 커널은 입력 데이터의 채널 수와 동일한 채널 수를 가져야 합니다. 이에 따라 하나의 커널이 가지는 매개변수의 수는 $K_i \\times K_o \\times C_i$입니다. 그런데 이러한 커널이 총 $C_o$개가 있어야 하므로 가중치 매개변수의 총 수는 다음과 같습니다.\n",
    "\n",
    "가중치 매개변수의 총 수: $K_i \\times K_o \\times C_i \\times C_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 풀링(Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적입니다. 풀링 층에서는 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어집니다. 풀링 연산에는 일반적으로 최대 풀링(max pooling)과 평균 풀링(average pooling)이 사용됩니다. 우선 최대 풀링을 통해 풀링 연산을 보겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/62306/maxpooling.PNG\">\n",
    "\n",
    "풀링 연산에서도 합성곱 연산과 마찬가지로 커널과 스트라이드의 개념을 가집니다. 위의 그림은 스트라이드가 2일 때, 2 x 2 크기 커널로 맥스 풀링 연산과정입니다. 특성맵이 절반의 크기로 다운샘플링되는 것을 볼 수 있습니다. 맥스풀링은 커널과 겹치는 영역 안에서 최대값을 추출하는 방식으로 다운샘플링합니다.\n",
    "\n",
    "평균 풀링은 최대값대신 평균값을 추출하는 연산이 됩니다. 풀링 연산은 커널가 스트라이드 개념이 존재한다는 점에서 합성곱 연산과 유사하지만 합성곱 연산과 차이점은 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 점입니다. \n",
    "\n",
    "풀링을 사용하면, 특성맵의 크기가 줄어드므로 특성맵의 가중치 개수를 줄여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. Fully-Connected Layer vs Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1의 실습들을 통해 model의 다양한 node를 바꿔가며 mnist의 성능 변화를 확인해보았습니다. 비록, fully-connected network가 mnist 데이터에서 높은 성능을 내는데 문제가 없었지만, 모든 layer를 fully-connected layer로 만드는 것은 엄청난 파라미터와 연산량을 필요로 합니다. 그렇기에 큰 고화질의 이미지 데이터를 처리하는데 적합합지 않습니다.\n",
    "\n",
    "따라서, 이번 실습은 이미지 데이터 처리에 주로 사용되는 convolution layer를 사용하여 파라미터 수와 성능이 어떻게 변화하는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1xdjTf4ab0P8qfu_TaLJ4TZzt5sk3twS6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(8, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conv()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.204\n",
      "epoch: 2  loss: 0.098\n",
      "epoch: 3  loss: 0.089\n",
      "epoch: 4  loss: 0.086\n",
      "epoch: 5  loss: 0.078\n",
      "epoch: 6  loss: 0.081\n",
      "epoch: 7  loss: 0.079\n",
      "epoch: 8  loss: 0.076\n",
      "epoch: 9  loss: 0.079\n",
      "epoch: 10  loss: 0.074\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.973\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4274"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|구분|Fully-connected layer|Covolution operation|\n",
    "|:---:|:---:|:---:|\n",
    "|정확도|0.972|0.973|\n",
    "|parameter 개수|26634|4274|\n",
    "\n",
    "챕터1에서 0.972가 나온 것을 보면 0.001의 차이는 크진 않습니다. 그러나 parameter의 개수가 6배 정도 차이납니다. 이는 연산량을 줄일 수 있고 큰 데이터를 다루기에 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습2. 나만의 model 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "챕터1, 2의 실습을 바탕으로 20,000개 이하의 파라미터를 가지며 98%이상의 정확도를 갖는 모델을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim=784, \n",
    "                output_dim=10):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=4,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4,\n",
    "                            out_channels=8,\n",
    "                            kernel_size=7,\n",
    "                            stride=2)\n",
    "        self.fc = nn.Linear(3*3*8, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # should reshape data into image\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 3*3*8)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel()\n",
    "if count_parameters(model) > 20000:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv2): Conv2d(4, 8, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (fc): Linear(in_features=72, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.240\n",
      "epoch: 2  loss: 0.112\n",
      "epoch: 3  loss: 0.099\n",
      "epoch: 4  loss: 0.093\n",
      "epoch: 5  loss: 0.094\n",
      "epoch: 6  loss: 0.093\n",
      "epoch: 7  loss: 0.090\n",
      "epoch: 8  loss: 0.085\n",
      "epoch: 9  loss: 0.084\n",
      "epoch: 10  loss: 0.086\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_dataloader,\n",
    "                testloader = test_dataloader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.975\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습3. Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Convolution Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서는 [nn.sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) 등의 함수를 이용해서 복잡한 모델 구조를 종종 축약해서 사용하곤 합니다.\n",
    "\n",
    "아래 예제는 conv-relu-maxpool의 model을 서로 다른 방법으로 표현한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = torch.rand(64, 3, 32, 32)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **nn.Sequential**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv1, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv2, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv3, self).__init__()\n",
    "        layer = []\n",
    "        layer.append(nn.Conv2d(3, 64, kernel_size=3, padding=1))\n",
    "        layer.append(nn.ReLU())\n",
    "        layer.append(nn.MaxPool2d(2))\n",
    "        self.layer = nn.Sequential(*layer)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model1 = Conv1()\n",
    "model2 = Conv2()\n",
    "model3 = Conv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "output = model1(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2(\n",
      "  (layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model2)\n",
    "output = model2(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv3(\n",
      "  (layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(model3)\n",
    "output = model3(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Let's practice to calculate the shape of the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self): # input image = batch_size x 3 x 32 x 32\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        # input_size, output_size, kerner, stride, padding\n",
    "        self.conv1 = nn.Conv2d(3, 512, 3, 1, 1)   # -> batch_size x 512 x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(512, 256, 3, 1, 1) # -> batch_size x 256 x 32 x 32\n",
    "        self.conv3 = nn.Conv2d(256, 256, 3, 2, 1) # -> batch_size x 256 x 16 x 16\n",
    "        self.conv4 = nn.Conv2d(256, 256, 3, 4, 1) # -> batch_size x 256 x 4 x 4\n",
    "        self.linear = nn.Linear(256*4*4, 10)      # -> batch_size x 10\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = out.contiguous().view(-1, 256*4*4)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv(\n",
      "  (conv1): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1))\n",
      "  (linear): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = Conv()\n",
    "print(model)\n",
    "output = model(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. CNN - Case Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 CNN은 다양한 모델들이 있습니다. 그리고 이 모델을 바로 사용할 수 있습니다. 예를 들어 고양이와 강아지를 분류하는 모델이 있다면 FC층 전까지 학습된 layer를 가져오고 FC층만 이어붙여서 다른 분류 모델로 만들어 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 LeNet-5 [LeCun et al., 1998]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lenet](_image/lenet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv: 5 x 5 filter, stride = 1\n",
    "- Subsampling(Pooling) layers: 2 x 2, stride = 2\n",
    "- [CONV - POOL - CONV - POOL - FC - FC]  \n",
    "    * FC층에 돌입하기 전 1줄로 reshape를 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 AlexNet [Krizhevsky et al., 2012]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alexnet](_image/alexnet.PNG)\n",
    "\n",
    "위 모형에서 위쪽 그림이 잘려 있는데 이는 아래와 동일한 동작을 하기에 원저자가 위를 잘랐습니다. \n",
    "\n",
    "위에서 CONV1은 96개의 filter를 가지며 크기는 11 x 11 x 3입니다. 이렇게 다른 layer들의 정보도 확인할 수 있습니다. \n",
    "\n",
    "또한 오른쪽 learning rate는 초기값은 1e-2로 설정하고 val accuracy가 더이상 증가하지 않으면 10배씩 줄여서 진행한다는 의미입니다. 그리고 7 CNN ensemble의 %는 오차율로 줄어든 것을 확인할 수 있습니다. 다른 detail들은 읽어보면 알 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 VGGNet [Simonyan and Zisserman, 2014]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vggnet](_image/vggnet.PNG)\n",
    "\n",
    "VGGNet은 Alexnet보다 작은 필터, 깊은 network를 사용합니다. 위 그림을 보면 layer가 8개에서 16~19개로 늘어난 것을 확인할 수 있습니다. layer를 16개 갖는 것을 VGG16, 19개를 가지면 VGG19라고 합니다.\n",
    "\n",
    "그렇다면 왜 작은 필터를 사용했을까요??\n",
    "\n",
    "예를 들어 7 x 7 fliter로 100 x 100 x 64의 input을 처리한다고 해봅시다. 그렇다면 이때 필요한 parameter의 수는 \n",
    "\n",
    "$$(filter안의\\;parameter의 수) \\times (input의\\;depth) \\times (filter의\\;개수)$$\n",
    "\n",
    "입니다. 우리는 output과 input의 크기를 동일하게 만들 것이기에 fliter는 depth와 같이 54개입니다. 그렇기에 parameter는 총 $7 \\times 7 \\times 64 \\times 64$개가 필요합니다.\n",
    "\n",
    "이제 이 과정을 3 x 3 filter로 해보겠습니다. 3 x 3 filter로 7 x 7과 같은 동작을 해야하므로 layer가 더 필요합니다. 그렇기에 parameter의 수는\n",
    "\n",
    "$$(filter안의\\;parameter의\\;수) \\times (input의\\;depth) \\times (filter의\\;개수) \\times (7 \\times 7과\\;동일한\\;역할을\\;위해\\;필요한\\;layer의\\;수)$$\n",
    "\n",
    "입니다. 이를 대입해보면 $3 \\times 3 \\times 64 \\times 64 \\times 3$으로 7 x 7의 filter를 이용하는 것보다 parameter의 개수가 줄어든 것을 확인할 수 있습니다. 또한 layer는 증가해서 non-linear 층이 더 많아졌기에 non-linear한 표현을 더 잘할 수 있게 됩니다. \n",
    "\n",
    "![vggnet2](_image/vggnet2.PNG)\n",
    "\n",
    "전체 동작을 보면 위와 같습니다. 이때 CONV3은 filter의 크기가 3 x 3임을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 GoogLeNet [Szegedy et al., 2014]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet은 layer의 개수를 더 늘려서 22개를 사용한 model입니다. 특별히 Inception module을 사용합니다. 먼저 inception module을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inception Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception](_image/inception1.PNG)\n",
    "\n",
    "위 사진처럼 input을 여러 개의 conv를 거치고 그것을 다시 합치는 방법입니다. 이는 feature들을 효율적으로 추출할 수 있도록 만들어줍니다. 여기서 pooling은 padding도 함께 해서 크기를 유지시켜줍니다. 그러나 이는 연산량이 굉장히 늘어납니다. 이를 위해 1 x 1 Convolution을 도입합니다.\n",
    "\n",
    "![1x1conv](_image/1x1conv.PNG)\n",
    "\n",
    "위에서 볼 수 있듯이 56 x 56 x 64 크기의 input을 1 x 1 conv의 filter 개수로 depth를 바꿔주는 것을 볼 수 있습니다. 즉, dimension reduction을 할 수 있게 됩니다. 그렇기에 이를 바탕으로 차원을 줄여 연산량을 줄이고 다시 차원을 늘려 과정을 진행할 수 있습니다. 그렇기에 inception module을 수정하여 밑의 그림처럼 사용합니다.\n",
    "\n",
    "![inception2](_image/inception2.PNG)\n",
    "\n",
    "Feature들을 효율적으로 추출하면서 연산량을 줄일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 inception module을 이용한 GoogLeNet의 structure를 살펴보겠습니다. \n",
    "\n",
    "![googlenet](_image/googlenet.PNG)\n",
    "\n",
    "층이 깊게 쌓여있는 것을 볼 수 있습니다. 그리고 inception module이 쭉 이어져 있는 것을 볼 수 있습니다. \n",
    "\n",
    "여기서 눈에 띄는 두 부분이 있습니다. 가지를 치고 나온 두 부분은 softmax, 즉 output을 출력하는 곳입니다. 이는 실제 결과값에 반영하는 곳은 아닙니다. 이는 신경망이 깊어질 때 발생하는 기울기 소실 문제를 해결하기 위해 중간 층에서도 backprop을 수행하면서 가중치 갱신을 시도한다. 이는 왼쪽으로 계속 backprop되도 가중치가 더 쉽게 갱신됩니다. \n",
    "\n",
    "이는 학습 시에만 사용하고 추론 시에는 이 부분은 사용하지 않습니다.\n",
    "\n",
    "정리하자면 22개의 layer를 사용하며 그 layer는 inception module을 활용합니다. parameter의 개수가 극적으로 줄어들지만 inception module 자체가 어렵다는 특징도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 ResNet [He et al., 2015]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet을 알아보기 전에 먼저 망을 깊게 팔 경우 어떤 문제가 생기는지 확인하겠습니다. \n",
    "\n",
    "![deeprob](_image/deepprob.PNG)\n",
    "\n",
    "위 그래프는 각각 20개, 56개의 layer를 가졌을 때 성능을 보여줍니다. 보면 알 수 있듯이 56개의 layer를 가진 모델이 20개의 layer를 가진 모델보다 성능이 안 좋은 것을 확인할 수 있습니다. 이는 overfitting 때문에 생긴 성능 저하는 아닙니다. \n",
    "\n",
    "이 문제에 대해 전문가들은 Optimizer가 깊은 신경망을 최적화하기엔 너무나 어렵기에 모델 성능이 안 좋아진다는 가설을 세웁니다. 그리고 적어도 얕은 모델의 성능만큼 깊은 모델도 성능이 나와야 한다는 조건으로 더 얕은 모델에서 학습된 레이어를 복사하여 학습된 모델과 함께 사용하는 방법을 구현하게 되었습니다. \n",
    "\n",
    "학습을 직접시도하는 대신 이전 복사된 학습을 이용하여 현재 학습 결과를 구하는 것입니다. 밑의 그림이 이 과정을 보여줍니다. 연결할 때는 덧셈을 사용합니다.\n",
    "\n",
    "![resnet](_image/resnet.PNG)\n",
    "\n",
    "이 과정을 Skip Connection이라고 합니다. 이제 ResNet의 전체 과정을 보면 다음과 같습니다.\n",
    "\n",
    "![resnet2](_image/resnet2.PNG)\n",
    "\n",
    "FC층은 output을 위한 용도 하나만 사용하며 conv는 모두 3 x 3을 사용합니다. 만약 50층 이상으로 깊어진다면 conv를 수행할 때, 밑의 그림처럼 앞서 GoogLeNet에서 사용한 bottleneck을 사용하여 연산을 간단하게 해줍니다.\n",
    "\n",
    "![resnet3](_image/resnet3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 DenseNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet은 모든 레이어의 feature map을 연결하는 방식의 신경망입니다. 이전 레이어의 feature map을 그 이후 모든 레이어의 feature map에 연결합니다. 연결할 때는 덧셈이 아닌 concatenate(연결)을 수행합니다. 따라서 연결하는 feature map의 크기는 동일해야 합니다. \n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F68j0G%2Fbtq0jOzmtVf%2FN9VjOkQErvexl7CvMS5ejK%2Fimg.png\">\n",
    "\n",
    "모든 레이어와 연결되었기에 기울기 소실 문제를 자연스레 해결할 수 있습니다. 또한 concatenate를 연결 연산으로 사용하기에 적은 depth(채널 수)를 사용합니다. 이는 파라미터수와 연산을 줄이게 됩니다. \n",
    "\n",
    "DenseNet 역시 연결 연산을 위해 크기가 같아야 하는데 pooling 연산은 크기를 감소시킵니다. 그렇기에 크기를 유지하면서 pooling을 수행할 수 있게하는 Dense Block을 사용합니다. Denseblock 사이에 pooling 연산을 수행하며 pooling 연산은 BN, 1 x 1 conv, 2 x 2 avg pool로 수행합니다. \n",
    "\n",
    "'Densely Connected Convolutional Networks'논문에서는 transition layer(pooling 연산)에 theta라는 하이퍼파라미터를 곱해줍니다. 이는 transition layer의 입력값 채널 수 m가 곱해져 theta x m 개의 채널수를 출력합니다. 논문에선 theta를 0.5로 설정하여 transition layer의 output의 채널 수를 0.5m으로 합니다. 즉, feature map의 크기와 채널 수를 감소시킵니다.\n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcRv7cz%2Fbtq0n9WUSCS%2FXHPMVeH4hP6jmv2UXnKdQk%2Fimg.png\">\n",
    "\n",
    "링크: https://deep-learning-study.tistory.com/528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. VGG Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 3 x 3 filter로 이루어진 VGG-19 Network를 직접 구현해보겠습니다. \n",
    "\n",
    "자세한 모델의 configuration은 아래 그림의 option E와 같습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mFwqxP5rB4lhEfRk7OJla1wffKqEibQy)\n",
    "\n",
    "VGG-19는 2개의 layer로 구성된 2개의 convolution block과 4개의 layer로 구성된 3개의 convoultion block으로 나누어져 있습니다.\n",
    "\n",
    "또한, 학습 안전성을 위하여 각 layer는 convolution - batchnorm - relu로 이루어지며 매 block에 끝마다 2 x 2 maxpooling을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) VGG-19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TwoLayerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FourLayerBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.output_dim, self.output_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        \n",
    "        self.block1 = TwoLayerBlock(3, 64) # [batch_size, 3, 32, 32] -> [batch_size, 64, 16, 16]\n",
    "        self.block2 = TwoLayerBlock(64, 128) # [batch_size, 64, 16, 16] -> [batch_size, 128, 8, 8]\n",
    "        self.block3 = FourLayerBlock(128, 256) # [batch_size, 128, 8, 8] -> [batch_size, 256, 4, 4]\n",
    "        self.block4 = FourLayerBlock(256, 512) # [batch_size, 256, 4, 4] -> [batch_size, 512, 2, 2]\n",
    "        self.block5 = FourLayerBlock(512, 512) # [batch_size, 512, 2, 2] -> [batch_size, 512, 1, 1]\n",
    "        \n",
    "        # squeeze로 1인 차원 제거 / [batch_size, 512, 1, 1] -> [batch_size, 512]\n",
    "        self.linear = nn.Sequential( # [batch_size, 512] -> [batch_size, 10]\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x) \n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = x.squeeze() # 차원이 1인 차원을 제거\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.rand(64, 3, 32, 32)\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = VGG19()\n",
    "#print(model)\n",
    "\n",
    "output = model(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if count_parameters(model) == 20365002:\n",
    "    print(\"success!\")\n",
    "else:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습2. Train CIFAR-10 with own VGG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train image: 50,000\n",
    "- test image: 10,000\n",
    "- class: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
    "\n",
    "data는 [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)에서 받아주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Use pre-defined dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTroch는 custom dataset과 dataloader를 사용해도 되지만 CIFAR-10과 같은 유명 데이터셋에 대해서는 pre-defined된 dataset이 존재합니다. \n",
    "\n",
    "이번 실습에선 custom dataset을 직접 만드는 대신 pre-trained dataset을 불러와서 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data/',      \n",
    "                                train=True, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data/',\n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        model: model to train\n",
    "        optimizer: optimizer to update your model\n",
    "        criterion: loss function\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epoch = 1):\n",
    "        self.model.train()\n",
    "        loss_list = []\n",
    "        acc_list = []\n",
    "        for e in range(epoch):\n",
    "            running_loss, running_acc = 0.0, 0.0\n",
    "            for i, data in tqdm.tqdm(enumerate(self.trainloader, 0)): \n",
    "                inputs, labels = data \n",
    "                # model에 input으로 tensor를 gpu-device로 보낸다\n",
    "                inputs = inputs.to(self.device)  \n",
    "                labels = labels.to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()    \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs) \n",
    "                loss = self.criterion(outputs, labels)  \n",
    "                loss.backward() \n",
    "                self.optimizer.step() \n",
    "                running_loss += loss.item()\n",
    "                pred = outputs.max(1, keepdim=True)[1]\n",
    "                running_acc += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "            running_loss = running_loss / len(self.trainloader)\n",
    "            running_acc = running_acc / len(self.trainloader.dataset)\n",
    "            loss_list.append(running_loss)\n",
    "            acc_list.append(running_acc)\n",
    "            print('epoch: %d  loss: %.3f  acc:%.3f' % (e + 1, running_loss, running_acc))\n",
    "            \n",
    "        return loss_list, acc_list\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval() \n",
    "        correct = 0\n",
    "        for inputs, labels in self.testloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            output = self.model(inputs) \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        test_acc = correct / len(self.testloader.dataset)\n",
    "        print('test_acc: %.3f' %(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG19(\n",
       "  (block1): TwoLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): TwoLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block5): FourLayerBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU()\n",
       "      (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG19()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:26,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.295  acc:0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2  loss: 0.842  acc:0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [41:52,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3  loss: 0.636  acc:0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [44:36,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4  loss: 0.507  acc:0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [26:35,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5  loss: 0.405  acc:0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [27:31,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6  loss: 0.321  acc:0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [29:01,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7  loss: 0.255  acc:0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [31:17,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8  loss: 0.208  acc:0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [35:16,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9  loss: 0.169  acc:0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [29:24,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10  loss: 0.144  acc:0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_loader,\n",
    "                testloader = test_loader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "loss_list, acc_list = trainer.train(epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(x, loss_list)\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_title('loss_plot')\n",
    "\n",
    "axs[1].plot(x, acc_list)\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_title('acc_plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.816\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 test 정확도가 train 정확도에 비해 현저히 낮은 성능을 보입니다. 왜냐하면 overfitting이 일어나기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Train CIFAR-10 with pre-trained VGG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 위에서 만든 VGG Network를 통해 유의미한 정확도를 달성할 수 있습니다. 그러나 더 큰 모델을 사용할 경우, 매번 처음부터 학습을 진행하는 것은 쉬운 일이 아닙니다.\n",
    "\n",
    "따라서, 이번엔 pre-trained된 VGG model을 불러와서 linear layer만 추가적으로 학습시켜 적은 시간만으로 높은 성능을 달성하는 실습을 해보겠습니다.\n",
    "\n",
    "PyTorch에서 공식적으로 제공하는 pre-trained model은 [Torchvision.models](https://pytorch.org/vision/stable/models.html)에 있는 해당 document에서 찾아볼 수 있습니다. 또는 종종 google이나 facebook 등에서는 자신들이 개발한 모델에 대해서 pre-trained된 weight를 이와 같이 배포합니다. \n",
    "\n",
    "[예시](https://github.com/google-research/simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to C:\\Users\\delphinus/.cache\\torch\\hub\\checkpoints\\vgg19_bn-c79401a0.pth\n",
      "100%|██████████| 548M/548M [00:52<00:00, 10.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "pretrained_vgg = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrained_VGG19(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Pretrained_VGG19, self).__init__()\n",
    "        # inherit the weights from the pre-trained model\n",
    "        self.features = nn.Sequential(\n",
    "            *list(pretrained_model.features.children())\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze() \n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pretrained_VGG19(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pretrained_VGG19(pretrained_vgg)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [27:39,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.742  acc:0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(trainloader = train_loader,\n",
    "                testloader = test_loader,\n",
    "                model = model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                device = device)\n",
    "\n",
    "loss_list, acc_list = trainer.train(epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.863\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Recurrent Neural Networks(RNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks(RNN)은 CNN과 다르게 이전 출력값을 다시 입력값으로 넣으며 사용하는 순환 신경망입니다. \n",
    "\n",
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" height = \"200px\" width = \"700px\">\n",
    "\n",
    "위 그림처럼 반복하는 것을 시간으로 나누어 나타낼 수 있습니다. 이러한 RNN의 은닉층 계산은 다음과 같이 진행됩니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22886/rnn_images4-5.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$h_t = tanh(W_x X_t + W_h h_{t-1} + b)$$\n",
    "\n",
    "$$y_t = W_y h_t$$\n",
    "\n",
    "이때 RNN은 zero-centered, 즉 평균이 0이여야 하기 때문에 tanh 함수를 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"hello\"라는 단어를 생성할 수 있도록 RNN을 학습시킨다고 가정할 때, 동작 과정은 다음과 같습니다.\n",
    "\n",
    "![hello](_image/hello.PNG)\n",
    "\n",
    "이때, 처음 시작할 때 h_0이 들어가야 할 텐데 대체적으로 가우시안분포로 무작위 설정을 합니다. 이때 h_0를 매 epoch마다 초기화할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Backpropagation throught Time (BPTT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 학습시킬 때 loss를 구하기 위해선 시간을 거슬러 올라가며 loss를 구해야 합니다. 이때 시간 방향으로 펼친 신경망의 Backpropagation이므로 이를 BPTT라고 합니다. \n",
    "\n",
    "![bptt](_image/bptt.PNG)\n",
    "\n",
    "BPTT를 이용하면 RNN을 학습할 수 있습니다. 그러나 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 메모리도 증가합니다. 또한 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 문제도 발생합니다. \n",
    "\n",
    "그렇기에 큰 시계열 데이터를 취급할 때는 신경망을 적당한 길이로 끊습니다. 이를 **Truncated BPTT** 라고 합니다. 이는 잘라낸 신경망끼리 독립적으로 backpropagation을 실행합니다. 물론 순전파의 연결은 끊으면 안되고 역전파의 연결만 끊는 것임을 기억해야 합니다. \n",
    "\n",
    "![truncated bptt](_image/truncated_bptt.PNG)\n",
    "\n",
    "BPTT는 RNN을 학습하는 방법이지만 기울기 소실이나 기울기 폭발 문제가 있습니다. BPTT는 $h_t= tanh(W_x X_t + W_h h_{t-1} + b)$를 역전파합니다. 덧셈은 기울기를 흘려보내기에 곰셉에 의해서 기울기는 전달됩니다. 이떄 곱셈의 역전파는 $W_h^T$가 계속해서 곱해집니다. 그렇기에 $W_h^T$가 1보다 크면 폭발하고 1보다 작으면 소실됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Gradient Clipping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 폭발 대책은 **기울기 클리핑(Gradient Clipping)** 을 사용합니다. \n",
    "\n",
    "$$if \\; \\lVert \\hat{g} \\rVert \\geq threshold \\; \\rightarrow \\; \\hat{g} = \\frac{threshold}{\\lVert \\hat{g} \\rVert} \\hat{g} $$\n",
    "\n",
    "threshold란 문턱값을 정하고 신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다 가정하고 이를 $\\hat{g}$라고 썼습니다. 이때 기울기의 크기가 threshold를 넘어가면 오른쪽 항처럼 수정합니다. 간단하지만 많은 곳에서 잘 작동됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 LSTM(Long Short-Term Memory)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 소실 문제는 기울기 폭팔처럼 간단하게 해결되지 않습니다. 이를 해결하기 위해 RNN에 게이트를 추가하여 기울기 소실을 방지합니다. 먼저 게이트를 추가한 RNN 중 가장 대표적인 LSTM을 알아보겠습니다.\n",
    "\n",
    "<center><img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" height = \"300px\" width = \"700px\"></center>\n",
    "\n",
    "### <center> <**Standard RNN**> </center>\n",
    "\n",
    "<center><img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" height = \"300px\" width = \"700px\"></center>\n",
    "\n",
    "### <center><**LSTM**></center>\n",
    "\n",
    "위 두 그림은 일반적인 RNN과 LSTM을 나타냅니다. 내부 상황을 보지 않고 입출력만 봤을 때, LSTM은 입력이 하나 더 있음을 볼 수 있습니다. 그 입력은 출력되진 않으며 다음 layer로 넘어갑니다. 이를 $c$(memory cell, 기억셀)이라 하며, LSTM의 기억 매커니즘입니다. $c_t$는 시각 t에서의 LSTM의 기억이 저장되어있습니다. 이를 이용하여 기울기 소실을 방지합니다. 이제 자세한 동작을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Forget Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "Forget gate는 이전 레이어에서 넘어온 기억셀에서 불필요한 기억을 지워주는 게이트입니다. 위 $\\sigma$는 시그모이드 함수를 의미합니다. 시그모이드 함수를 지나면 0과 1 사이의 값이 나오는데 0에 가까울수록 정보가 많이 삭제된 것이고 1에 가까울수록 정보를 보존한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Input Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "입력된 정보들을 기억셀에 새로 추가해야 합니다. 그러기 위해서 입력된 값에 tanh를 사용합니다. 이때, 모든 정보를 무비판적으로 다 수용하는 것이 아닌, 적절히 골라서 입력하도록 input gate가 동작합니다. 역시 시그모이드 함수를 지나는 값은 0 ~ 1, tanh 함수를 지나는 값은 -1 ~ 1 사이의 값이 나옵니다. 이를 가지고 선택된 기억할 정보의 양을 정하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) New Memory Cell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "forget gate와 input gate로 얻은 정보를 이용해 기억셀을 갱신합니다. 기억셀 중 한 줄기는 다음 레이어의 입력값으로 사용됩니다. \n",
    "\n",
    "forget gate와 input gate의 영향력이 여기서 드러납니다. 만약 forget gate의 출력값 $f_t$가 0이라면, 이전 시점의 셀 상태 $C_{t-1}$은 현재 시점의 $C_t$를 결정하기 위한 영향력이 0이 되면서 오직 input gate의 결과만이 $C_t$를 결정하게 됩니다.\n",
    "\n",
    "반대로 input gate의 값인 $i_t$가 0이 된다면 $C_t$는 오직 이전 시점의 $C_{t-1}$의 값에 의해서만 결정됩니다. 이는 input gate를 완전히 닫고 forget gate만 연 상태를 의미합니다.\n",
    "\n",
    "결과적으로 forget gate는 이전 시점의 입력을 얼마나 반영할지 의미하고, input gate는 현재 시점의 입력을 얼마나 반영할지 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Output Gate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "Output gate는 먼저 얼마나 다음으로 흘려보낼지 $h_{t-1}$과 $x_t$로 결정합니다. 그리고 이 비율과 갱신한 기억셀을 이용해 output을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM이 기울기 소실을 방지하는 이유는 기억셀의 역전파에 주목해야 알 수 있습니다. 기억셀의 역전파는 곱셈과 덧셈만 있습니다. 덧셈은 기울기를 흘려보내고 곱셈은 원소별 곱으로 이루어져 있습니다. RNN은 역전파에서 계속 같은 가중치 행렬을 사용하여 행렬 곱을 사용했지만 원소별 곱셈을 사용하는 LSTM은 항상 곱해지는 값이 달라 기울기 소실을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 GRU(Gated Recurrent Unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" height = \"300px\" width = \"1000px\">\n",
    "\n",
    "\n",
    "LSTM과 비슷한 생각으로 구현된 다른 방지 방법도 존재합니다. 그러나 GRU는 기억셀이 없습니다. 그리고 reset과 update라는 두 개의 게이트만 사용합니다. \n",
    "\n",
    "reset 게이트 $r$은 이전 은닉 상태를 얼마나 무시할지 결정합니다. 만약 $r$이 0이면 새로운 은닉 상태는 입력 $x_i$만으로 결정됩니다. 즉, 0이면 과거 상태는 완전히 무시하게 됩니다.\n",
    "\n",
    "update 게이트 $z$는 은닉 상태를 갱신하는 게이트입니다. LSTM의 forget 게이트, input 게이트가 담당하는 것을 혼자 담당하는 것입니다. $(1-z_t) * h_{t-1}$ 부분이 forget 게이트의 역할을 합니다. 이 계산으로 인해 과거의 은닉 상태에서 잊어야 할 정보를 삭제합니다. 그리고 $z * \\tilde{h}$ 부분이 input 게이트 기능을 하여 새로 추가된 정보에 가중치를 부여하게됩니다. \n",
    "\n",
    "이처럼 GRU는 LSTM을 더 단순하게 만든 것입니다. 따라서 LTSM보다 계산 비용을 줄이고 매개변수 개수도 줄일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6 seq2seq** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq(Sequence-to-Sequence)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에 사용되는 모델입니다. 챗봇, 기계 번역 등이 대표적인 예이며 각각 질문과 대답, 입력 문장과 번역 문장을 사용합니다.  \n",
    "\n",
    "그 중 번역기에 사용되는 구동에 대해 알아보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/24996/%EC%8B%9C%ED%80%80%EC%8A%A4%ED%88%AC%EC%8B%9C%ED%80%80%EC%8A%A4.PNG\">\n",
    "\n",
    "위 그림은 seq2seq 모델로 만들어진 번역기가 'I am a student'라는 영어 문장을 입력 받아서, 'je suis étudiant'라는 프랑스 문장을 출력하는 모습을 보여줍니다. 그렇다면 모델 내부의 모습은 어떻게 구성되었을까요??\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG\">\n",
    "\n",
    "seq2seq는 크게 인토더와 디코더, 두 개의 모듈로 구성됩니다. 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤, 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 컨텍스트 벡터(context vector)라고 합니다. 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다. 인코더와 디코더의 내부를 더 자세히 살펴보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더와 디코더의 내부는 사실 두 개의 RNN입니다. 입력 문장을 받는 RNN 셀을 인코더라고 하며, 출력 문장을 출력하는 RNN 셀을 디코더라고 합니다. 여기서 인코더의 RNN은 주황, 디코더의 RNN은 연두색으로 표현합니다. 그리고 RNN은 **LSTM 셀** 이나 **GRU 셀** 로 구성됩니다. \n",
    "\n",
    "우선 인코더를 살펴보겠습니다. 입력 문장은 단어 토큰화를 통해서 단어 단위로 쪼개지고 단어 토큰 각각은 RNN 셀의 각 시점의 입력이 됩니다. 인코더 RNN 셀은 모든 단어를 입력받은 뒤에 **인코더 RNN 셀의 마지막 시점의 은닉 상태** 를 디코더 RNN 셀로 넘겨주는데 이를 **컨텍스트 벡터** 라고 합니다. 컨텍스트 벡터는 **디코더 RNN 셀의 첫번째 은닉 상태** 에 사용됩니다. \n",
    "\n",
    "디코더는 기본적으로 RNNLM(RNN Language Model)입니다. 인코더를 통해 나온 컨텍스트 벡터를 LSTM layer가 입력받습니다. 이때 초기 입력의 문장의 시작을 의미하는 심볼 <sos>가 들어갑니다. 디코더는 <sos>가 입력되면, 다음에 등장할 확률이 높은 단어를 예측합니다. 첫번째 시점의 디코더 RNN 셀은 다음에 등장할 단어로 je를 다음 시점의 RNN 셀의 입력으로 입력합니다. 그리고 두번째 시점의 디코더 RNN 셀은 입력된 단어 je로부터 다시 다음에 올 단어인 suis를 예측하고 이를 반복하여 이어갑니다. 여기까지 **테스트 과정** 의 이야기입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq의 훈련 과정은 테스트 과정과 조금 다릅니다. 훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제 정답을 받고 정답을 알려주면서 훈련합니다. \n",
    "\n",
    "이제 입,출력에 쓰이는 단어 토큰들이 있는 부분을 좀 더 확대해보겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG\">\n",
    "\n",
    "자연어 처리에서 텍스트를 벡터로 바꾸는 방법으로 주로 워드 임베딩이 사용되고 있습니다. 즉, seq2seq에서 사용되는 모든 단어들은 워드 임베딩을 통해 임베딩 벡터로서 표현된 임베딩 벡터입니다. 위 그림은 모든 단어에 대해서 임베딩 과정을 거치게 하는 단계인 임베딩 층(embedding layer)의 모습을 보여줍니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/24996/%EC%9E%84%EB%B2%A0%EB%94%A9%EB%B2%A1%ED%84%B0.PNG\">\n",
    "\n",
    "예를 들어 I, am, a, student라는 단어들에 대한 임베딩 벡터는 위와 같은 모습을 가집니다. 여기서는 그림으로 표현하고자 4로 하였지만, 보통 실제 임베딩 벡터는 수백 개의 차원을 가질 수 있습니다. 이제 하나의 RNN 셀을 확대해서 보겠습니다. 하나의 RNN 셀은 각각의 시점마다 두 개의 입력을 받습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/24996/rnn%EA%B7%BC%ED%99%A9.PNG\">\n",
    "\n",
    "현재 시점을 t라고 할 때, RNN 셀은 t-1에서의 은닉 상태와 t에서의 입력 벡터를 입력으로 받고, t에서의 은닉 상태를 만듭니다. 이때 t에서의 은닉 상태는 바로 위에 또 다른 은닉층이나 출력층이 존재할 경우에는 위의 층으로 보내거나, 필요없으면 값을 무시할 수 있습니다. 그리고 RNN 셀은 다음 시점에 해당하는 t+1의 RNN 셀의 입력으로 현재 t에서의 은닉 상태를 입력으로 보냅니다. \n",
    "\n",
    "이런 구조에서 현재 시점 t에서의 은닉 상태는 과거 시점의 동일한 RNN 셀에서의 모든 은닉 상태의 값들의 영향을 누적해서 받아온 값이라고 할 수 있습니다. 그렇기 때문에 앞서 언급했던 컨텍스트 벡터는 사실 인코더에서 마지막 RNN 셀의 은닉 상태값을 말하는 것이며, 이는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고있다고 할 수 있습니다.\n",
    "\n",
    "디코더는 인코더의 마지막 RNN 셀의 은닉 상태인 컨텍스트 벡터를 첫번째 은닉 상태의 값으로 사용됩니다. 디코더의 첫번째 RNN 셀은 이 첫번째 은닉 상태의 값과 현재 t에서의 입력값인 <sos>로부터, 다음에 등장할 단어를 예측합니다. 그리고 이 예측된 단어는 다음 시점인 t+1 RNN에서의 입력값이 되고, 이 t+1에서의 RNN 또한 이 입력값과 t에서의 은닉 상태로부터 t+1에서의 출력 벡터, 즉 또 다시 다음에 등장할 단어를 예측하게 됩니다. 디코더가 다음에 등장할 단어를 예측하는 부분을 확대해보겠습니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/24996/decodernextwordprediction.PNG\">\n",
    "\n",
    "출력 단어로 나올 수 있는 단어들은 다양한 단어들이 있습니다. seq2seq 모델은 선택될 수 있는 모든 단어들로부터 하나의 단어를 골라서 예측해야 합니다. 이를 예측하기 위해서 쓸 수 있는 함수로 소프트맥스 함수를 사용합니다. 디코더에서 각 시점의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 외에도 컨텍스트 벡터를 디코더의 초기 은닉 상태로만 사용하거나, 컨텍스트 벡터를 디코더가 단어를 예측하는 매 시점마다 하나의 입력으로 사용할 수 있으며 입력 데이터 반전(reverse), 엿보기(peeky) 등의 트릭도 있습니다. 그리고 다음 챕터에서 배울 어텐션 매커니즘이라는 방법을 통해 문맥을 더욱 반영할 수도 있습니다. \n",
    "\n",
    "밑의 그림은 전체적인 동작을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://docs.chainer.org/en/v7.8.0/_images/seq2seq.png\" height = \"600px\" width = \"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7 깊은 순환 신경망**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배웠던 기본 RNN을 다시 생각해보겠습니다. 일반 RNN은 은닉층을 1개 가지고 있습니다. 근데 여기서 1개를 더 추가하여 2개의 은닉층을 사용하는 것입니다. 밑의 그림과 같은 모습이 나타납니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22886/rnn_image4.5_finalPNG.PNG\">\n",
    "\n",
    "이떄 마지막 output만 사용하는 것이 아니라 각 은닉층마다 나오는 output을 가중 평균을 해서 최종 output을 구할 수도 있습니다. 또한 dropout도 각 layer마다 다르게 적용할 수도 있습니다. 대체적으로 hiddeb layer의 size를 input layer의 size의 4배를 할 때 가장 성능이 좋습니다. 만약 두 layer의 size가 같다면 (원래는 고정되어있던) 워드 임베딩과 (매 epoch마다 학습되었던) 가중치를 같이 학습하던가, 결합하여 사용할 수도 있습니다. 이렇게 되면 워드 임베딩 벡터도 학습을 시킬 수는 있게됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.8 양방향 순환 신경망**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "양방향 순환 신경망(Bidirectional Recurrent Neural Network)은 시점 t에서의 출력값을 예측할 때, 이전 시점의 데이터뿐만 아니라 이후 데이터로도 예측하겠다는 아이디어를 도입한 것입니다.\n",
    "\n",
    "영어 빈칸 채우기 문제에 비유해보겠습니다.\n",
    "\n",
    "> Exercise is very effective at [      ] belly fat.\n",
    ">\n",
    "> 1) reducing\n",
    "> 2) increasing\n",
    "> 3) multiplying\n",
    "\n",
    "'운동은 복부 지방을 [] 효과적이다'라는 영어 문장이고 정답은 reducing입니다. 그런데 위의 영어 빈 칸 채우기 문제를 잘 생각하면 정답을 찾기 위해서는 이전에 나온 단어들만으로는 부족합니다. 목적어인 belly fat(복부 지방)을 모르는 상태라면 정답을 결정하기 어렵습니다.\n",
    "\n",
    "즉, RNN이 과거 시점(time step)의 데이터들을 참고해서, 찾고자하는 정답을 예측하지만 실제 문제에서는 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에 힌트가 있는 경우도 있는 것입니다. 그래서 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG\">\n",
    "\n",
    "양방향 RNN은 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용합니다. 첫 번째 메모리 셀은 앞서 배운 것처럼 **`앞 시점의 은닉 상태(Forward States)`** 를 전달받아 현재의 은닉 상태를 계산합니다. 위 그림에서 주황색 메모리 셀에 해당합니다. 두 번째 메모리 셀은 앞 시점 은닉 상태가 아니라 **`뒤 시점의 은닉 상태(Backward States)`** 를 전달 받아 현재의 은닉 상태를 계산합니다. 위 그림에서 초록색 메모리 셀에 해당됩니다. 그리고 이 두 개의 값 모두가 출력층에서 출력값을 예측하기 위해 사용됩니다.\n",
    "\n",
    "물론 양방향 RNN도 다수의 은닉층을 가질 수 있습니다. 아래의 그림은 양방향 RNN에서 은닉층이 1개 더 추가되어 2개의 은닉층을 가지는 깊은 양방향 순환 신경망의 모습입니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG\">\n",
    "\n",
    "다른 인공 신경망도 마찬가지지만, 은닉층을 무조건 추가한다고 해서 모델의 성능이 좋아지지 않습니다. 은닉층을 추가하면, 학습할 수 있는 양이 많아지지만 또한 반대로 훈련 데이터 또한 그만큼 많이 필요합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. Language Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 RNN기반의 Language Model을 구현해서 텍스트를 직접 생성해보는 실습을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) DataLoader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 실습들에서 사용한 것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다. 이 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 'unkown'으로 이미 전처리가 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentence: 42068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b\" plans that give advertisers discounts for maintaining or increasing ad spending have become permanent <unk> at the news <unk> and underscore the fierce competition between newsweek time warner inc. 's time magazine and <unk> b. <unk> 's u.s. news & world report \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('num_sentence:',len(data))\n",
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length_list = []\n",
    "for line in data:\n",
    "    seq_length_list.append(len(line.split()))\n",
    "\n",
    "counts, bins = np.histogram(seq_length_list, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 왜 여기서만 kernel이 죽는가\n",
    "#plt.hist(bins[:-1], bins, weights=counts)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](_image/plot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터에 있는 문장 길이들의 histogram을 볼 때, 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 model에 집어넣을 최대 문장 길이를 50으로 세팅합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n",
    "\n",
    "이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(data, max_seq_len):\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    ## Build Dictionary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1 # keyerror가 났을 때 출력할 value\n",
    "    idx2word[0] = '<pad>'\n",
    "    idx2word[1] = '<unk>' \n",
    "    idx = 2\n",
    "    for line in data:\n",
    "        words = line.decode('utf-8').split()\n",
    "        words = words[:max_seq_len]        \n",
    "        # Build Dictionary to convert word to index and index to word\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = idx\n",
    "                idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = build_dictionary(data, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "print(len(word2idx), len(idx2word))\n",
    "\n",
    "if len(word2idx) == len(idx2word) == 10000:\n",
    "    print(\"Test Passed!\")\n",
    "else:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, word2idx, idx2word, max_seq_len):\n",
    "    tokens = []\n",
    "    for line in data:\n",
    "        words = line.decode('utf-8').split()\n",
    "        words = words[:max_seq_len]\n",
    "        # Convert dataset with tokens\n",
    "        # For each line, append <pad> token to match the number of max_seq_len\n",
    "        words += [\"<pad>\"] * (max_seq_len - len(words))\n",
    "        for word in words:\n",
    "            tokens.append(word2idx[word])\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(data, word2idx, idx2word, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "if len(tokens) == 2103400:\n",
    "    print(\"Test Passed!\")\n",
    "else:\n",
    "    raise AssertionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42068, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([745,  93, 746, 739, 747, 181, 748, 467, 749, 740, 750, 154, 751,\n",
       "       752,   1, 160,  32, 753,   1,  48, 754,  32, 755, 756, 757, 728,\n",
       "       555, 758,  99, 119, 555, 733,  48,   1, 759,   1, 119, 237, 753,\n",
       "       230, 760, 347,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.array(tokens).reshape(-1, max_seq_len)\n",
    "print(tokens.shape)\n",
    "tokens[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n",
    "\n",
    "Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        super(LMDataset, self).__init__()\n",
    "        self.PAD = 0\n",
    "        self.UNK = 1\n",
    "        self.tokens = tokens\n",
    "        self._getitem(2)\n",
    "\n",
    "    def _getitem(self, index):\n",
    "        X = self.tokens[index]\n",
    "        y = np.concatenate((X[1:], [self.PAD]))\n",
    "\n",
    "        X = torch.from_numpy(X).unsqueeze(0).long()\n",
    "        y = torch.from_numpy(y).unsqueeze(0).long()\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.tokens[index]\n",
    "        y = np.concatenate((X[1:], [self.PAD]))\n",
    "\n",
    "        X = torch.from_numpy(X).long()\n",
    "        y = torch.from_numpy(y).long()\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "658\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataset = LMDataset(tokens)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(dataset))\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다. \n",
    "\n",
    "Standard한 RNN model은 기울기 소실 문제에 취약하기 때문에, 이번 실습에선 LSTM RNN을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lycT_9vwaJN"
   },
   "source": [
    "LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1h6nfvYwN8n"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n",
    "\n",
    "LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        # input-gate\n",
    "        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # forget-gate\n",
    "        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # gate-gate\n",
    "        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # output-gate\n",
    "        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "        # non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h_0, c_0):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input (x): [batch_size, input_size]\n",
    "            hidden_state (h_0): [batch_size, hidden_size]\n",
    "            cell_state (c_0): [batch_size, hidden_size]\n",
    "        Outputs\n",
    "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
    "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
    "        \"\"\"\n",
    "        h_1, c_1 = None, None\n",
    "        input = torch.cat((x, h_0), 1) \n",
    "        # Implement LSTM cell as noted above\n",
    "        i = self.sigmoid(self.Wi(input))\n",
    "        f = self.sigmoid(self.Wf(input))\n",
    "        g = self.tanh(self.Wg(input))\n",
    "        o = self.sigmoid(self.Wo(input))\n",
    "        c_1 = f * c_0 + g * i\n",
    "        h_1 = self.tanh(c_1) * o        \n",
    "\n",
    "        return h_1,c_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==LSTM cell test passed!==\n"
     ]
    }
   ],
   "source": [
    "def test_lstm():\n",
    "    batch_size = 2\n",
    "    input_size = 5\n",
    "    hidden_size = 3\n",
    "\n",
    "    #torch.manual_seed(1234)\n",
    "    lstm = LSTMCell(input_size ,hidden_size)\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.constant_(m.weight, 0.1)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    lstm.apply(init_weights)\n",
    "\n",
    "    x = torch.ones(batch_size, input_size) \n",
    "    hx = torch.zeros(batch_size, hidden_size) \n",
    "    cx = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "    hx, cx = lstm(x, hx, cx)\n",
    "    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784], \n",
    "                                            [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n",
    "            f\"Output of the hidden state does not match.\"\n",
    "    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936], \n",
    "                                            [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n",
    "            f\"Output of the cell state does not match.\"\n",
    "\n",
    "    print(\"==LSTM cell test passed!==\")\n",
    "\n",
    "test_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 위에서 정의한 LSTM Cell을 활용하여 아래와 같은 Language Model을 만들어보도록 하겠습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_layer = LSTMCell(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, hx, cx, predict=False):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            input (x): [batch_size]\n",
    "            hidden_state (h_0): [batch_size, hidden_size]\n",
    "            cell_state (c_0): [batch_size, hidden_size]\n",
    "            predict: whether to predict and sample the next word\n",
    "        Outputs\n",
    "            output (ox): [batch_size, hidden_size]\n",
    "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
    "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
    "        \"\"\"\n",
    "        x = self.input_layer(x)\n",
    "        hx, cx = self.hidden_layer(x, hx, cx)\n",
    "        ox = self.output_layer(hx)\n",
    "\n",
    "        if predict == True:\n",
    "            probs = F.softmax(ox, dim=1)\n",
    "            # torch distribution allows sampling operation\n",
    "            # see https://pytorch.org/docs/stable/distributions.html\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            ox = dist.sample()\n",
    "\n",
    "        return ox, hx, cx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, \n",
    "                word2idx, \n",
    "                idx2word,\n",
    "                dataloader, \n",
    "                model, \n",
    "                criterion,\n",
    "                optimizer, \n",
    "                device):\n",
    "        \"\"\"\n",
    "        dataloader: dataloader\n",
    "        model: langauge model\n",
    "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
    "        optimizer: optimizer for model\n",
    "        \"\"\"\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.dataloader = dataloader\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, epochs = 1):\n",
    "        self.model.to(self.device)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n",
    "                self.model.train()\n",
    "                \n",
    "                batch_size, max_seq_len = x_batch.shape\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "\n",
    "                # initial hidden-states\n",
    "                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
    "                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
    "\n",
    "                # Implement LSTM operation\n",
    "                ox_batch = []\n",
    "                # Get output logits for each time sequence and append to the list, ox_batch\n",
    "                for s_idx in range(max_seq_len):\n",
    "                    x = x_batch[:, s_idx]\n",
    "                    ox, hx, cx = self.model(x, hx, cx)\n",
    "                    ox_batch.append(ox)\n",
    "\n",
    "                # outputs are ordered by the time sequence\n",
    "                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n",
    "                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n",
    "                y_batch = y_batch.reshape(-1)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                loss = self.criterion(ox_batch, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            end_time = time.time() - start_time\n",
    "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
    "            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n",
    "                % (end_time, epoch+1, epochs, np.mean(losses)))\n",
    "            if epoch % 5 == 0:\n",
    "                generated_sentences = self.test()\n",
    "                print('[Generated Sentences]')\n",
    "                for sentence in generated_sentences:\n",
    "                    print(sentence)\n",
    "            \n",
    "    def test(self):\n",
    "        # Test model to genereate the sentences\n",
    "        self.model.eval()\n",
    "        num_sentence = 5\n",
    "        max_seq_len = 50\n",
    "\n",
    "        # initial hidden-states\n",
    "        outs = []\n",
    "        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n",
    "        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
    "        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
    "\n",
    "        outs.append(x)\n",
    "        with torch.no_grad():\n",
    "            for s_idx in range(max_seq_len-1):\n",
    "                x, hx, cx = self.model(x, hx, cx, predict=True)\n",
    "                outs.append(x)\n",
    "        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n",
    "        outs = outs.permute(1, 0)\n",
    "        outs = outs.detach().cpu().numpy()\n",
    "\n",
    "        sentences = []\n",
    "        for out in outs:\n",
    "            sentence = []\n",
    "            for token_idx in out:\n",
    "                word = self.idx2word[token_idx]\n",
    "                sentence.append(word)\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [11:09,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:11:09], Epoch [1/50], loss: 6.0897\n",
      "[Generated Sentences]\n",
      "['demanding', 'following', 'all', 'policy', 'for', 'donaldson', 'lufkin', '&', 'equipment', 'denied', 'him', 'michael', 'cutting', 'the', 'products', 'and', 'we', 'has', 'had', 'already', 'long', 'inherited', 'over', 'bad', 'for', 'trouble', 'on', 'sales', 'saying', 'any', 'as', 'it', 'was', 'a', 'level', '<unk>', 'for', 'a', '<unk>', 'starts', 'of', 'the', 'past', 'certain', 'rights', 'of', 'use', 'rates', 'million', 'or']\n",
      "['boyd', 'improved', 'anti-drug', 'six', 'months', 'until', 'the', 'loans', 'in', 'general', 'electric', 'co.', 'broke', 'only', 'new', '<unk>', 'would', \"n't\", 'turn', 'us', '$', 'N', 'million', 'of', 'about', '$', 'N', 'a', 'share', 'in', 'a', 'capita', 'or', 'mortgage', 'and', 'do', 'is', 'willing', 'of', 'communist', 'investor', \"'s\", 'upscale', 'directors', 'will', 'on', 'barriers', 'over', 'anybody', 'to']\n",
      "['labeled', 'scuttle', 'invest', 'for', 'the', 'purchasing', 'earthquake', 'the', 'underwriter', 'recently', 'plunged', 'N', 'billion', 'planes', 'to', 'partners', 'up', 'out', 'tourism', 'purposes', 'and', 'maximize', 'states', 'to', 'sell', 'september', 'contracts', 'it', 'would', 'go', 'delayed', 'he', 'adds', 'their', '<unk>', 'price', 'from', 'the', 'country', \"'s\", 'largest', 'concern', 'often', 'its', 'financial', 'wednesday', 'of', 'his', 'failed', 'has']\n",
      "['fighting', 'evening', 'trucks', 'because', 'he', 'said', 'net', 'by', 'seconds', 'in', 'line', 'that', 'or', 'two-thirds', 'links', 'of', 'the', 'investment', 'survey', 'offices', 'on', 'the', 'queen', 'of', 'N', 'billion', 'of', 'N', 'that', 'earlier', 'securities', 'press', 'after', 'a', 'spin', 'card', 'at', 'guilty', 'to', '<unk>', 'versions', 'traders', 'will', 'stand', 'less', 'than', 'power', 'indiana', 'entertainment', 'which']\n",
      "['recipients', 'race', '<unk>', 'the', 'case', 'of', 'a', 'producer', 'marketing', 'second-largest', 'functions', 'chairman', 'increases', 'is', 'working', 'to', '<unk>', 'what', 'they', 'are', 'included', 'at', 'least', 'three', 'securities', 'when', 'he', 'wrote', 'the', '<unk>', 'museum', 'would', 'be', 'a', '<unk>', 'with', 'them', 'william', 'chairman', '<unk>', 'and', '<unk>', 'with', 'a', 'negative', 'sales', 'of', 'a', 'resort', 'alleges']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:56,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:21:06], Epoch [2/50], loss: 5.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:44,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:30:51], Epoch [3/50], loss: 4.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:29,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:40:21], Epoch [4/50], loss: 4.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:32,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:49:53], Epoch [5/50], loss: 4.5483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:50,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [0:59:44], Epoch [6/50], loss: 4.4220\n",
      "[Generated Sentences]\n",
      "['profit', 'creates', 'two', 'more', 'willing', 'to', 'ensure', 'the', 'tv', 'of', 'a', 'dozen', 'examples', 'of', 'the', 'company', 'open', '<unk>', 'form', 'as', 'the', 'companies', 'esso', 'totaled', '$', 'N', 'million', 'in', '<unk>', 'in', 'which', 'a', 'transaction', 'service', 'wo', \"n't\", 'be', 'one', 'more', 'attractive', 'and', 'the', 'growth', 'of', 'the', 'amount', 'of', 'market', 'volatility', 'especially']\n",
      "['season', 'competent', 'systems', 'to', 'a', 'doctor', 'is', 'counting', 'on', 'kidder', \"'s\", 'world-wide', 'agency', 'while', 'produce', 'significant', 'cnw', 'stocks', '<unk>', 'on', 'hong', 'kong', \"'s\", 'statement', 'N', 'of', 'the', 'securities', 'and', 'exchange', 'rate', 'increases', 'rose', 'in', 'power', 'and', 'will', 'go', 'if', 'certain', 'of', 'its', 'pricing', 'plan', 'such', 'as', '<unk>', 'blocks', 'driving', 'into']\n",
      "['hunt', 'project', 'gained', 'N', 'N', 'in', 'far', 'on', 'the', 'sale', 'investment', 'plan', 'which', 'moved', 'that', 'do', 'not', 'the', 'good', 'fee', 'to', '<unk>', '<unk>', 'aircraft', 'most', 'livestock', 'paintings', 'abroad', 'to', 'appellate', 'channel', 'more', 'words', 'the', 'grand', 'consolidated', 'company', \"'s\", 'determination', 'to', 'pay', 'the', '<unk>', 'rare', 'to', 'develop', 'letter', 'including', 'a', 'military']\n",
      "['often', \"'80s\", 'spring', 'i', \"'ve\", 'got', 'to', 'sequester', \"'s\", 'major', 'big', 'real', 'estate', 'where', 'his', 'wife', 'were', 'discovered', 'for', 'one', 'to', 'come', 'the', 'possibility', 'is', 'getting', 'out', 'to', 'close', 'themselves', 'this', 'time', 'in', 'sound', 'magnetic', '<unk>', 'by', 'mr.', 'azoff', 'plan', 'has', 'had', 'several', 'greater', 'than', 'doing', 'nothing', 'in', 'his', 'wife']\n",
      "['philip', 'morris', 'cos.', 'maker', 'reported', 'net', 'income', 'rose', 'at', 'citicorp', \"'s\", 'advertising', 'shares', 'outstanding', 'stake', 'in', 'a', 'ford', 'profit', 'is', 'index', 'of', 'western', 'union', 'capital', 'corp.', 'won', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share', 'on', 'revenue', 'and', 'an', 'reviewed', 'at', 'first', 'boston', 'corp.', 'the', 'issuer', \"'s\", 'ailing', '<unk>', 'assets']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:31,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:09:16], Epoch [7/50], loss: 4.3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:29,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:18:45], Epoch [8/50], loss: 4.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:29,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:28:15], Epoch [9/50], loss: 4.1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:24,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:37:40], Epoch [10/50], loss: 4.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:46:46], Epoch [11/50], loss: 4.0075\n",
      "[Generated Sentences]\n",
      "['tilt', 'stocks', 'new', 'technologies', 'jerry', 'mich.', 'auto', 'maker', 'stores', 'inc.', 'marketing', 'acquired', 'last', 'spring', 'appears', 'to', 'be', 'meeting', 'dropped', 'N', 'N', 'to', 'N', 'after', 'the', 'state', 'of', 'an', '<unk>', 'of', 'a', 'big', 'players', \"'\", 'listed', 'travel', 'and', 'had', 'agreed', 'in', 'the', 'u.s.', 'budget', 'on', 'over', 'three', 'years', 'that', 'expects', 'would']\n",
      "['host', 'environmentalism', 'services', 'say', 'that', 'the', 'industrial', 'average', 'will', 'be', 'called', 'for', 'one', '<unk>', 'and', 'relationship', 'because', 'takeover', 'stocks', 'will', 'remain', 'on', 'the', 'community', 'against', 'N', 'federal', 'agencies', 'including', 'the', '<unk>', 'assets', 'of', 'u.s.', 'price', 'as', 'part', 'of', 'this', 'savings', 'bank', '<unk>', 'its', 'sunnyvale', 'manufacturing', 'coatings', 'as', 'mcdonald', '&', 'co.']\n",
      "['cost', 'note', 'that', 'must', 'be', 'replaced', 'by', 'the', 'project', 'with', 'no', 'one', 'has', 'no', 'way', 'of', 'men', \"'s\", 'perceptions', 'that', 'when', \"n't\", 'passed', 'over', 'financial', 'or', 'ancient', '<unk>', '<unk>', '<unk>', 'added', 'here', 'is', 'a', 'turnaround', 'by', 'only', 'about', 'N', 'months', 'later', 'called', 'mr.', 'laff', 'shopping', 'which', 'the', 'u.s.', 'actress', 'weather']\n",
      "['bound', 'to', 'an', '<unk>', 'view', 'as', 'inadequate', '<unk>', 'says', 'a', 'third', 'bank', 'set', 'in', 'july', 'N', 'to', '<unk>', '<unk>', '<unk>', '<unk>', 'to', 'have', 'a', 'new', 'york', 'union', 'camp', 'corp', 'president', 'of', 'research', 'this', 'tv', 'almost', 'now', 'is', 'in', 'a', '<unk>', '<unk>', 'cela', 'the', 'pilot', 'list', 'i', \"'d\", 'delay', 'a', 'comprehensive']\n",
      "['sagan', 'do', 'you', 'have', 'had', 'to', 'present', 'him', 'out', 'as', 'a', 'manuel', 'in', 'rare', '<unk>', 'if', 'i', 'have', 'said', 'he', 'did', 'sheet', 'may', 'happen', 'at', 'readers', 'and', 'furniture', 'bunny', 'reruns', 'says', 'senior', 'vice', 'president', 'giants', 'following', 'a', 'diagnostic', 'assembly', 'plant', 'in', 'britain', 'but', 'not', 'headed', 'before', 'we', 'humans', 'down', 'to']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [1:55:51], Epoch [12/50], loss: 3.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:04:56], Epoch [13/50], loss: 3.9014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:14:03], Epoch [14/50], loss: 3.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:23:10], Epoch [15/50], loss: 3.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:32:16], Epoch [16/50], loss: 3.7738\n",
      "[Generated Sentences]\n",
      "['larsen', '&', 'toubro', 'had', 'agreed', 'to', 'sell', 'its', 'obligations', 'under', 'the', 'spinoff', 'in', '<unk>', 'n.y.', 'industry', 'is', 'negotiating', 'with', 'the', 'west', 'coast', 'and', 'sen.', 'kennedy', 'and', 'invited', 'meredith', 'were', 'concerned', 'with', 'our', 'first', 'mainframe', 'deliveries', 'in', 'north', 'carolina', 'holding', 'its', 'withdrawal', 'from', 'sir', 'james', 'goldsmith', 'intends', 'to', 'sell', 'its', 'responsibilities']\n",
      "['improvements', 'absorbed', 'for', 'the', 'first', 'humans', 'will', 'be', 'negative', 'have', 'been', '<unk>', 'some', '<unk>', 'with', 'the', 'offering', 'cuts', 'that', 'might', 'go', 'along', 'to', 'the', 'ratings', 'point', 'by', 'staff', 'up', 'N', 'after', 'N', 'N', 'of', 'which', 'wang', \"'s\", 'eight', 'institute', 'of', 'new', 'york-based', 'dealers', 'also', 'u.s.', 'attorney', 'general', 'for', 'spare', 'emergency']\n",
      "['worsen', 'earnings', 'per', 'common', 'shares', 'including', 'six', 'months', 'to', 'N', 'N', 'annual', 'revenue', 'was', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'from', 'N', 'cents', 'to', '$', 'N', 'a', 'share', 'for', 'increased', 'N', 'N', 'N', 'to', 'N', 'N', 'to', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'up', 'N', 'N']\n",
      "['ministry', 'rumored', 'watch', 'this', 'week', 'called', 'readily', 'and', 'gray', 'and', 'the', 'leadership', 'battle', 'successfully', 'manuel', 'made', 'in', '<unk>', 'front', 'itself', 'with', 'congress', 'in', 'the', '<unk>', 'and', 'do', 'without', 'him', 'that', 'i', \"'m\", 'made', 'a', '<unk>', 'trap', 'at', 'home', 'savings', 'and', 'other', 'airline', '<unk>', 'as', 'well', 'predicted', 'that', 'period', 'of', 'a']\n",
      "['determined', 'need', 'to', 'yesterday', 'in', '<unk>', 'the', 'market', 'overbuilt', 'builders', 'used', 'to', 'track', 'the', 'intense', 'competition', 'held', 'joint', 'product', 'line', 'between', 'france', 'to', 'discuss', '<unk>', 'all', 'its', 'popular', 'operations', 'in', 'manufacturing', 'authorities', 'first', 'in', 'december', 'fell', 'through', 'the', 'october', 'N', 'crash', 'last', 'week', 'in', 'the', 'quarter', \"'s\", 'selling', 'of', '<unk>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:03,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:41:20], Epoch [17/50], loss: 3.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:08,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:50:29], Epoch [18/50], loss: 3.7061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [2:59:35], Epoch [19/50], loss: 3.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:05,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:08:40], Epoch [20/50], loss: 3.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:17:48], Epoch [21/50], loss: 3.6227\n",
      "[Generated Sentences]\n",
      "['morris', 'homes', 'rose', 'at', '$', 'N', 'at', 'par', 'and', 'N', 'tentatively', 'priced', 'at', 'N', 'million', 'guilders', 'up', 'N', 'cents', 'in', 'composite', 'trading', 'on', 'the', 'benchmark', '30-year', 'bond', 'ended', 'higher', 'in', 'quiet', 'dealings', 'monday', 'largely', 'by', 'salomon', 'inc.', 'said', 'third-quarter', 'net', 'income', 'plunged', 'N', 'to', 'N', 'N', 'on', 'loans', 'to', 'a']\n",
      "['widespread', 'wells', 'rich', 'co', 'closed', 'an', 'annual', 'rate', 'of', 'N', 'N', 'of', 'the', 'european', 'stock', 'in', 'heavy', 'exchange', 'N', 'shares', 'are', 'scheduled', 'to', 'be', 'about', 'N', 'million', 'shares', 'in', 'N', 'the', 'tender', 'offer', 'will', 'place', 'payment', 'in', 'cash', 'charges', 'partly', 'because', 'of', 'its', 'problems', 'for', 'campeau', 'employees', 'and', 'jaguar', 'became']\n",
      "['administrative', 'manipulation', 'of', 'the', 'banks', \"'\", 'expectations', '$', 'N', 'million', 'and', 'put', 'either', 'projections', 'of', 'bank', 'investments', 'as', 'general', 'obligation', 'of', 'state', 'monopolies', 'for', '$', 'N', 'in', 'N', 'wednesday', 'and', 'the', 'nasdaq', 'financial', 'times', 'group', 'into', 'an', 'loss', 'from', 'u.s.', 'third-quarter', 'gnp', 'forecasts', 'for', 'the', 'disk', 'drives', 'that', '<unk>', '<unk>']\n",
      "['gubernatorial', 'candidate', 'center', 'constitution', 'in', 'stock', 'prices', 'of', 'columbia', 'stock', 'and', 'had', 'forgotten', 'to', 'increase', 'its', 'devices', 'this', 'morning', 'and', 'found', 'that', 'yields', 'on', 'the', 'balance', 'of', 'the', 'company', \"'s\", 'decision', 'still', 'lowered', 'its', 'extensive', 'authority', 'over', 'an', 'arizona', 'spirit', 'is', 'a', 'moderate', 'N', 'N', 'of', 'its', 'N', 'N', 'stake']\n",
      "['firstsouth', 'advisers', 'one', 'that', 'acts', 'before', 'entering', 'other', 'areas', 'of', '<unk>', 'which', 'they', 'are', \"n't\", 'even', 'confident', 'that', 'the', 'experience', 'of', 'experience', 'from', '<unk>', 'and', 'floor', 'brokers', 'to', 'put', 'up', 'as', 'enough', 'unfairly', 'listed', 'companies', 'with', 'capital', 'spending', 'a', 'sound', 'amount', 'N', 'that', 'fasb', 'debt', 'and', 'foreign', 'currencies', 'which', 'continues']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:26:53], Epoch [22/50], loss: 3.5981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:35:59], Epoch [23/50], loss: 3.5752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:05,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:45:05], Epoch [24/50], loss: 3.5552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [3:54:12], Epoch [25/50], loss: 3.5378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:03:17], Epoch [26/50], loss: 3.5174\n",
      "[Generated Sentences]\n",
      "['holmes', 'averages', 'by', 'tax', 'seasonally', 'adjusted', 'grew', 'N', 'N', 'to', 'N', 'N', 'on', 'tuesday', 'that', 'banks', 'will', 'be', 'determined', 'because', 'of', 'japan', 'for', 'bankers', 'corp.', 'said', 'the', 'transaction', 'is', 'using', 'privately', 'interest', 'under', 'shares', 'of', 'stock', 'prices', 'to', 'start', 'earlier', 'loan', 'guarantees', 'executed', 'or', '<unk>', 'natural', 'gas', 'when', 'retail', 'sales']\n",
      "['lower', 'feed', 'prices', 'prices', 'closed', 'slightly', 'higher', 'in', 'zurich', 'brussels', 'milan', 'and', 'operations', 'in', 'fact', 'the', 'british', 'tobacco', 'market', 'wo', \"n't\", 'disrupt', 'plans', 'for', 'the', 'face', 'value', 'payable', 'short', 'interest', 'in', 'profit', 'for', 'this', 'year', 'matching', 'funds', 'would', 'gain', 'as', 'a', 'significant', 'net', 'loss', 'and', 'the', 'administration', 'said', 'the', 'state']\n",
      "['hinted', 'recall', 'georgia', 'led', 'by', 'the', 'time', 'and', 'so', 'low', 'prices', 'might', 'limited', 'operations', 'for', 'disease', 'control', 'more', 'than', 'the', 'laws', 'of', 'mortgage-backed', 'securities', 'during', 'the', 'same', 'period', 'the', 'year', 'about', 'key', 'units', 'in', 'a', 'young', 'industry', 'newsletter', 'N', 'vacant', 'to', 'N', 'N', 'of', 'its', '$', 'N', 'million', 'for', 'example']\n",
      "['inaccurate', 'books', 'similar', 'investment', 'controlling', 'over', 'N', 'years', 'and', 'will', 'spur', 'living', 'figures', 'she', 'reached', 'an', 'additional', 'multimillion-dollar', 'fortune', '$', 'N', 'million', 'in', 'compromise', 'over', 'the', 'next', 'few', 'weeks', 'also', 'shows', 'and', 'junk', 'bonds', 'than', 'economists', 'said', 'they', 'expected', 'to', 'the', 'youngsters', \"'\", 'volatility', 'mr.', '<unk>', 'concludes', 'with', 'only', 'the']\n",
      "['quietly', 'ordering', 'ill', 'subsidized', 'audiences', 'each', '<unk>', 'are', 'still', 'only', 'a', 'minimal', 'encouragement', 'for', '<unk>', 'status', 'the', 'price', 'of', 'the', '<unk>', 'pickup', 'and', 'bally', 'shares', 'to', 'have', 'N', 'shares', 'stock', 'family', 'were', 'issued', 'after', 'tracking', 'steel', 'grants', 'provided', 'the', 'new', 'products', 'said', 'the', 'soviet', 'union', 'continues', 'to', 'lend', 'the', 'results']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:12:22], Epoch [27/50], loss: 3.5039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:21:26], Epoch [28/50], loss: 3.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:30:33], Epoch [29/50], loss: 3.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:08,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:39:41], Epoch [30/50], loss: 3.4584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:29,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [4:50:11], Epoch [31/50], loss: 3.4462\n",
      "[Generated Sentences]\n",
      "['stemming', 'from', 'tax-loss', 'carry-forward', 'wheat', 'currently', 'holds', 'N', 'octel', 'rose', 'N', 'N', 'to', 'N', 'N', 'in', 'N', '<unk>', 'added', 'N', 'to', 'N', 'N', 'of', 'residents', 'more', 'than', 'expected', 'sales', 'of', 'mips', 'earnings', 'for', 'several', 'for', 'seasonal', 'control', 'which', 'a', 'stock', 'split', 'in', 'august', 'to', '<unk>', 'u.s.', 'goods', 'and', 'services', 'will']\n",
      "['jose', 'informed', 'after', 'using', 'a', 'controversial', 'junk-bond', 'report', 'showing', 'up', 'its', 'plan', 'to', 'raise', 'that', 'krasnoyarsk', 'may', 'be', 'phased', 'out', 'by', 'promotion', 'with', 'the', 'nasdaq', 'composite', 'wage', 'dean', 'plus', 'switzerland', 'and', 'N', 'a.m.', 'and', 'no', 'pieces', 'off', 'in', '<unk>', 'lives', 'that', '<unk>', 'affected', 'them', 'along', 'alaska', 'ltd.', 'the', 'world', \"'s\"]\n",
      "['warning', 'against', 'a', 'series', 'of', '<unk>', 'which', 'promises', 'working', 'constantly', '<unk>', 'to', 'stop', 'interstate', 'large', 'additional', 'tax', 'expansion', 'in', 'the', 'audience', 'she', 'was', 'too', 'big', 'as', 'mr.', 'reagan', \"'s\", 'death', '<unk>', 'in', 'the', 'hearts', 'of', 'staff', 'the', 'world', \"'s\", 'largest', 'publishing', 'analyst', 'also', 'known', 'as', 'well', 'both', 'in', 'january', 'to']\n",
      "['collection', 'jokes', 'preliminary', 'expectations', 'rallied', 'a', 'quarterly', 'loss', 'of', 'N', 'cents', 'a', 'share', 'compared', 'with', 'a', '$', 'N', 'annual', 'payment', 'owed', 'on', 'the', 'N', 'trillion', 'yen', 'a', 'language', 'called', 'fighting', 'taking', 'a', 'stark', 'sentence', 'in', 'jan.', 'N', '<unk>', 'by', 'corporate', 'time', 'hence', 'one', 'percentage', 'of', 'the', 'federal', 'trade', 'commission', 'extremely']\n",
      "['productivity', '<unk>', 'river', 'is', 'touting', 'the', 'chicago', 'merc', 'fined', '$', 'N', 'off', 'four', 'cents', 'a', 'share', 'because', 'the', 'company', 'reported', 'that', 'gap', 'for', 'its', 'high-yield', 'business', 'plunged', 'N', 'N', 'to', 'N', 'at', '$', 'N', 'a', 'share', 'billion', 'yen', 'off', 'some', 'of', 'about', '$', 'N', 'million', 'or', 'N', 'cents', 'from', 'half']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:27,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:00:38], Epoch [32/50], loss: 3.4357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:43,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:11:22], Epoch [33/50], loss: 3.4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:15,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:21:37], Epoch [34/50], loss: 3.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:39,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:32:17], Epoch [35/50], loss: 3.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:18,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:42:35], Epoch [36/50], loss: 3.3944\n",
      "[Generated Sentences]\n",
      "['not', 'counting', 'the', '<unk>', 'and', 'the', '<unk>', 'probably', 'would', 'be', 'taking', 'numbers', 'of', 'people', 'who', 'are', 'trying', 'to', 'contribute', 'to', 'see', 'other', 'fronts', 'by', 'granting', 'inc.', 'of', 'the', 'lower-than-expected', 'increase', 'in', 'the', 'recovery', 'in', 'the', 'east', '<unk>', 'of', '<unk>', '<unk>', 'as', 'having', 'profit', 'during', 'the', 'market', 'made', 'they', 'did', \"n't\"]\n",
      "['smoothly', 'listen', 'employed', 'by', 'N', 'metric', 'tons', 'of', 'wheat', 'cards', 'according', 'to', 'officers', 'each', 'group', 'said', 'it', 'extended', 'a', 'total', 'gain', 'attributable', 'in', 'over', 'which', 'the', 'dollar', 'to', 'reopen', 'today', 'or', 'those', 'maturities', 'rose', 'by', 'N', 'million', 'francs', 'us$', 'N', 'million', 'of', 'north', 'debt', 'accord', 'are', 'expected', 'to', 'begin', 'operations']\n",
      "['miss', 'conservatives', 'are', 'going', 'to', 'lose', 'some', 'other', 'witnesses', 'to', 'preserve', 'the', 'chairman', \"'s\", 'book', 'the', 'board', 'including', 'american', 'express', 'bondholders', \"'s\", 'entertainment', 'division', 'has', 'been', 'able', 'to', 'sell', 'an', 'income-tax', 'treaty', 'subject', 'to', 'corporate', '<unk>', 'software', 'for', 'public', 'buyers', 'has', 'an', 'insurance', 'council', 'on', 'wall', 'street', 'in', 'N', 'and']\n",
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'from', 'a', 'california', 'department', 'and', 'commerce', 'department', 'at', 'fault', 'with', 'increasing', 'frequency', 'they', 'note', 'mitsubishi', 'estate', 'has', 'available', 'will', 'revive', 'stable', 'forecasts', 'though', 'it', 'was', 'increased']\n",
      "['scope', 'specializing', 'vacant', 'indicate', 'the', 'london', 'brokers', 'and', '<unk>', 'on', 'london', 'and', 'higher', 'of', 'the', 'market', \"'s\", 'mostly', 'benefit', 'of', 'our', 'empty', 'hotel', 'sales', 'in', 'offering', 'would', 'continue', 'launching', 'hybrid', 'goods', 'and', 'expected', 'european', 'businesses', 'and', 'traders', 'to', 'decline', 'and', 'bigger', 'companies', 'in', 'which', 'engines', 'one', 'has', 'been', 'up', 'nearly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:06,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [5:52:42], Epoch [37/50], loss: 3.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:12,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:02:54], Epoch [38/50], loss: 3.3763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [10:06,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:13:01], Epoch [39/50], loss: 3.3678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:59,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:23:00], Epoch [40/50], loss: 3.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:06,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:32:07], Epoch [41/50], loss: 3.3539\n",
      "[Generated Sentences]\n",
      "['restrict', 'paper', 'risk', 'of', 'liability', 'have', 'been', 'pop', 'because', 'the', 'national', 'will', 'go', 'due', 'against', 'ms.', 'brief', 'puts', 'me', 'during', 'its', 'most', 'the', 'gramm-rudman', 'cuts', 'will', 'call', 'the', 'federal', 'economy', '<unk>', 'long', 'strange', 'payment', 'base', 'which', 'which', 'had', 'taken', 'on', 'prices', 'to', 'sell', 'increasing', 'competition', 'in', 'the', 'bidding', 'wars', 'to']\n",
      "['tens', 'intact', 'to', '$', 'N', 'reflecting', 'share', 'from', 'the', 'solutions', 'to', 'issue', 'of', 'maturing', 'though', 'they', 'do', \"n't\", 'introduce', 'a', 'filing', 'on', 'behalf', 'of', 'its', 'products', 'are', 'ignoring', 'by', 'an', 'overhaul', 'that', 'last', 'week', \"'s\", 'rail', 'and', 'hurricane', 'hugo', 'which', 'had', 'been', '<unk>', 'most', 'painfully', 'resorts', 'to', 'drexel', 'the', 'security']\n",
      "['yeast', 'covering', 'the', 'heart', 'of', 'lufthansa', 'jal', 'and', 'the', 'resolution', 'trust', 'co.', 'a', 'announced', 'of', 'the', 'largest', 'foreign', 'investor', 'dealer', 'already', 'has', 'eroded', 'during', 'the', 'week', 'by', 'late', 'monday', 'when', 'mcdonald', \"'s\", 'corp.', 'and', 'director', 'of', 'the', 'enterprises', '<unk>', 'program', 'that', 'commercial', 'paper', '<unk>', 'match', 'for', 'general', 'and', 'had', 'a']\n",
      "['evenly', 'grid', 'hosts', 'allowing', 'express', 'into', 'a', 'field', 'to', 'set', 'another', 'transportation', 'for', 'anti-abortionists', 'would-be', 'criminals', 'and', 'what', 'they', 'need', 'to', 'convince', 'anti-abortion', 'activists', 'such', '<unk>', 'and', 'minorities', 'hispanics', 'feel', 'so', 'early', 'to', 'such', 'risk', 'serious', 'problems', 'squeezed', 'back', 'into', 'different', 'cocoa', 'and', '<unk>', '<unk>', 'and', 'after', 'august', 'imports', 'from']\n",
      "['enforcement', 'and', 'arthur', '<unk>', 'an', 'attorney', '<unk>', '<unk>', 'a', 'former', 'unisys', 'called', 'rep.', 'mario', '<unk>', 'who', 'earns', 'the', 'old', 'guard', 'to', 'establish', 'a', 'percentage', 'to', 'one', 'morning', 'that', 'were', 'made', 'by', 'who', 'joined', 'in', 'the', 'socialist', 'world', 'interested', 'in', 'casting', 'and', 'local', 'congressman', 'mario', '<unk>', 'who', 'would', 'urge', 'to', 'the']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:02,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:41:10], Epoch [42/50], loss: 3.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:50:17], Epoch [43/50], loss: 3.3422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:04,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [6:59:22], Epoch [44/50], loss: 3.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:03,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:08:25], Epoch [45/50], loss: 3.3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:17:33], Epoch [46/50], loss: 3.3309\n",
      "[Generated Sentences]\n",
      "['friend', 'represented', 'four', 'of', 'the', 'guarantee', 'for', 'the', 'past', 'few', 'days', 'they', 'were', '<unk>', 'one', 'percentage', 'point', 'less', 'than', 'for', 'record', 'declines', 'and', 'it', 'is', 'reset', 'then', 'is', 'a', 'cut', 'in', 'the', 'market', 'public', 'subsidies', 'which', '<unk>', '$', 'N', 'billion', 'of', 'the', 'company', \"'s\", 'earning', 'loans', 'and', 'repay', '$', 'N']\n",
      "['welcomed', 'hatch', 'stuff', 'gets', 'too', 'broker', 'notes', 'they', 'could', \"n't\", 'resume', 'production', 'in', 'sci', 'tv', 'stocks', 'in', 'futures', 'to', 'make', 'the', 'tokyo', 'and', 'highland', 'valley', 'mine', 'in', 'the', 'restaurant', 'and', 'financial', 'services', 'which', 'has', 'been', 'expected', 'to', 'reach', 'that', 'when', 'the', 'banks', 'will', 'move', 'out', 'reassuring', 'and', 'hospitals', 'to', 'fuel']\n",
      "['conspiracy', 'granted', 'a', 'move', 'said', 'in', 'the', 'great', 'depression', 'and', 'its', 'merchant', 'banking', 'committee', 'has', 'fallen', 'below', 'this', 'year', \"'s\", 'u.n.', 'desire', 'to', 'explain', 'some', 'of', 'the', 'projects', 'such', 'as', 'an', '<unk>', 'experience', 'the', '<unk>', 'are', 'made', 'to', 'explore', 'any', 'and', 'that', 'argument', 'yet', 'said', 'in', 'N', 'favored', 'by', 'a']\n",
      "['ensuring', 'richmond', 'cohen', 'cable', 'brand', 'inc', 'employees', 'lowered', 'the', 'stock', 'interests', 'in', 'the', 'nov.', 'N', 'leveraged', 'buy-out', 'venture', 'is', 'said', 'to', 'drain', 'liquidity', 'by', '<unk>', '<unk>', 'but', 'final', 'orders', 'at', 'midnight', 'tonight', 'yesterday', 'soo', 'shares', 'will', 'assume', '$', 'N', 'range', 'on', 'next', 'levels', 'of', 'billions', 'of', 'dollars', 'taken', 'advertising', 'to']\n",
      "['frustration', 'immediately', 'squeezed', 'off', 'cool', 'mr.', 'steinhardt', '<unk>', '<unk>', 'advertising', 'deaths', 'among', 'diabetics', 'and', 'is', 'riding', 'a', 'little', 'richer', 'more', 'sluggish', 'pace', 'for', 'the', 'past', 'six', 'months', 'to', 'be', 'delivered', 'about', 'a', 'record', 'kind', 'of', 'farm', 'significance', 'is', 'to', 'name', 'but', 'the', 'sec', 'chairman', '<unk>', '<unk>', 'and', '<unk>', '<unk>', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:08,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:26:41], Epoch [47/50], loss: 3.3211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:07,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:35:49], Epoch [48/50], loss: 3.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:10,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:44:59], Epoch [49/50], loss: 3.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [09:42,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time [7:54:42], Epoch [50/50], loss: 3.3092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "batch_size = 256\n",
    "\n",
    "dataset = LMDataset(tokens)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n",
    "# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "trainer = Trainer(word2idx = word2idx,\n",
    "                idx2word = idx2word,\n",
    "                dataloader=dataloader, \n",
    "                model = model,\n",
    "                criterion=criterion,\n",
    "                optimizer = optimizer,\n",
    "                device=device)\n",
    "\n",
    "trainer.train(epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습2. 파이토치를 활용하여 LSTM 사용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습에선 PyTorch를 활용한 LSTM 사용법을 익히겠습니다.\n",
    "\n",
    "> Reference: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2f0a9df4150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn을 활용하여 LSTM cell을 생성하는 방법은 다음과 같습니다.\n",
    "\n",
    "- input_size: The number of expected features in the input x\n",
    "- hidden_size: The number of features in the hidden state h\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size: 3, hidden_size: 3으로 설정하여 LSTM cell을 생성합니다.\n",
    "lstm = nn.LSTM(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM cell을 생성한 후엔, 입력으로 들어갈 input x, hidden state h, cell state c를 생성해야 합니다. \n",
    "\n",
    "위에서 정한 input_size와 hidden_size를 고려하여 inputs와 hidden(h와 c)을 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length가 5인 input을 생성합니다.\n",
    "# 이때, input_size를 3으로 설정했기에 3차원 벡터 5개를 생성해야 합니다.\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "\n",
    "# lstm은 input x와 hidden state h를 입력으로 받기 때문에, hidden state도 생성해 줍니다.\n",
    "# 이때, hidden_size를 3으로 설정했으므로, 3차원 벡터를 생성합니다.\n",
    "# lstm의 입력으로 들어가는 h는 RNN에서의 hidden state와 lstm에서 등장한 개념인 cell state로 구성되어 있기 때문에\n",
    "# hidden은 3차원 벡터 2개로 구성되어야 합니다.\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**방법 1:** Sequence length가 5인 input에 대하여 한 번에 하나의 element를 lstm cell에 통과시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**방법 2:** 전체 시퀀스를 한 번에 통과시키는 방법도 있습니다.\n",
    "\n",
    "LSTM이 반환하는 출력의 첫 번째 값은 전체 시퀀스에 대한 통과한 hidden state이고, 두 번째 값은 마지막 step의 hidden state입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1) # 방법 2를 적용하기 위해 input을 list가 아닌 하나의 tensor로 만듭니다.\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3)) # 방법 2를 적용하기 위해 hidden을 다시 초기화합니다.\n",
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>)\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) LSTM을 이용해 PoS Tagging 학습하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM을 이용해 Part-of-Speech(PoS) Tagging을 하기 위해 학습 데이터를 준비합니다. \n",
    "\n",
    "- training_data에는 단어 시퀀스와 각 단어의 품사 태그를 준비해야 합니다.\n",
    "- word_to_ix는 모델의 입력으로 사용하기 위해 각 단어를 id로 mapping한 것을 저장합니다.\n",
    "- tag_to_ix는 품사 태그 또한 id로 mapping하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix: # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix) # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2} # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer, output layer, lstm cell을 포함한 LSTMTagger 모듈을 정의합니다. \n",
    "\n",
    "- **embeds:** input id를 embedding layer로 encode하여 input에 해당하는 embedding을 생성합니다.\n",
    "- **lstm_out:** embedding을 lstm에 통과하여 전체 시퀀스에 대한 hidden state를 저장합니다.\n",
    "- **tag_space:** lstm의 output인 hidden을 이용해 존재하는 tag (DET, NN, V) 공간으로 linear transform합니다. \n",
    "- **tag_scores:** 이후softmax를 적용하여 각 tag가 될 score를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hiddem_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# loss function, optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습하기 전 tag score를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i, j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 training data를 이용해 모델을 학습합니다. 즉, input을 LSTMTagger에 통과시켜 각 단어의 PoS tag를 예측하고, 정답 tag와 비교하여 loss를 계산한 후 loss를 backpropagate하여 모델 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4. Compute the loss, gradients and update the parameters by calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터를 이용해 모델이 잘 학습되었는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0462, -4.0106, -3.6096],\n",
      "        [-4.8205, -0.0286, -3.9045],\n",
      "        [-3.7876, -4.1355, -0.0394],\n",
      "        [-0.0185, -4.7874, -4.6013],\n",
      "        [-5.7881, -0.0186, -4.1778]])\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    # The sentence is \"the dog ate the apple\". i, j corresponds to score for tag j for word i.\n",
    "    # The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1 \n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) GRU를 이용하여 Pos Tagging 학습하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(gru_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습하기 전 tag score를 확인하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1221, -0.9948, -1.1887],\n",
      "        [-0.9758, -1.0355, -1.3166],\n",
      "        [-0.9364, -1.2581, -1.1277],\n",
      "        [-1.0418, -0.9920, -1.2861],\n",
      "        [-1.2863, -0.7497, -1.3815]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습하고 tag score를 다시 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.1540e-03, -5.5202e+00, -5.7687e+00],\n",
      "        [-5.9891e+00, -3.9923e-03, -6.5167e+00],\n",
      "        [-4.7706e+00, -5.0780e+00, -1.4817e-02],\n",
      "        [-4.7149e-03, -6.5471e+00, -5.7231e+00],\n",
      "        [-6.2363e+00, -3.1703e-03, -6.7185e+00]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 실습은 한 가지 문제가 있습니다. 바로 훈련 데이터로 테스트까지 한다는 것입니다. 학습데이터를 테스트에도 사용한다면 객관적으로 성능을 판단할 수 없습니다. 이미 학습데이터의 답을 알고 그 답에 가까워지도록 학습되었기 때문입니다. 그렇기에 데이터를 최소한 학습과 테스트 데이터로 나누어 사용해야 실질적인 성능을 측정할 수 있습니다. 그렇기에 데이터를 최소한 학습과 테스트 데이터로 나누어 사용해야 실질적인 성능을 측정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Attention Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 seq2seq 모델은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들었습니다. \n",
    "\n",
    "하지만 RNN에 기반한 seq2seq 모델에는 크게 두 가지 문제가 있습니다. \n",
    "\n",
    "1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하기에 정보 손실이 발생합니다.\n",
    "2. RNN의 고질적 문제인 기울기 소실 문제가 존재합니다.\n",
    "\n",
    "이를 보정하기 위한 방법 중 하나로 어텐션 기법이 나왔습니다. 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다. \n",
    "\n",
    "key-value 자료형을 가지고 어텐션 함수에 대해서 알아보겠습니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG\">\n",
    "\n",
    "어텐션을 함수로 표현하면 주로 다음과 같이 표현됩니다.   \n",
    "**Attention(Q, K, V) = Attention Value**\n",
    "\n",
    "어텐션 함수는 주어진 '쿼리'에 대해서 모든 '키'와의 유사도를 각각 구합니다. 그리고 구해낸 유사도를 키와 맵핑되어있는 각각의 '값'에 반영해줍니다. 긜고 유사도가 반영된 '값'을 모두 더해서 리턴합니다. 여기서 이 값을 어텐션 값이라고 하겠습니다.\n",
    "\n",
    "지금부터 살펴볼 seq2seq + 어텐션 모델에서 Q, K, V는 다음을 의미합니다.\n",
    "\n",
    "> Q = Query : t 시점의 디코더 셀에서 은닉 상태  \n",
    "> K = Keys : 모든 시점의 인코더 셀의 은닉 상태들  \n",
    "> V = Values : 모든 시점의 인코더 셀의 은닉 상태들  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Dot-Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 어텐션 중 수식적으로 가장 이해하기 쉬운 닷-프로덕트 어텐션을 통해 어텐션을 이해해보겠습니다. seq2seq에서 사용되는 어텐션 중에서 닷-프로덕트 어텐션과 다른 어텐션의 차이는 주로 중간 수식의 차이로 매커니즘 자체는 거의 유사합니다. \n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG\">\n",
    "\n",
    "위 그림은 디코더의 세번째 LSTM 셀에서 출력 단어를 예측할 때, 어텐션 매커니즘을 사용하는 모습을 보여줍니다. 디코더의 첫번째, 두번째 LSTM 셀은 이미 어턴션 매커니즘을 통해 je와 suis를 예측하는 과정을 거쳤다고 합시다. 디코더의 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 인코더의 모든 입력 단어들의 정보를 다시 한 번 참고하고자 합니다. 중간 과정에 대한 설명은 현재는 생략하고 여기서 주목할 것은 인코더의 소프트맥스 함수입니다.\n",
    "\n",
    "소프트맥스 함수를 통해 나온 결과값은 I, am, a, student 단어 각각이 출력 단어를 예측할 때, 얼마나 도움이 되는지의 정도를 수치화한 값입니다. 위 그림에서 빨간 직사각형의 크기로 소프트맥스 함수의 결과값의 크기가 표현되어 있습니다. 각 입력 단어가 디코더의 예측에 도움이 되는 정도가 수치화하여 측정되면 이를 하나의 정보로 담아서 디코더로 전송됩니다. 위 그림에서 초록색 삼각형이 이에 해당합니다. 결과적으로, 디코더는 출력 단어를 더 정확하게 예측할 확률이 높아집니다. 이제 구체적으로 과정을 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 어텐션 스코어를 구한다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG\">\n",
    "\n",
    "인코더의 시점(time step)을 각각 1, 2, ..., N이라고 했을 때 인코더의 은닉 상태(hidden state)를 각각 $h_1, h_2, ..., h_N$라고 하겠습니다. 디코더의 현재 시점 t에서의 디코더의 은닉 상태를 $s_t$라고 하겠습니다. 또한 여기서는 인코더의 은닉 상태와 디코더의 은닉 상태의 차원이 같다고 가정합니다. 위의 그림의 경우에는 인코더의 은닉 상태와 디코더의 은닉 상태가 동일하게 차원이 4입니다.\n",
    "\n",
    "어텐션 매커니즘의 첫 걸음인 어텐션 스코어에 대해서 배우기전에, 이전 챕터에서 배웠던 디코더의 현재 시점 t에서 필요한 입력값을 다시 상기해보겠습니다. 시점 t에서 출력 단어를 예측하기 위해서 디코더의 셀은 두 개의 입력값을 필요로 하는데, 바로 이전 시점인 t-1의 은닉 상태와 이전 시점 t-1에 나온 출력 단어입니다.\n",
    "\n",
    "그런데 어텐션 매커니즘에서는 출력 단어 예측에 또 다른 값을 필요로 하는데 바로 어텐션 값이라는 새로운 값입니다. t번째 단어를 예측하기 위한 어텐션값을 $a_t$이라고 정의하겠습니다. 지금부터 구하는 모든 과정은 $a_t$를 구하기 위한 과정입니다. \n",
    "\n",
    "먼저 어텐션 스코어를 구해야 합니다. 어텐션 스코어란 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지를 판단하는 스코어값입니다. \n",
    "\n",
    "닷-프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 $s_t$를 전치(transpose)하고 각 은닉 상태와 내적(dot product)을 수행합니다. 즉, 모든 어텐션 스코어 값은 스칼라입니다. 예를 들어 $s_t$와 인토더의 i번째 은닉 상태의 어텐션 스코어의 계산 방법은 아래와 같습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22893/i%EB%B2%88%EC%A7%B8%EC%96%B4%ED%85%90%EC%85%98%EC%8A%A4%EC%BD%94%EC%96%B4_final.PNG\">\n",
    "\n",
    "어텐션 스코어 함수를 정의해보면 다음과 같습니다.\n",
    "\n",
    "**$score(s_t, h_i) = s_t^T h_i$**\n",
    "\n",
    "$s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값을 $e^t$라고 정의하겠습니다. $e^t$의 수식은 다음과 같습니다.\n",
    "\n",
    "**$e^t = [s_t^T h_1, ..., s_t^T h_N]$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 소프트맥스 함수를 통해 어텐션 분포를 구한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG\">\n",
    "\n",
    "$e^t$에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포를 얻습니다. 이를 어텐션 분포(Attention Distribution)라고 하며, 각각의 값은 어텐션 가중치(Attention Weight)라고 합니다. \n",
    "\n",
    "디코더의 시점 t에서의 어텐션 가중치의 모음값인 어텐션 분포를 $\\alpha^t$라고 할 때, 정의되는 식은 다음과 같습니다.\n",
    "\n",
    "**$\\alpha^t = softmax(e^t)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG\">\n",
    "\n",
    "이제 구했던 정보들을 하나로 합칠 시간입니다. 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 모두 더합니다. 즉, 가중합(Weighted Sum)을 진행합니다. 아래는 어텐션의 최종 결과, 어텐션 함수의 출력값인 어텐션 값 $a_t$에 대한 식을 보여줍니다.\n",
    "\n",
    "$$a_t = \\sum_{i=1}^N \\alpha_i^t h_i$$\n",
    "\n",
    "이러한 어텐션 값 $a_t$은 종종 인코더의 문맥을 포함하고 있다고하여, 컨텍스트 벡터라고도 불립니다. 앞서 배운 가장 기본적인 seq2seq에서는 인코더의 마지막 은닉 상태를 컨텍스트 벡터라고 부르는 것과 대조됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 어텐션 값과 디코더의 t시점의 은닉 상태를 연결한다.(Concatenate)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG\">\n",
    "\n",
    "어텐션 함수의 최종값인 어텐션 값을 구했습니다. 이제 어텐션 매커니즘은 $a_t$를 $s_t$와 결합(Concatenate)하여 하나의 벡터로 만드는 작업을 수행합니다. 이를 $v_t$라고 정의하겠습니다. 그리고 이 $v_t$를 $\\hat{y}$ 예측 연산의 입력으로 사용하여 인코더로부터 얻은 정보를 활용하여 $\\hat{t}$를 좀 더 잘 예측할 수 있게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) 출력층 연산의 입력이 되는 $\\tilde{s_t}$를 계산합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/22893/st.PNG\">\n",
    "\n",
    "논문에서는 $v_t$를 바로 출력층으로 보내기 전에 신경망 연산을 한 번 더 추가하였습니다. 가중치 행렬과 곱한 후에 tanh 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터 $\\tilde{s_t}$를 얻습니다. 어텐션 매커니즘을 사용하지 않는 seq2seq에서 출력층의 입력이 t시점의 은닉 상태인 $s_t$인 반면, 어텐션 매커니즘에서 출력층의 입력이 $\\tilde{s_t}$가 되는 셈입니다. \n",
    "\n",
    "식으로 표현하면 다음과 같습니다. 이때 $\\mathbf{W_c}$는 학습 가능한 가중치 행렬, $b_c$는 편향입니다. 그림에선 편향이 생략되었습니다.\n",
    "\n",
    "$$\\tilde{s_t} = tanh(\\mathbf{W_c}[a_t;s_t] + b_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6) $\\tilde{s_t}$를 출력층의 입력으로 사용합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{s_t}$를 출력층의 입력으로 사용하여 예측 벡터를 얻습니다.\n",
    "\n",
    "$$\\hat{y_t} = Softmax(W_y \\tilde{s_t} + b_y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 다양한 종류의 어텐션(Attention)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 앞서 배운 어텐션은 어텐션 스코어 함수로 내적을 사용했습니다. 그리고 그 외에 다른 함수를 사용하는 어텐션 방법들이 있습니다.\n",
    "\n",
    "|이름|스코어 함수|Definded by|\n",
    "|:---|:---|:---|\n",
    "|$dot$|$score(s_t, h_i) = s_t^T h_i$|Luong et al. (2015)|\n",
    "|$scaled dot$|$score(s_t, h_i) = \\frac{s_t^t h_i}{\\sqrt{n}}$|Vaswani et al. (2017)|\n",
    "|$general$|$score(s_t, h_i) = s_t^T W_a h_i$, 단 $W_a$는 학습 가능한 가중치 행렬|Luong et al. (2015)|\n",
    "|$concat$|$score(s_t, h_i) = W_a^T tanh(W_b[s_t;h_i])score(s_t, h_i) = W_a^T tanh(W_b s_t + W_c h_i)$|Bahdanau et al. (2015)|\n",
    "|$location-base$|$\\alpha_t = softmax(W_a s_t)$, $\\alpha_t$ 산출 시에 $s_t$만 사용하는 방법|Luong et al. (2015)|\n",
    "\n",
    "위 함수들은 제안한 사람의 이름을 따서 루옹 어텐션, 바다나우 어텐션 등으로도 불립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4 다양한 분야에서 사용되는 어텐션**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Captioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagecaptioning](_image/imagecaptioning.PNG)\n",
    "\n",
    "이미지를 여러 부분으로 나눠서 각 부분을 보고 각 단어와 유사도를 판단하여 가장 유사한 단어들의 가중치를 늘리는 방법을 사용합니다. 위 그림의 설명들은 다음과 같습니다.\n",
    "\n",
    "- $z_i$: i번째 weight들\n",
    "- $y_i$: i번째 input 단어\n",
    "- $a_i$: 다음 단어를 생성하기 위해 보고자 하는 패턴을 가진 벡터\n",
    "- $d_i$: i번째 output 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visual Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vqa](_image/vqa.PNG)\n",
    "\n",
    "이미지를 주고 질문을 할 때, 그에 맞는 이미지를 추려주는 것입니다. LSTM으로 질문을 먼저 이해하여 가중치를 조절한 뒤, 어텐션이 포함된 RNN을 사용하여 답을 구합니다. 예를 들어 위 그림에선 'brown'에 가중치가 커져서 여러 음식 중 갈색인 빵을 출력하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **seq2seq**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 공부했던 seq2seq에 대해 간단한 이미지와 github를 참고할 수 있습니다.\n",
    "\n",
    "![seq2seq](_image/seq2seq.PNG)\n",
    "\n",
    "https://github.com/google/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Generative Adversarial Network(GAN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1 GAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN이 동작하는 방식을 경찰과 지패위조범으로 간단하게 먼저 알아보겠습니다.\n",
    "\n",
    "<img src = \"https://blogfiles.pstatic.net/MjAxOTA2MDlfMjA1/MDAxNTYwMDA3NTg1NTU1.GtC-256rGjaWgnMgW4Z6B_RtTCYvT9GvU00Nk89swqYg.J8yIs9VeVKJ7VmwpsSMpZVsDnDE4nAG99TJlvB1WLoIg.PNG.euleekwon/image.png?type=w1\">\n",
    "\n",
    "1. 처음 0과 1이 무작위로 흩뿌려진 noise가 있습니다. 위 수식에선 z라고 표시됩니다.\n",
    "2. 해당 noise를 가지고 generator(위조 지폐범)가 위조지폐를 만듭니다. G가 z를 가지고 만들었으니 만들어진 위조지폐는 G(z)라고 하겠습니다. \n",
    "3. 이제 Discriminator(경찰)가 위조 지폐와 실제 지폐를 구분합니다. 경찰은 위조지폐라면 0을, 진짜 지폐라면 1을 출력합니다. 위조 지폐 G(z)와 실제 지폐 x가 넘어갔을 때, D(G(z))는 위조 지폐이므로 0, D(x)는 실제 지폐이므로 1을 출력합니다. \n",
    "\n",
    "위와 같은 epoch를 반복하며 경찰은 위조 지폐를 더욱 잘 구분하도록, 위조지폐범은 더 진짜 같은 위조지폐를 만들도록 노력할 것이다. 그러다가 너무나도 완벽한 위조지폐가 탄생한다면 결국 해당 지폐를 구분하지 못하게 되고 확률은 50%가 될 것입니다. 이때 학습을 끝내게 됩니다. \n",
    "\n",
    "이제 Disriminator와 Generator 모델을 더 살펴보겠습니다. D는 지도학습인 반면에 G는 비지도 학습입니다. 즉, D모델은 어떠한 input 데이터가 들어갔을 때, 해당 input 값이 어떤 것인지 classify하고 Generator 모델은 어떤 latent code(잠재적인 코드)와 해당 데이터를 가지고 training 데이터가 되도록 학습하는 과정을 거칩니다. 또한 Generator는 실제 데이터 분포가 나타내는 확률 분포 그래프와 유사한 모델을 제작하려 노력합니다. \n",
    "\n",
    "![확률분포](_image/확률분포.PNG)\n",
    "\n",
    "GAN에서 Discriminator 입장에선 진짜와 가짜를 잘 구별하도록 학습됩니다. 그렇기에 G(z)와 x의 차원은 이미지와 같이 고차원 vector일지라도 output은 0 또는 1의 값을 가지게 됩니다. Genrateor 입장에선 잘 맞추는 것은 관심이 없습니다. 오직 Discriminator를 얼마나 속일 수 있는지가 유일한 목적이 됩니다. 그렇기에 D(G(z))가 1이 되도록 학습합니다. \n",
    "\n",
    "objective function을 보면 다음과 같이 Discriminator와 Generator 관점으로 살펴볼 수 있습니다. 먼저 Discriminator 관점에서 본 식입니다.\n",
    "\n",
    "![D_function](_image/discriminator_function.PNG)\n",
    "\n",
    "위에서 볼 수 있듯, D(G(z)) = 0, D(x) = 1이 되어야 D는 최대값을 가질 수 있습니다. D는 이 방향으로 학습합니다.\n",
    "\n",
    "![G_function](_image/generator_function.PNG)\n",
    "\n",
    "그러나 G는 반대로 작용합니다. 먼저 앞 텀은 G와 독립적이므로 G는 영향을 받거나 미치지 않습니다. 그리고 D(G(z)) = 1이 되는 방향으로 학습합니다. 이를 간단하게 코드로 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def GAN_train():\n",
    "    # Discriminator\n",
    "    D = nn.Sequential(\n",
    "        nn.Linear(784, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    # Generator\n",
    "    G = nn.Sequential(\n",
    "        nn.Linear(100, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 784),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCELoss() # binary cross entropy loss\n",
    "\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), lr = 0.01)\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), lr = 0.01)\n",
    "\n",
    "    # Assume x be real images of shape (batch_size, 784)\n",
    "    # Assume z be random noise of shape (batch_size, 100)\n",
    "\n",
    "    while True:\n",
    "        # train D\n",
    "        loss = criterion(D(x), 1) + criterion(D(G(z)), 0) # min (D(x) - 1)^2 + min D(G(z))^2\n",
    "        loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # train G\n",
    "        loss = criterion(D(G(z)), 1) # min (D(G(z)) - 1)^2\n",
    "        loss.backward()\n",
    "        g_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복습차원으로 구동 방식을 보면 다음과 같습니다.\n",
    "\n",
    "<img src = \"https://i.imgur.com/jNAXwhE.png\">\n",
    "\n",
    "위와 같이 GAN은 minmax문제를 푸는 과정입니다. 즉, saddle point를 찾는 문제입니다. 그렇기에 이론적으론 고정된 해답에 수렴하는 것처럼 보이지만 실제론 불안정한 구조적 단점을 보이게 됩니다. 이를 해결하기 위해 기존의 fully-connected 구조의 대부분을 CNN구조로 대체한 DCGAN이 나오게 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN을 보기 전에 loss 함수에 대해 좀 더 살펴보겠습니다.\n",
    "\n",
    "Generator는 Discriminator를 속이는 것에 성공할 수록 $D_{\\theta}(G_{\\phi}(z)) \\approx 1$ 낮은 loss를 갖게 됩니다. \n",
    "\n",
    "하지만, 이미지 생성의 난이도를 생각하면, 학습 초반에 Discriminator에 비해 Generator가 못하는 일은 자명한 일입니다. 이 때, D(G(z))가 0에 가까운 지점 $D_{\\theta}(G_{\\phi}(z)) \\approx 0$ 에서의 함수의 기울기가 너무 작기 때문에 학습 초반에 Generator가 충분한 양의 학습 시그널을 받지 못하게 되는 문제점이 발생하게 됩니다.\n",
    "\n",
    "따라서, 위의 식과 직관적으로 유사한 의미를 가지는 다른 loss function을 정의해보도록 하겠습니다.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{G_{\\phi}} = -E_{z}[\\log(D_{\\theta}(G_{\\phi}(z))]  \\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 DCGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN(Deep Convolutional GAN)은 기존 GAN에 대하여 다음과 같은 변경점이 있습니다. 이 내용은 2016년 발표된 논문을 기초로 합니다.\n",
    "\n",
    "논문: [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks - Alec Radford el ec, 2016](https://arxiv.org/abs/1511.06434)\n",
    "\n",
    "- Generator의 pooling layers를 **fractional-strided convolutions** 로, Discriminator의 pooling layer들을 **strided convolutions** 로 바꿉니다.\n",
    "- Generator와 Discriminator에 batch-normalization을 사용합니다. 이를 통해 deep generator의 초기 실패를 막습니다. 그러나 모든 layer에 적용하면 sample oscillation과 model instability 문제가 발생하여 Generator output layer와 Discriminator input layer에는 적용하지 않습니다.\n",
    "- Fully-connected hidden layers를 삭제합니다.\n",
    "- Generator에서 모든 활성화 함수로 ReLU를 사용하되 마지막 결과에서만 Tanh를 사용합니다.\n",
    "- Discriminator에서 모든 활성화 함수로 Leaky ReLU를 사용합니다.\n",
    "\n",
    "### **Fractionally-strided convolutions**\n",
    "<img src = \"https://angrypark.github.io/images/2017-08-03-DCGAN-paper-reading/padding_strides_transposed.gif\">\n",
    "\n",
    "### **Strided convolutions**\n",
    "<img src = \"https://angrypark.github.io/images/2017-08-03-DCGAN-paper-reading/padding_strides.gif\">\n",
    "\n",
    "여기서 Fractionally-strided convolutions(transpose convolution or deconvolution)은 input에 padding을 하고 convolution을 하면서 오히려 크기가 커지는 특징이 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1h6nfvYwN8n"
   },
   "source": [
    "DCGAN의 Generator와 Discriminator의 구조는 아래와 같습니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mp8jgDC5CDoZQNSGnq3kQRwSNQA7TIXl)\n",
    "\n",
    "\n",
    "이 때, Generator는 output의 width와 height를 키우는 convolution을 진행해주어야 하기 때문에, standard한 convolution operation이 아닌 deconvolution 혹은 transpose convolution이라고 불리는 연산을 통해 output의 size를 키워주는 연산을 진행하게 됩니다.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1mqoDvM3a4qnnu9IH60isrXtN7-RB_vgD)\n",
    "\n",
    "반대로, Discriminator는 Generator와 대칭되는 구조를 통해 standard한 convolution을 사용하여 classification을 진행해주게 됩니다.\n",
    "\n",
    "Transpose Convolution:(https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.3 CGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGAN(conditional GAN)은 x뿐만 아니라 데이터의 정답 레이블 정보 y도 GAN에 적용한 방법입니다. \n",
    "\n",
    "<img src = \"https://i.imgur.com/pSICG3J.png\">\n",
    "\n",
    "$$\\min _{ G }{ \\max _{ D }{ V\\left( D,G \\right)  }  } ={ E }_{ x\\sim { p }_{ data }\\left( x \\right)  }\\left[ \\log { D\\left( x,y \\right)  }  \\right] +{ E }_{ z\\sim { p }_{ z }\\left( z \\right)  }\\left[ \\log { \\left\\{ 1-D\\left( G\\left( z,y \\right), y\\right)  \\right\\}  }  \\right]$$\n",
    "\n",
    "objective function을 보면 $y$가 $D$와 $G$에 추가된 것 외에는 같은 것을 확인할 수 있습니다. 이렇게 구성할 경우 zero-mean gaussian으로 생성한 z와 우리가 생성하고 싶은 범주의 레이블 정보에 해당하는 벡터 y를 함께 넣어 원하는 데이터를 생성할 수 있게 됩니다. G가 z뿐만 아니라 y의 정보도 함께 고려하여 데이터를 생성하기에 z가 속한 벡터공간을 해석하기 쉬워졌다고 볼 수도 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.4 ACGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://i.imgur.com/IUR0e8Q.png\">\n",
    "\n",
    "ACGAN(Auxiliary Classifier GAN)은 GAN에서 classifier 하나를 더 추가한 것입니다. 데이터가 실제인지 가짜인지 판별하는 것 외에 해당 데이터의 클래스를 분류합니다. 이 덕분에 ACGAN으로 생성된 데이터는 다른 분류기에 넣어도 범주 분류가 잘 된다고 합니다. G는 CGAN처럼 레이블 정보와 z를 합쳐 가짜 데이터를 생성합니다. \n",
    "\n",
    "ACGAN의 objective function은 다음과 같습니다.\n",
    "\n",
    "$$L_s = E[log_p(S = real|X_{real})] + E[log_p(S=fake|X_{fake})]$$\n",
    "$$L_c = E[log_p(C = c|X_{real})] + E[log_p(C=c|X_{fake})]$$\n",
    "\n",
    "$L_s$는 기존 GAN의 $D$ 목적함수와 동일합니다. 다시 말해 해당 데이터가 진짜인지 가짜인지 판별해내는 것과 관련있습니다. $L_d$는 해당 데이터의 범주를 분류하는 것에 해당합니다. \n",
    "\n",
    "$D$는 $L_s+L_c$를, $G$는 $L_c-L_s$를 최대화하도록 학습됩니다. \n",
    "\n",
    "ACGAN은 class를 D에 직접 주지 않고 추측하도록 학습시키지만 CGAN은 D에게 class를 직접 주는 차이 외에는 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에 종류에 대해서도 알고 싶다면 [이 곳](https://ratsgo.github.io/generative%20model/2017/12/21/gans/)을 참고하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. DCGAN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습에서는 DCGAN을 구현해서 이미지를 직접 생성해보겠습니다.\n",
    "\n",
    "- dataset: CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- model: DCGAN (https://arxiv.org/abs/1511.06434)\n",
    "- evaluation: Inception Score (https://arxiv.org/abs/1801.01973)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0) Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for reproducibility\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) DataLoader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 실습들에서 사용한 것과 마찬가지로, pre-defined된 CIFAR-10 dataset을 활용해서 dataloader를 만들어 두겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def create_dataloader(batch_size=64, num_workers=1):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, transform=transform, download=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data/', train=False, transform=transform, download=True)\n",
    "\n",
    "    trainloader = data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Convolution Block**\n",
    "\n",
    "우선 모델을 쉽게 구현할 수 있도록, Generator와 Discriminator에서 반복적으로 사용할 convolution block과 deconvolution block을 정의하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bias=False, norm='bn', activation=None):\n",
    "    layers = []\n",
    "    \n",
    "    # Conv\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=bias))\n",
    "    \n",
    "    # Normalization\n",
    "    if norm == \"bn\":\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    elif norm == None:\n",
    "        pass\n",
    "    \n",
    "    # Activation\n",
    "    if activation == \"lrelu\":\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == None:\n",
    "        pass\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, output_padding=0, bias=False, norm='bn', activation=None):\n",
    "    layers = []\n",
    "    \n",
    "    # Deconv\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, output_padding, bias=bias))\n",
    "    \n",
    "    # Normalization\n",
    "    if norm == 'bn':\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    elif norm == None:\n",
    "        pass\n",
    "    \n",
    "    # Activation\n",
    "    if activation == 'lrelu':\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    elif activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == None:\n",
    "        pass\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        model = []\n",
    "        \n",
    "        # DCGAN Generator\n",
    "        model.append(deconv(256, 256, 4, 1, 0, norm='bn', activation='relu'))\n",
    "        model.append(deconv(256, 128, 4, 2, 1, norm='bn', activation='relu'))\n",
    "        model.append(deconv(128, 64, 4, 2, 1, norm='bn', activation='relu'))\n",
    "        model.append(deconv(64, 3, 4, 2, 1, norm=None, activation='tanh'))\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Input (z) size : [Batch, 256, 1, 1]\n",
    "        # Output (Image) size : [Batch, 3, 32, 32]\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        output = self.model(z)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = []\n",
    "\n",
    "        # DCGAN Discriminator\n",
    "        model.append(conv(3, 64, 4, 2, 1, norm='bn', activation='lrelu'))\n",
    "        model.append(conv(64, 128, 4, 2, 1, norm='bn', activation='lrelu'))\n",
    "        model.append(conv(128, 256, 4, 2, 1, norm='bn', activation='lrelu'))\n",
    "        model.append(conv(256, 1, 4, 1, 0, norm=None, activation=None))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Input (z) size : [Batch, 3, 32, 32]\n",
    "        # Output (probability) size : [Batch, 1]\n",
    "        output = self.model(x).squeeze()\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Generator와 Discriminator가 잘 구현됐는지 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Model Initializer Test Case======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "All 2 tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    print(\"=====Model Initializer Test Case======\")\n",
    "    netG = Generator()\n",
    "    # the first test\n",
    "    try:\n",
    "        netG.load_state_dict(torch.load(\"data/sanity_check_dcgan_netG.pth\", map_location='cpu'))\n",
    "    except Exception as e:\n",
    "        print(\"Your DCGAN generator initializer is wrong. Check the comments in details and implement the model precisely.\")\n",
    "        raise e\n",
    "    print(\"The first test passed!\")\n",
    "\n",
    "    # the second test\n",
    "    netD = Discriminator()\n",
    "    try:\n",
    "        netD.load_state_dict(torch.load(\"data/sanity_check_dcgan_netD.pth\", map_location='cpu'))\n",
    "    except Exception as e:\n",
    "        print(\"Your DCGAN discriminator initializer is wrong. Check the comments in details and implement the model precisely.\")\n",
    "        raise e\n",
    "    print(\"The second test passed!\")\n",
    "    print(\"All 2 tests passed!\")\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Inception Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본격적으로 학습을 진행하기 전에 지도학습과 다르게 한 가지 추가적으로 필요한 것이 있습니다.\n",
    "\n",
    "기존의 지도학습에서는 loss나 validation accuracy를 통해 학습이 원활히 진행되고 있는지 모니터링했습니다. 그러나 GAN은 generator가 discriminator를 잘 속이고 있을지라도 (i.e. 낮은 loss) discriminator가 학습이 충분히 되지 못했다면 낮은 퀄리티의 이미지가 생성됩니다.\n",
    "\n",
    "이미지 퀄리티를 측정하는 방법은 크게 2가지 입니다.\n",
    "1. Fidelity(충실도): 얼마나 고품질의 이미지를 생성하는가?\n",
    "2. Diversity(다양성): 생성된 이미지들이 얼마나 다양한가? (e.g 고양이만 생성하지 않음)\n",
    "\n",
    "보통 충실도를 측정하기 위해서는 **Frechet Inception Distance** 라는 metric이, 다양성을 측정하기 위해서는 **Inception Score** 라는 evaluation metric이 사용되곤 합니다. \n",
    "\n",
    "이번 실습에서는 이미지의 다양성을 측정하는 Inception Score를 통해 학습이 원활히 진행되고 있는지 모니터링 하도록 하겠습니다. \n",
    "\n",
    "Inception score를 측정하는 방법은 아래와 같습니다.\n",
    "1. Generator를 통해 이미지를 N개 생성한다.\n",
    "2. 생성된 이미지들을 pre-trained된 inception network(googleNet)에 통과시킨다.\n",
    "3. inception network가 예측한 생성된 image의 label별 probability의 평균이 얼마나 diverse한지 측정한다.\n",
    "\n",
    "Inception score에 대한 자세한 내용은 아래를 참고해주세요.\n",
    "- https://arxiv.org/abs/1801.01973\n",
    "- https://cyc1am3n.github.io/2020/03/01/is_fid.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.inception import inception_v3\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class Inception_Score():\n",
    "    def __init__(self, dataset):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Dataset & DataLoader\n",
    "        self.N = len(dataset)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = data.DataLoader(dataset=dataset, batch_size=self.batch_size, num_workers=1)\n",
    "        self.transform = nn.Upsample(size=(299, 299), mode='bilinear').to(self.device)\n",
    "\n",
    "        # Inception Model\n",
    "        self.inception_model = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
    "        self.inception_model.eval()\n",
    "\n",
    "    def get_pred(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform(x)\n",
    "            x = self.inception_model(x)\n",
    "            return F.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    def compute_score(self, splits=1):\n",
    "        preds = np.zeros((self.N, 1000))\n",
    "\n",
    "        for i, batch in tqdm.tqdm(enumerate(self.dataloader)):\n",
    "            batch = batch.to(self.device)\n",
    "            batch_size_i = batch.size(0)\n",
    "            preds[i * self.batch_size : i * self.batch_size + batch_size_i] = self.get_pred(batch)\n",
    "\n",
    "        # Compute the mean KL-divergence\n",
    "        # You have to calculate the inception score.\n",
    "        # The logit values from inception model are already stored in 'preds'.\n",
    "        inception_score = 0.0\n",
    "        split_scores = []\n",
    "        for k in tqdm.tqdm(range(splits)):\n",
    "            part = preds[k * (self.N // splits): (k + 1) * (self.N // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores = []\n",
    "            for i in range(part.shape[0]):\n",
    "                pyx = part[i, :]\n",
    "                scores.append(entropy(pyx, py))\n",
    "            split_scores.append(np.exp(np.mean(scores)))\n",
    "        inception_score = np.mean(split_scores)\n",
    "\n",
    "        return inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Inception Score Test Case======\n",
      "Calculating Inception Score...\n"
     ]
    }
   ],
   "source": [
    "# CPU로 하기에 너무 느림\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# CIFAR10 Datset without Label\n",
    "class CIFAR10woLabel():\n",
    "    def __init__(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root='./data/', transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "        \n",
    "def test_inception_score():\n",
    "    print(\"======Inception Score Test Case======\")\n",
    "    print(\"Calculating Inception Score...\")\n",
    "\n",
    "    Inception = Inception_Score(CIFAR10woLabel())\n",
    "    score = Inception.compute_score(splits=1)\n",
    "\n",
    "    assert np.allclose(score, 9.719672, atol=1e-3), \\\n",
    "        \"Your inception score does not match expected result.\"\n",
    "\n",
    "    print(\"All test passed!\")\n",
    "\n",
    "test_inception_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Trainer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 선언한 dataloader, model, evaluator를 모두 활용해서 GAN을 학습시키는 Trainer를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preliminary**\n",
    "\n",
    "$$D_{\\theta}: \\text{Discriminator network} \\\\\n",
    "G_{\\phi}: \\text{Generator network} \\\\\n",
    "x: \\text{real image} \\\\\n",
    "z: \\text{latent vector}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Discriminator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}_{D_{\\theta}} = -E_{x \\sim p_{data}}[logD_{\\theta}(x) + E_{z}[\\log(1 - D_{\\theta}(G_{\\phi}(z)))]]\n",
    "\\end{equation}\n",
    "\n",
    "Discriminator loss는 위와 같이 real_image는 1으로, generated_image는 0으로 판별하는 방식으로 학습을 진행하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIHlU8M0cpPy"
   },
   "source": [
    "#### **Generator Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generator network는 이론적으로는 discriminator의 loss에서 generator가 해당되는 부분에 -1을 곱해서 표현할 수 있습니다.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{G_{\\phi}} = E_{z}[\\log(1-D_{\\theta}(G_{\\phi}(z))]  \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "하지만, 위의 식으로 학습을 진행할 경우 Generator의 학습이 원활히 이루어지지 않게되는 문제점이 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I0ImQBI_8oEr"
   },
   "outputs": [],
   "source": [
    "plt.title('log(1-D(G(z))')\n",
    "x = np.arange(0, 1.0, 0.01)\n",
    "y = np.log(1-x)\n",
    "plt.xlabel('D(G(z))')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnx579Fk9jBr"
   },
   "source": [
    "위의 loss plot에서 볼 수 있듯이, Generator는 Discriminator를 속이는 것에 성공할 수록 $D_{\\theta}(G_{\\phi}(z)) \\approx 1$ 낮은 loss를 갖게 됩니다. \n",
    "\n",
    "하지만, 이미지 생성의 난이도를 생각하면, 학습 초반에 Discriminator에 비해 Generator가 못하는 일은 자명한 일입니다. 이 때, D(G(z))가 0에 가까운 지점 $D_{\\theta}(G_{\\phi}(z)) \\approx 0$ 에서의 함수의 기울기가 너무 작기 때문에 학습 초반에 Generator가 충분한 양의 학습 시그널을 받지 못하게 되는 문제점이 발생하게 됩니다.\n",
    "\n",
    "따라서, 위의 식과 직관적으로 유사한 의미를 가지는 다른 loss function을 정의해보도록 하겠습니다.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{G_{\\phi}} = -E_{z}[\\log(D_{\\theta}(G_{\\phi}(z))]  \\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def save_checkpoint(model, save_path, device):\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    torch.save(model.cpu().state_dict(), save_path)\n",
    "    model.to(device)\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(\"Invalid path!\")\n",
    "        return\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.to(device)\n",
    "\n",
    "class FolderDataset(data.Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_list = os.listdir(folder)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(os.path.join(self.folder, self.image_list[index]))\n",
    "        return self.transform(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "# Trainer\n",
    "class Trainer():\n",
    "    def __init__(self, \n",
    "                trainloader, \n",
    "                testloader, \n",
    "                generator, \n",
    "                discriminator, \n",
    "                criterion,\n",
    "                g_optimizer, \n",
    "                d_optimizer, \n",
    "                device):\n",
    "        \"\"\"\n",
    "        trainloader: train data's loader\n",
    "        testloader: test data's loader\n",
    "        generator: generator\n",
    "        discriminator: discriminator\n",
    "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
    "        g_optimizer: optimizer for generator\n",
    "        d_optimizer: optimizer for discriminator\n",
    "        \"\"\"\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.G = generator\n",
    "        self.D = discriminator\n",
    "        self.criterion = criterion\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.device = device\n",
    "\n",
    "        # Make directory to save the images & models for a specific checkpoint\n",
    "        os.makedirs(os.path.join('./results/', 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('./results/', 'checkpoints'), exist_ok=True)\n",
    "        os.makedirs(os.path.join('./results/', 'evaluation'), exist_ok=True)\n",
    "        \n",
    "    def train(self, epochs = 1):\n",
    "        self.G.to(self.device)\n",
    "        self.D.to(self.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            for iter, (real_img, _) in enumerate(self.trainloader):\n",
    "                self.G.train()\n",
    "                self.D.train()\n",
    "                \n",
    "                batch_size = real_img.size(0)\n",
    "                real_label = torch.ones(batch_size).to(self.device)\n",
    "                fake_label = torch.zeros(batch_size).to(self.device)\n",
    "                # get real CIFAR-10 image\n",
    "                real_img = real_img.to(self.device)\n",
    "                # initialize latent_vector to feed into the Generator\n",
    "                z = torch.randn(real_img.size(0), 256).to(self.device)\n",
    "\n",
    "                ##########################################################################################\n",
    "                # Discriminator Loss 구현                                                                #\n",
    "                # Note : Discriminator Loss는 Generator network의 parameter에 영향을 주지 않아야 합니다. #\n",
    "                #        detach() function을 참고하세요.                                                 #\n",
    "                #        https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html              #\n",
    "                ##########################################################################################\n",
    "                D_loss: torch.Tensor = None\n",
    "                real_out = self.D(real_img)\n",
    "                fake_img = self.G(z)\n",
    "                fake_out = self.D(fake_img.detach())\n",
    "                D_loss = self.criterion(real_out, real_label) + self.criterion(fake_out, fake_label)\n",
    "\n",
    "                # TEST CODE\n",
    "                # (TEST의 통과가 맞는 구현을 보장하지는 못합니다. 일반적으로는 loss가 1.38~1.45 사이의 값이 나와야 합니다.)\n",
    "                if epoch == 0 and iter == 0:\n",
    "                    assert D_loss.detach().allclose(torch.tensor(1.4000), atol=2e-1), \\\n",
    "                    f\"Discriminator Loss of the model does not match expected result.\"\n",
    "                    print(\"==Discriminator loss function test passed!==\")\n",
    "\n",
    "                self.D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                #######################################################\n",
    "                # Generator Loss 구현                                 #\n",
    "                # Note : 위의 정의된 두번 째 식을 사용해서 구현하세요 #\n",
    "                #######################################################\n",
    "                G_loss: torch.Tensor = None\n",
    "                fake_img = self.G(z)\n",
    "                fake_out = self.D(fake_img)\n",
    "                G_loss = self.criterion(fake_out, fake_label)\n",
    "\n",
    "                # Test code\n",
    "                # (TEST의 통과가 맞는 구현을 보장하지는 못합니다. 일반적으로는 loss가 1.35~1.52 사이의 값이 나와야 합니다.)\n",
    "                if epoch == 0 and iter == 0:\n",
    "                    assert G_loss.detach().allclose(torch.tensor(1.5), atol=2e-1), \\\n",
    "                    f\"Generator Loss of the model does not match expected result.\"\n",
    "                    print(\"==Generator loss function test passed!==\")\n",
    "\n",
    "                self.G.zero_grad()\n",
    "                G_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "            # verbose\n",
    "            end_time = time.time() - start_time\n",
    "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
    "            print('Time [%s], Epoch [%d/%d], lossD: %.4f, lossG: %.4f'\n",
    "                % (end_time, epoch+1, epochs, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            # Save Images\n",
    "            fake_img = fake_img.reshape(fake_img.size(0), 3, 32, 32)\n",
    "            torchvision.utils.save_image(denorm(fake_img), os.path.join('./results/', 'images', 'fake_image-{:03d}.png'.format(epoch+1)))\n",
    "            if epoch % 10 == 0:\n",
    "                self.test()\n",
    "\n",
    "        # Save Checkpoints\n",
    "        save_checkpoint(self.G, os.path.join('./results', 'checkpoints', 'G_final.pth'), self.device)\n",
    "        save_checkpoint(self.D, os.path.join('./results', 'checkpoints', 'D_final.pth'), self.device)\n",
    "\n",
    "    def test(self):\n",
    "        print('Start computing Inception Score')\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            for iter in tqdm.tqdm(range(5000)):\n",
    "                z = torch.randn(1, 256).to(self.device)\n",
    "                fake_img = self.G(z)\n",
    "                torchvision.utils.save_image(denorm(fake_img), os.path.join('./results/', 'evaluation', 'fake_image-{:03d}.png'.format(iter)))\n",
    "\n",
    "        # Compute the Inception score\n",
    "        dataset = FolderDataset(folder = os.path.join('./results/', 'evaluation'))\n",
    "        Inception = Inception_Score(dataset)\n",
    "        score = Inception.compute_score(splits=1)\n",
    "        print('Inception Score : ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKACzMg9SzDF"
   },
   "source": [
    "#### **Train**\n",
    "\n",
    "자, 이제 학습을 진행해 보겠습니다.\n",
    "학습이 진행됨에 따라 generator가 생성하는 image는 \\\\\n",
    "[파일]->[results]->[images]에서 각 epoch별로 확인해보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBuw5xCdIglG"
   },
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "\n",
    "trainloader, testloader = create_dataloader()\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "device = torch.device('cuda')\n",
    "\n",
    "trainer = Trainer(trainloader=trainloader, \n",
    "                testloader=testloader,\n",
    "                generator=G,\n",
    "                discriminator=D,\n",
    "                criterion=criterion,\n",
    "                g_optimizer=g_optimizer,\n",
    "                d_optimizer=d_optimizer,\n",
    "                device=device)\n",
    "\n",
    "trainer.train(epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Self-Supervised Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1 Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전이 학습(Transfer Learning)은 특정 분야에서 학습된 신경망의 일부 능력을 유사하거나 전혀 새로운 분야에서 사용되는 신경망 학습을 의미합니다. 다시 말하면 보편적인 모델을 만들고 원하는 분야에 이 모델을 조금만 더 학습시켜 사용하는 것을 의미합니다. 특히 이미지 특징을 추출하는 신셩망의 능력이 다른 분야에 많이 사용될 수 있습니다. 그렇기에 ReNet이나 VGGNet 등 이미지 특징을 추출하는 신경망의 능력을 이용하고 마지막 layer만(주로 선형으로, Affine:가중치와 편향에 대한 행렬 연산) 변경하여 변경된 레이머만 재학습시키는 것입니다. \n",
    "\n",
    "이것을 평가하기 위해선 보편적 모델의 학습을 거의 마친 후, 가중치를 고정시키고 레이블이 있는 데이터를 가져와 마지막 layer만 학습시키며 유의미한 정확도가 나오는지 확인합니다. 이때 특징을 구체적으로 잘 잡는다면 학습이 잘 진행되고 있는 것입니다. \n",
    "\n",
    "이 방법을 사용하려면 보편적인 모델이 먼저 학습되어야 합니다. 그러기 위해서 아주 많은 데이터가 필요한데 label의 가격이나 수가 너무 한정됩니다. 이를 해결하기 위해 데이터 자체를 label로 쓰는 Self-Supervised Learning을 사용합니다. Self-supervised learning은 지도학습과 비지도학습의 특성을 모두 가지고 있습니다. 이를 학습하기 위해 contrastive learning을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.2 Contrastive Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive Learning은 특징이 비슷한 것은 거리가 가깝고 다른 것은 거리가 멀다는 개념으로 시작합니다. 여기서 우리는 이를 self-supervised learning에 사용할 것이므로 가진 이미지를 자르거나 색을 바꿔서 데이터를 여러개 만들고 학습시킵니다. \n",
    "\n",
    "![contrastive](_image/contrastive.PNG)\n",
    "\n",
    "위 그림은 그 과정을 간략하게 보여줍니다. 먼저 x라는 데이터에서 자르거나 색을 바꾸는 등의 변환 과정을 거쳐 $\\tilde{x_i}$와 $\\tilde{x_j}$를 만듭니다. 두 벡터의 크기는 N(이미지 개수) x D(이미지 크기)입니다. 이때 $\\tilde{x_i}$와 $\\tilde{x_j}$는 행마다 이미지의 데이터가 있으며 같은 행은 같은 이미지에서 추출된 데이터입니다. 즉, 같은 행은 같은 레이블을 가지고 둘의 유사도는 높아야 합니다. 그리고 이를 아주 큰 모델을 거쳐 $z_i$와 $z_j$를 얻습니다. 구한 것을 $z_i \\cdot z_j^T$하면 각 열은 $z_i$의 각 열과 $z_j$와의 유사도로 채워집니다. 여기서 같은 레이블을 가지는 것은 행과 열이 같은 곳(같은 이미지에서 추출한 데이터들끼리 내적된 곳)만 해당됩니다. 그렇기에 cross-entropy loss를 이용하여 N x N 크기의 단위행렬이 되는 것을 목표로 계속해서 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3 Contrastive Learning의 문제점과 해결 방안들**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 Contrastive Learning을 사용하면 한 가지 문제가 생깁니다. \n",
    "\n",
    "예를 들어 같은 레이블을 갖는 a와 b가 있다고 합시다. 학습 결과 a를 b와 가깝게 만들기 위해 encoder를 사용하여 거리를 가깝게 만듭니다. 그런데 b 역시 a와 같은 encoder를 쓰기에 바뀐 encoder에 의해 도망가게 됩니다. 그리고 이를 해결하기 위해 크게 3가지 대안이 제시됐습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) MoCo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoCo(Momentum Contrast for Unsupervised Visual Representation Learning)는 문제를 해결하기 위해 momentum을 사용합니다.\n",
    "\n",
    "<img src = \"https://images.velog.io/images/fornanaa/post/3330b72e-bc8c-40bd-ac15-379f1175ff26/37.png\">\n",
    "\n",
    "기존 방법에서 key가 매우 많아 다양한 negative sample을 확보할 수 있는 아주 큰 dictionary와 일관된 표현, 즉 거리를 좁힐 때 도망가지 않도록 b의 위치를 어느 정도 제약시키는 momentum encoder를 사용합니다.\n",
    "\n",
    "[Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) PIRL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIRL(Pretext-Invariant Representation Learning)의 기본적 아이디어는 원래 이미지와 transformation을 적용한 후의 이미지는 비슷하고 다른 이미지와는 달라야 한다는 것입니다. \n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/21357649/78502864-340c9780-779e-11ea-8ab3-8d3b72cdd2db.png\">\n",
    "\n",
    "위 그림을 보면 $I$와 $I^t$는 Memory Bank에서 같은 레이블로 향하고 다른 레이블과 멀어지게 동작하는 것을 볼 수 있습니다. 이때 memory bank에 있는 레이블들은 원본 이미지들을 통해 만들어진 것입니다. Memory bank를 도입한 이유는 더 많은 negative 이미지를 이용하기 위해서입니다. 더 좋은 결과를 얻기 위해 비교할 수 있는 수많은 negative 이미지가 필요했는데 이를 이용하면서 자원의 한계에 도달하지 않도록 추가한 것입니다. \n",
    "\n",
    "[Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/abs/1912.01991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) SimCLR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimCLR은 원본 데이터 셋에서 예제를 무작위로 추출하여 두 번 변환한 뒤, representation을 계산하여 비선형 투영 계산을 합니다. \n",
    "\n",
    "<img src = \"https://images.velog.io/images/fornanaa/post/29805d79-d510-4aa4-a9f0-05cba60a7106/38.png\" width = \"500px\" height = \"500px\">\n",
    "<img src = \"https://media.vlpt.us/images/fornanaa/post/1f4980cb-9692-4b32-b210-285ae34db9a5/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-11-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.23.54.png\" width = \"500px\" height = \"500px\">\n",
    "\n",
    "비선형 투영 계산을 하는 이유는 representation 자체를 이용하여 loss를 구하는 것보다 성능이 더 좋아지기 때문입니다. 그리고 두 번의 변환은 동일한 이미지의 다른 변형을 인식하는데 도움이 되고 유사한 개념을 학습하는 효과도 있습니다. 또한 배치 사이즈가 크고 오랫동안 학습하기에 negative 쌍을 더 많이 고려할 수 있습니다. 7.2에 설명을 위해 사용된 그림도 SimCLR의 동작을 보여주는 그림입니다. \n",
    "\n",
    "SimCLRv2에선 기존보다 더 큰 ResNet을 사용하고 projection head $g$를 pretraining이 끝난 후에도 제거하지 않고 첫 번째 레이어를 인코더에 포함시켜 fine-tunung을 합니다. 결과적으로 3개의 MLP layer를 projection head로 사용하면서 projection head의 첫 번째 layer부터 fine-tuning을 수행합니다.\n",
    "\n",
    "[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) BYOL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BYOL(Bootstrap Your Own Latent)는 positive sample로만 representation을 학습할 수 없는가에 대한 의문에서 시작합니다. 위 세 가지 방법은 모두 negative sample이 많으면 많을수록 성능이 올라가는 형식이었습니다. \n",
    "\n",
    "![byol1](_image/byol1.PNG)\n",
    "\n",
    "위와 같이 초기화된 가중치를 갖는 CNN에 classifier만 ImageNet에 대해 CrossEntropy로 학습시킨 Target Network를 저장합니다. 그리고 이번에도 초기화된 가중치를 갖는 CNN을 classifier만 학습시키는데 이번엔 label 대신 target network를 따라 학습하도록 했더니 성능이 10배 이상 향상된 것을 확인할 수 있습니다. 여기서 영감을 얻어 만들어진 것이 BYOL입니다.\n",
    "\n",
    "![byol2](_image/byol2.PNG)\n",
    "\n",
    "BYOL의 구조는 앞서 본 그림과 동일한 형태입니다. 학습 이미지를 보고 target network가 출력하는 output을 가지고 online network가 학습합니다. 즉, online network는 target network를 label로 여기고 똑같이 따라 하도록 학습하는 것입니다.\n",
    "\n",
    "그 후, 이번엔 target network가 online network를 label로 여기며 학습합니다. 정확히는 backpropagation을 통한 학습이 아닌, MoCo에서 Momentum Moving Average 방식을 차용하여 학습합니다. \n",
    "\n",
    "종합하면 처음 online network가 target network를 보며 학습하고 이어 target network는 online network를 보며 학습하는 동작이 반복됩니다. 서로가 서로를 알려주며 시너지를 일으키는 것입니다. \n",
    "\n",
    "이 방식은 특히 ImageNet에서 큰 두각을 보이고 있습니다.\n",
    "\n",
    "[Bootstrap Your Own Latent A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.4 현황**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "그렇다면 레이블이 없는 데이터를 주었을 때, 모델이 알아서 레이블을 만들어 학습하는 것이 지도학습 정도의 성능을 보장할까요? \n",
    "\n",
    "![byol3](_image/byol3.PNG)\n",
    "\n",
    "위 그래프는 SimCLR, MoCo, BYOL의 성능과 지도학습의 성능을 보여줍니다. 지도학습과 self-supervised learning의 차이가 크지 않으며 파라미터 수가 늘어날수록 거의 동일한 것을 확인할 수 있습니다. 또한 BYOL은 ImageNet에서 지도학습을 따라잡은 것도 확인할 수 있습니다.\n",
    "\n",
    "![현황2](_image/현황2.PNG)\n",
    "\n",
    "또한 SimCLR의 경우, 전체 데이터 중 레이블이 있는 데이터의 비율을 줄여도 성능의 차가 아주 크지 않음을(모델이 깊을수록) 확인할 수 있습니다. \n",
    "\n",
    "그렇기에 점점 label이 있는 데이터때문에 막혔던 문제들이 해결될 것으로 보입니다. \n",
    "\n",
    "더 자세한 자료는 [Self-Supervised Learning 전반적 설명](https://velog.io/@tobigs-gm1/Self-Supervised-Learning), [MoCo 리뷰](https://ffighting.tistory.com/entry/CVPR-2020-MoCo-%EB%A6%AC%EB%B7%B0), [PIRL 리뷰](https://2-chae.github.io/category/2.papers/20), [SimCLR 리뷰](https://rauleun.github.io/SimCLR), [BOYL 리뷰 1](https://ffighting.tistory.com/entry/NIPS-2020-BYOL-%ED%95%B5%EC%8B%AC-%EB%A6%AC%EB%B7%B0?category=981962?category=981962), [BOYL 리뷰 2](https://blog.promedius.ai/ssl_byol/)을 참고하세요. 각 블로그마다 다른 모델의 리뷰도 있으니 교차해서 보면 더욱 좋습니다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
