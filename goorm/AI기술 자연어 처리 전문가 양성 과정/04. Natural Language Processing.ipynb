{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Intro\n",
    "1. Bag-of-Words\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1 NLP란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP(Natural Language Processing)란 자연어를 컴퓨터와 같은 기계를 이용해서 묘사하도록 연구하는 인공지능 분야입니다. 이는 다시 NLU(Natural Language Understanding)과 NLG(Natural Language Generation)으로 나뉩니다. \n",
    "\n",
    "NLU는 문장이나 문단이 주어졌을 때, 이를 이해하고 질의응답이 가능하도록 만드는 분야입니다. NLG는 자동 번역 등 자연스럽게 말이 이어지도록 생성하는 과정입니다. 현재 NLU는 어느 정도 연구가 진행되었다고 여겨지고 NLG에 대한 연구가 더 활발하게 이루어지고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.2 NLP Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP를 사용하는 분야는 많이 있습니다. 이를 하나씩 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam detection, sentiment analysis 등 받은 자연어들로 분류하는 분야입니다. 스팸 메일을 분류하거나 평점을 가지고 긍부정을 나누는 등에 사용됩니다. \n",
    "\n",
    "![text_classification](_image/text_classification.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) QA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA(Question Answering)는 말 그대로 질문과 context(위키 등 웹페이지들)가 input으로 주어졌을 때, output으로 답을 내보내는 것입니다. 대표적으로 search engine 등이 있습니다. 밑에서 구글을 보면 질문에 대해 위키에 답을 진하게 표현한 것을 볼 수 있습니다.\n",
    "\n",
    "![QA](_image/QA.PNG)\n",
    "\n",
    "이때 답을 찾는 방법은 크게 세 가지가 있습니다.\n",
    "\n",
    "1. Context 내에서 뽑아냄(위 이미지가 그 예시)\n",
    "2. 직접 문장을 생성하여 출력\n",
    "3. 객관식(Multi-Choice)를 사용하여 출력\n",
    "\n",
    "2번 같은 경우는 context가 주어지지 않고 질문만 주어지는 common sense reasoning입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Macine Translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Translation은 기존 언어(source language)를 번역할 언어(target language)로 바꿔주는 것입니다. 현재 연구, 실무 분야에서 가장 활발하게 진행되고 있습니다. 파파고, 구글 번역 등이 그 예시입니다.\n",
    "\n",
    "1. pair 사용(같은 의미, 다른 언어의 쌍)\n",
    "2. unsupervised learning(pair가 없이 학습)\n",
    "\n",
    "학습 방법은 위에 두 가지 방법이 있습니다. pair는 직관적이지만 하나하나 학습 데이터를 만들기 어렵고 데이터가 너무 커집니다. 그래서 최근 비지도 학습을 이용한 학습이 새로운 방법으로 나오고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot은 자연어가 input으로 주어졌을 때, 대화가 이어지도록 output이 이어서 주어지는 프로그램입니다. 대화가 계속 이어지기 때문에 맥락을 잘 보는 것이 필요합니다. 그렇기에 직전 한 문장만 input으로 받지 않고 어느 정도 문단을 같이 가져옵니다. input을 template와 generation 중 어떤 것으로 받을지 선택할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Personal Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal Assistant는 빅스비, 시리와 같이 특정 요청을 수행하는 것들을 말합니다. 아직은 노래 요청, 날씨 등 제한적인 사용만 가능하지만 점차 개발되어지고 있습니다. 음성으로 작동하기 때문에 특정 음성(하이 빅스비, 시리야 등)을 들어야 음성 모드로 들어가게 만들어서 여러 소음에 작동하지 않도록 하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6) Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization은 말그대로 글들을 요약해주는 것입니다. 예를 들어 뉴스가 있다면 제목과 내용을 보고 요약해주는 것을 말합니다. \n",
    "\n",
    "![text_summarization](_image/text_summarization.PNG)\n",
    "\n",
    "요약하는 방식은 extractive와 generative로 나뉩니다. extractive는 주어진 내용들을 통해 요약을 해주는 반면 generative는 본문에 없는 말로도 요약할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.3 NLP 연구 필드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 분야는 ACL, EMNLP, NAACL 등 유력한 컨퍼런스들이 있습니다. (AI는 급변하는 분야이기에 저널을 잘 보지 않습니다.) 이를 더욱 세분화하여 분야를 나눌 수 있습니다.\n",
    "\n",
    "#### **Low-level parsing**\n",
    "- Tokenization, stemming(과거형이나 미래형을 기본형으로 바꾸는 것)\n",
    "\n",
    "#### **Word and pharse level**\n",
    "- Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution $\\Rightarrow$ (언어학자가 만든 체계가 컴퓨터에게는 도움이 되지 않음이 확인되면서 연구가 사그라듬)\n",
    "- Semantic relation extraction\n",
    "\n",
    "#### **Sentence level**\n",
    "- Sentiment analysis, machine translation\n",
    "\n",
    "#### **Multi-sentence and paragraph level**\n",
    "- Entailment prediction, question answering, dialog systems, summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 text mining, information retrieval 등이 있습니다.(PDF 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding이란 단어들을 embedding이란 과정을 거쳐 컴퓨터가 이해할 수 있는 숫자로 바꾸는 과정을 말합니다. 이러면 단어들은 벡터가 되는데 이 벡터를 embedding vector라고 합니다.\n",
    "\n",
    "|구분|Bag-of_Words|언어 모델|분포 가정|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|내용|어떤 단어가 많이 쓰였는가|단어가 어떤 순서로 쓰였는가|어떤 단어가 같이 쓰였는가|\n",
    "|대표 통계량|TF-IDF|-|PMI|\n",
    "|대표 모델|Deep Averaging Network|ELMO, GPT|Word2Vec|\n",
    "\n",
    "단어를 임베딩하여 나오는 벡터들은 어떤 가정을 따르냐에 따라서 위와 같이 구분됩니다. 이를 더 자세히 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 백 오브 워즈 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백 오브 워즈는 저자의 의도가 단어 사용 여부나 그 빈도에서 드러난다고 보는 과정입니다. 그렇기에 단어의 순서는 고려대상이 아닙니다. 순서에 상관없이 하나의 가방에 단어들을 모두 넣고 빈도수를 확인하는 것입니다. 이때 대표적으로 TF-IDF가 사용됩니다. \n",
    "\n",
    "#### **TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
    "어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지고 반대의 경우 작아집니다. w는 단어, N은 문서의 개수, TF(Term Frequency)는 단락에서의 빈도, DF(Document Frequency)는 문서에서의 빈도입니다. \n",
    "\n",
    "$$TF - IDF(w) = TF(w) \\times log(\\frac{N}{DF(w)})$$\n",
    "\n",
    "그렇기에 문서 전체에서 빈도가 높은 조사들은 가중치가 줄어들고 특정 문장에서 빈번하게 나오는 단어들은 가중치가 증가합니다. 이에 대해 뒤에서 더 자세히 알아보겠습니다.\n",
    "\n",
    "\n",
    "#### **Deep Averaging Network(lyyer et al. 2015)**\n",
    "문장에 속한 단어의 임베딩의 평균을 구해 문장의 임베딩을 만드는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 언어 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여하는 방법입니다. 백 오브 워즈와 달리 등장 순서에 영향을 받기에 '나는 밥을 먹었다'와 '나는 먹었다 밥을'을 다른 문장으로 해석합니다. \n",
    "\n",
    "$$P(w_i) \\Rightarrow P(w_i|w_{i-1}, w_{i-2}, \\cdots, w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 분포 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 의미를 주변 문맥을 통해 유추하는 방법입니다. 가까운 단어들을 통해 의미를 유추합니다.\n",
    "\n",
    "#### **PMI(Pointwise Mutual Information)**\n",
    "두 단어 A, B가 얼마나 자주 같이 등장하는지 정보를 수치화하여 유추합니다.\n",
    "\n",
    "$$ PMI(A, B) = log \\frac{P(A, B)}{P(A) \\times P(B)}$$\n",
    "\n",
    "#### **Word2Vec**\n",
    "특정 단어 주변의 문맥, 즉 분포 정보를 함축하며 벡터로 만들어 사용하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Bag-of-Words Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 문법과 어순은 무시하지만 다중성을 유지하면서 단어의 가방에 단어들을 집어넣습니다. 그리고 이를 고유한 단어의 vocabulary를 만들거나 각 단어들을 one-hot vector로 만들어 사용하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes Classifier은 Bayes' theorem을 이용하여 간단한 분류를 하는 classifier입니다. \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\; (\\text{Bayes'\\, theorem})$$\n",
    "\n",
    "이제 이를 이용해 Naïve Bayes Classifier을 유추해보겠습니다. 각 document d는 class c를 가지고 있습니다. P(c|d)가 d가 c에 속할 확률입니다. Bayes' theorem을 적용하면 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "근데 여기서 분모는 항상 같기 때문에 분모를 제거해도 괜찮습니다. 그렇기에 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c)$$\n",
    "\n",
    "$d$는 $words w_1, w_2, \\cdots, w_n$로 이루어져 있습니다. 그렇기에 위의 식은 다시 쓰면 다음과 같습니다.\n",
    "\n",
    "$$P(d|c)P(c) = P(w_1, w_2, \\cdots, w_n|c)P(c)$$\n",
    "\n",
    "이때 우리가 관심이 있는 것은 $P(c|d) = P(c|w_1, w_2, \\cdots, w_n)$이다. 이제 여기에 chain rule까지 적용하면 다음과 같이 나옵니다. 이때 확률에 관한 chain rule은 다음과 같습니다.\n",
    "\n",
    "$$\\begin{aligned} P(X_4, X_3, X_2, X_1) &= P(X_4|X_3, X_2, x_1) \\cdot P(X_3, X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2|X_1) \\cdot P(X_1) \\end{aligned}$$\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c) = P(c) \\prod_{w_i \\in W} P(w_i|c)$$\n",
    "\n",
    "예시를 통해 알아보겠습니다. 밑에처럼 주어진 document와 word, class가 있습니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:skyblue\">?</span>|\n",
    "\n",
    "위와 같이 샘플이 주어져 있고 7번의 문장이 spam인지 아닌지 확인해야 합니다. 우리가 구한 식을 이용하여 식을 세워보겠습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) = P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})$$\n",
    "$$P(c_{Inbox}|d_7) = P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})$$\n",
    "\n",
    "만약 $P(c_{spam}|d_7) > P(c_{inbox}|d_7)$이면 스팸이고 $P(c_{spam}|d_7) < P(c_{inbox}|d_7)$이면 inbox로 분류될 것이다. 각 단어의 개수와 클래스 개수를 통해 결과를 보면 다음과 같습니다. \n",
    "\n",
    "![spam_result](_image/spam_result.PNG)\n",
    "\n",
    "이제 결과를 통해 종합적으로 계산하면 다음과 같습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) \n",
    "= P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{3}{10} \\times \\frac{2}{10} = \\frac{6}{1000}$$\n",
    "$$P(c_{Inbox}|d_7) \n",
    "= P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{2}{10} \\times \\frac{0}{10} = 0$$\n",
    "\n",
    "그러므로 7번 문장은 spam입니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:red\">Spam</span>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 필요한 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# POS(Part of Speech) tagger\n",
    "from konlpy import tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data와 test data를 준비하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# training data. input text와 정답 label (긍정(1), 부정(0))으로 구성\n",
    "data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n",
    "                {'text': \"기대했던 것보단 별로였네요.\"},\n",
    "                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n",
    "                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n",
    "                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n",
    "                {'text': \"연기가 좀 별로였습니다.\"},\n",
    "                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n",
    "                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n",
    "                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n",
    "                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n",
    "                ]\n",
    "# test data\n",
    "data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n",
    "                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n",
    "                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n",
    "                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n",
    "                ]\n",
    "\n",
    "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "test_labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 선언\n",
    "morph_analyzer = tag.Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization 함수 정의\n",
    "def tokenization(data, morph_analyzer):\n",
    "    \"\"\"tokenization \n",
    "\n",
    "    Args:\n",
    "        data (list): list of data examples.\n",
    "        morph_analyzer (konlpy.tag._kkma.Kkma): morphological analyzer.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_data (list): list of tokenized data examples.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for example in tqdm(data):\n",
    "        tokens = morph_analyzer.morphs(example['text'])\n",
    "        tokenized_data.append(tokens)\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 12.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenization 함수를 이용한 데이터 tokenization\n",
    "tokenized_data = {}\n",
    "\n",
    "tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n",
    "tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n",
       " ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n",
       " ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n",
       " ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n",
       " ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n",
       " ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n",
       " ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n",
       " ['기념일',\n",
       "  '에',\n",
       "  '방문',\n",
       "  '하',\n",
       "  '었',\n",
       "  '는데',\n",
       "  '연기',\n",
       "  '도',\n",
       "  '연출',\n",
       "  '도',\n",
       "  '다',\n",
       "  '좋',\n",
       "  '았',\n",
       "  '습니다',\n",
       "  '.'],\n",
       " ['전반적',\n",
       "  '으로',\n",
       "  '지루',\n",
       "  '하',\n",
       "  '었',\n",
       "  '습니다',\n",
       "  '.',\n",
       "  '저',\n",
       "  '는',\n",
       "  '별',\n",
       "  '로',\n",
       "  '이',\n",
       "  '었',\n",
       "  '네요',\n",
       "  '.'],\n",
       " ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_data 확인\n",
    "tokenized_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tokenization 결과를 이용해서 word to index dictionary를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n",
    "tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i]]\n",
    "unique_train_tokens = set(tokens)\n",
    "\n",
    "# Naïve Bayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n",
    "word2index = defaultdict() # key: word, value: index of word\n",
    "idx = 0\n",
    "for token in tqdm(unique_train_tokens):\n",
    "    word2index[token] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Naïve Bayes Classifier 모델 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, word2index, k=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word2index (dict): mapping a word to a pre-assigned index.\n",
    "            k (float, optional): constant for smoothing. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        self.k = k # for smoothing\n",
    "        self.word2index = word2index\n",
    "        self.priors = {} # Prior probability for each class, P(c)\n",
    "        self.likelihoods = {} # Likelihood for each token, P(d|c)\n",
    "    \n",
    "    def _set_priors(self, labels):\n",
    "        \"\"\"\n",
    "        Set prior probability for each class, P(c).\n",
    "        Count the number of each class and caculate P(c) for each class.\n",
    "        \"\"\"\n",
    "        # Count the number of each class\n",
    "        class_counts = defaultdict(int)\n",
    "        for label in labels:\n",
    "            class_counts[label] += 1\n",
    "        \n",
    "        # For each class, calcuate P(c)\n",
    "        total = sum(class_counts.values())\n",
    "        for key, value in class_counts.items():\n",
    "            self.priors[key] = (value / total)\n",
    "    \n",
    "    def _set_likelihoods(self, tokens, labels):\n",
    "        \"\"\"\n",
    "        Set likelihood for each token, P(d|c).\n",
    "        First, count the number of each class for each token.\n",
    "        Then, calculate P(d|c) for a given class and token.\n",
    "        \"\"\"\n",
    "        token_dists = {}\n",
    "        class_counts = defaultdict(int)\n",
    "        \n",
    "        # 각 토큰에 대한 각 클래스 수를 계산합니다.\n",
    "        # Count the number of each class for each token\n",
    "        for i, label in enumerate(tqdm(labels)):\n",
    "            count = 0\n",
    "            for token in tokens[i]:\n",
    "                if token not in list(token_dists.keys()):\n",
    "                    if label == 1:\n",
    "                        token_dists[token] = [0, 1]\n",
    "                    else:\n",
    "                        token_dists[token] = [1, 0]\n",
    "            \n",
    "            count += 1\n",
    "            class_counts[label] += count\n",
    "\n",
    "        for token, dist in tqdm(token_dists.items()):\n",
    "            if token not in self.likelihoods:\n",
    "                self.likelihoods[token] = {\n",
    "                    0: (token_dists[token][0] + self.k) / (class_counts[0] + len(self.word2index) * self.k),\n",
    "                    1: (token_dists[token][1] + self.k) / (class_counts[1] + len(self.word2index) * self.k),\n",
    "                }\n",
    "    \n",
    "    def train(self, input_tokens, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized train data.\n",
    "            labels (): train labels for each sentence/document.\n",
    "        \"\"\"\n",
    "        self._set_priors(labels)\n",
    "        self._set_likelihoods(input_tokens, labels)\n",
    "    \n",
    "    def inference(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized test data.\n",
    "        \"\"\"\n",
    "        log_prob_0 = 0.0\n",
    "        log_prob_1 = 0.0\n",
    "        \n",
    "        for token in input_tokens:\n",
    "            if token in self.likelihoods:\n",
    "                log_prob_0 += math.log(self.likelihoods[token][0])\n",
    "                log_prob_1 += math.log(self.likelihoods[token][1])\n",
    "        \n",
    "        log_prob_0 += math.log(self.priors[0])\n",
    "        log_prob_1 += math.log(self.priors[1])\n",
    "        \n",
    "        if log_prob_0 >= log_prob_1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 학습 데이터에 대해 문장 분류 모델을 학습시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10029.42it/s]\n",
      "100%|██████████| 56/56 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 문장 분류 모델 선언 및 학습\n",
    "classifier = NaiveBayesClassifier(word2index)\n",
    "classifier.train(tokenized_data['train'], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 test 데이터에 대해 정답값을 예측하고 Accuracy를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test data inference\n",
    "preds = []\n",
    "for test_tokens in tqdm(tokenized_data['test']):\n",
    "    pred = classifier.inference(test_tokens)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
