{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Intro\n",
    "1. Bag-of-Words\n",
    "2. Topic Modeling\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1 NLP란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP(Natural Language Processing)란 자연어를 컴퓨터와 같은 기계를 이용해서 묘사하도록 연구하는 인공지능 분야입니다. 이는 다시 NLU(Natural Language Understanding)과 NLG(Natural Language Generation)으로 나뉩니다. \n",
    "\n",
    "NLU는 문장이나 문단이 주어졌을 때, 이를 이해하고 질의응답이 가능하도록 만드는 분야입니다. NLG는 자동 번역 등 자연스럽게 말이 이어지도록 생성하는 과정입니다. 현재 NLU는 어느 정도 연구가 진행되었다고 여겨지고 NLG에 대한 연구가 더 활발하게 이루어지고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.2 NLP Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP를 사용하는 분야는 많이 있습니다. 이를 하나씩 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam detection, sentiment analysis 등 받은 자연어들로 분류하는 분야입니다. 스팸 메일을 분류하거나 평점을 가지고 긍부정을 나누는 등에 사용됩니다. \n",
    "\n",
    "![text_classification](_image/text_classification.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) QA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA(Question Answering)는 말 그대로 질문과 context(위키 등 웹페이지들)가 input으로 주어졌을 때, output으로 답을 내보내는 것입니다. 대표적으로 search engine 등이 있습니다. 밑에서 구글을 보면 질문에 대해 위키에 답을 진하게 표현한 것을 볼 수 있습니다.\n",
    "\n",
    "![QA](_image/QA.PNG)\n",
    "\n",
    "이때 답을 찾는 방법은 크게 세 가지가 있습니다.\n",
    "\n",
    "1. Context 내에서 뽑아냄(위 이미지가 그 예시)\n",
    "2. 직접 문장을 생성하여 출력\n",
    "3. 객관식(Multi-Choice)를 사용하여 출력\n",
    "\n",
    "2번 같은 경우는 context가 주어지지 않고 질문만 주어지는 common sense reasoning입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Macine Translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Translation은 기존 언어(source language)를 번역할 언어(target language)로 바꿔주는 것입니다. 현재 연구, 실무 분야에서 가장 활발하게 진행되고 있습니다. 파파고, 구글 번역 등이 그 예시입니다.\n",
    "\n",
    "1. pair 사용(같은 의미, 다른 언어의 쌍)\n",
    "2. unsupervised learning(pair가 없이 학습)\n",
    "\n",
    "학습 방법은 위에 두 가지 방법이 있습니다. pair는 직관적이지만 하나하나 학습 데이터를 만들기 어렵고 데이터가 너무 커집니다. 그래서 최근 비지도 학습을 이용한 학습이 새로운 방법으로 나오고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot은 자연어가 input으로 주어졌을 때, 대화가 이어지도록 output이 이어서 주어지는 프로그램입니다. 대화가 계속 이어지기 때문에 맥락을 잘 보는 것이 필요합니다. 그렇기에 직전 한 문장만 input으로 받지 않고 어느 정도 문단을 같이 가져옵니다. input을 template와 generation 중 어떤 것으로 받을지 선택할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Personal Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal Assistant는 빅스비, 시리와 같이 특정 요청을 수행하는 것들을 말합니다. 아직은 노래 요청, 날씨 등 제한적인 사용만 가능하지만 점차 개발되어지고 있습니다. 음성으로 작동하기 때문에 특정 음성(하이 빅스비, 시리야 등)을 들어야 음성 모드로 들어가게 만들어서 여러 소음에 작동하지 않도록 하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6) Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization은 말그대로 글들을 요약해주는 것입니다. 예를 들어 뉴스가 있다면 제목과 내용을 보고 요약해주는 것을 말합니다. \n",
    "\n",
    "![text_summarization](_image/text_summarization.PNG)\n",
    "\n",
    "요약하는 방식은 extractive와 generative로 나뉩니다. extractive는 주어진 내용들을 통해 요약을 해주는 반면 generative는 본문에 없는 말로도 요약할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.3 NLP 연구 필드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 분야는 ACL, EMNLP, NAACL 등 유력한 컨퍼런스들이 있습니다. (AI는 급변하는 분야이기에 저널을 잘 보지 않습니다.) 이를 더욱 세분화하여 분야를 나눌 수 있습니다.\n",
    "\n",
    "#### **Low-level parsing**\n",
    "- Tokenization, stemming(과거형이나 미래형을 기본형으로 바꾸는 것)\n",
    "\n",
    "#### **Word and pharse level**\n",
    "- Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution $\\Rightarrow$ (언어학자가 만든 체계가 컴퓨터에게는 도움이 되지 않음이 확인되면서 연구가 사그라듬)\n",
    "- Semantic relation extraction\n",
    "\n",
    "#### **Sentence level**\n",
    "- Sentiment analysis, machine translation\n",
    "\n",
    "#### **Multi-sentence and paragraph level**\n",
    "- Entailment prediction, question answering, dialog systems, summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 text mining, information retrieval 등이 있습니다.(PDF 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding이란 단어들을 embedding이란 과정을 거쳐 컴퓨터가 이해할 수 있는 숫자로 바꾸는 과정을 말합니다. 이러면 단어들은 벡터가 되는데 이 벡터를 embedding vector라고 합니다.\n",
    "\n",
    "|구분|Bag-of_Words|언어 모델|분포 가정|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|내용|어떤 단어가 많이 쓰였는가|단어가 어떤 순서로 쓰였는가|어떤 단어가 같이 쓰였는가|\n",
    "|대표 통계량|TF-IDF|-|PMI|\n",
    "|대표 모델|Deep Averaging Network|ELMO, GPT|Word2Vec|\n",
    "\n",
    "단어를 임베딩하여 나오는 벡터들은 어떤 가정을 따르냐에 따라서 위와 같이 구분됩니다. 이를 더 자세히 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 백 오브 워즈 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백 오브 워즈는 저자의 의도가 단어 사용 여부나 그 빈도에서 드러난다고 보는 과정입니다. 그렇기에 단어의 순서는 고려대상이 아닙니다. 순서에 상관없이 하나의 가방에 단어들을 모두 넣고 빈도수를 확인하는 것입니다. 이때 대표적으로 TF-IDF가 사용됩니다. \n",
    "\n",
    "#### **TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
    "어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지고 반대의 경우 작아집니다. w는 단어, N은 문서의 개수, TF(Term Frequency)는 단락에서의 빈도, DF(Document Frequency)는 문서에서의 빈도입니다. \n",
    "\n",
    "$$TF - IDF(w) = TF(w) \\times log(\\frac{N}{DF(w)})$$\n",
    "\n",
    "그렇기에 문서 전체에서 빈도가 높은 조사들은 가중치가 줄어들고 특정 문장에서 빈번하게 나오는 단어들은 가중치가 증가합니다. 이에 대해 뒤에서 더 자세히 알아보겠습니다.\n",
    "\n",
    "\n",
    "#### **Deep Averaging Network(lyyer et al. 2015)**\n",
    "문장에 속한 단어의 임베딩의 평균을 구해 문장의 임베딩을 만드는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 언어 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여하는 방법입니다. 백 오브 워즈와 달리 등장 순서에 영향을 받기에 '나는 밥을 먹었다'와 '나는 먹었다 밥을'을 다른 문장으로 해석합니다. \n",
    "\n",
    "$$P(w_i) \\Rightarrow P(w_i|w_{i-1}, w_{i-2}, \\cdots, w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 분포 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 의미를 주변 문맥을 통해 유추하는 방법입니다. 가까운 단어들을 통해 의미를 유추합니다.\n",
    "\n",
    "#### **PMI(Pointwise Mutual Information)**\n",
    "두 단어 A, B가 얼마나 자주 같이 등장하는지 정보를 수치화하여 유추합니다.\n",
    "\n",
    "$$ PMI(A, B) = log \\frac{P(A, B)}{P(A) \\times P(B)}$$\n",
    "\n",
    "#### **Word2Vec**\n",
    "특정 단어 주변의 문맥, 즉 분포 정보를 함축하며 벡터로 만들어 사용하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Bag-of-Words Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 문법과 어순은 무시하지만 다중성을 유지하면서 단어의 가방에 단어들을 집어넣습니다. 그리고 이를 고유한 단어의 vocabulary를 만들거나 각 단어들을 one-hot vector로 만들어 사용하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes Classifier은 Bayes' theorem을 이용하여 간단한 분류를 하는 classifier입니다. \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\; (\\text{Bayes'\\, theorem})$$\n",
    "\n",
    "이제 이를 이용해 Naïve Bayes Classifier을 유추해보겠습니다. 각 document d는 class c를 가지고 있습니다. P(c|d)가 d가 c에 속할 확률입니다. Bayes' theorem을 적용하면 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "근데 여기서 분모는 항상 같기 때문에 분모를 제거해도 괜찮습니다. 그렇기에 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c)$$\n",
    "\n",
    "$d$는 $words w_1, w_2, \\cdots, w_n$로 이루어져 있습니다. 그렇기에 위의 식은 다시 쓰면 다음과 같습니다.\n",
    "\n",
    "$$P(d|c)P(c) = P(w_1, w_2, \\cdots, w_n|c)P(c)$$\n",
    "\n",
    "이때 우리가 관심이 있는 것은 $P(c|d) = P(c|w_1, w_2, \\cdots, w_n)$이다. 이제 여기에 chain rule까지 적용하면 다음과 같이 나옵니다. 이때 확률에 관한 chain rule은 다음과 같습니다.\n",
    "\n",
    "$$\\begin{aligned} P(X_4, X_3, X_2, X_1) &= P(X_4|X_3, X_2, x_1) \\cdot P(X_3, X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2|X_1) \\cdot P(X_1) \\end{aligned}$$\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c) = P(c) \\prod_{w_i \\in W} P(w_i|c)$$\n",
    "\n",
    "예시를 통해 알아보겠습니다. 밑에처럼 주어진 document와 word, class가 있습니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:skyblue\">?</span>|\n",
    "\n",
    "위와 같이 샘플이 주어져 있고 7번의 문장이 spam인지 아닌지 확인해야 합니다. 우리가 구한 식을 이용하여 식을 세워보겠습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) = P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})$$\n",
    "$$P(c_{Inbox}|d_7) = P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})$$\n",
    "\n",
    "만약 $P(c_{spam}|d_7) > P(c_{inbox}|d_7)$이면 스팸이고 $P(c_{spam}|d_7) < P(c_{inbox}|d_7)$이면 inbox로 분류될 것이다. 각 단어의 개수와 클래스 개수를 통해 결과를 보면 다음과 같습니다. \n",
    "\n",
    "![spam_result](_image/spam_result.PNG)\n",
    "\n",
    "이제 결과를 통해 종합적으로 계산하면 다음과 같습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) \n",
    "= P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{3}{10} \\times \\frac{2}{10} = \\frac{6}{1000}$$\n",
    "$$P(c_{Inbox}|d_7) \n",
    "= P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{2}{10} \\times \\frac{0}{10} = 0$$\n",
    "\n",
    "그러므로 7번 문장은 spam입니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:red\">Spam</span>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 필요한 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# POS(Part of Speech) tagger\n",
    "from konlpy import tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data와 test data를 준비하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# training data. input text와 정답 label (긍정(1), 부정(0))으로 구성\n",
    "data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n",
    "                {'text': \"기대했던 것보단 별로였네요.\"},\n",
    "                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n",
    "                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n",
    "                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n",
    "                {'text': \"연기가 좀 별로였습니다.\"},\n",
    "                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n",
    "                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n",
    "                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n",
    "                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n",
    "                ]\n",
    "# test data\n",
    "data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n",
    "                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n",
    "                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n",
    "                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n",
    "                ]\n",
    "\n",
    "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "test_labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 선언\n",
    "morph_analyzer = tag.Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization 함수 정의\n",
    "def tokenization(data, morph_analyzer):\n",
    "    \"\"\"tokenization \n",
    "\n",
    "    Args:\n",
    "        data (list): list of data examples.\n",
    "        morph_analyzer (konlpy.tag._kkma.Kkma): morphological analyzer.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_data (list): list of tokenized data examples.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for example in tqdm(data):\n",
    "        tokens = morph_analyzer.morphs(example['text'])\n",
    "        tokenized_data.append(tokens)\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 15.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenization 함수를 이용한 데이터 tokenization\n",
    "tokenized_data = {}\n",
    "\n",
    "tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n",
    "tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n",
       " ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n",
       " ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n",
       " ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n",
       " ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n",
       " ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n",
       " ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n",
       " ['기념일',\n",
       "  '에',\n",
       "  '방문',\n",
       "  '하',\n",
       "  '었',\n",
       "  '는데',\n",
       "  '연기',\n",
       "  '도',\n",
       "  '연출',\n",
       "  '도',\n",
       "  '다',\n",
       "  '좋',\n",
       "  '았',\n",
       "  '습니다',\n",
       "  '.'],\n",
       " ['전반적',\n",
       "  '으로',\n",
       "  '지루',\n",
       "  '하',\n",
       "  '었',\n",
       "  '습니다',\n",
       "  '.',\n",
       "  '저',\n",
       "  '는',\n",
       "  '별',\n",
       "  '로',\n",
       "  '이',\n",
       "  '었',\n",
       "  '네요',\n",
       "  '.'],\n",
       " ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_data 확인\n",
    "tokenized_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tokenization 결과를 이용해서 word to index dictionary를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<00:00, 55964.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n",
    "tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i]]\n",
    "unique_train_tokens = set(tokens)\n",
    "\n",
    "# Naïve Bayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n",
    "word2index = defaultdict() # key: word, value: index of word\n",
    "idx = 0\n",
    "for token in tqdm(unique_train_tokens):\n",
    "    word2index[token] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Naïve Bayes Classifier 모델 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, word2index, k=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word2index (dict): mapping a word to a pre-assigned index.\n",
    "            k (float, optional): constant for smoothing. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        self.k = k # for smoothing\n",
    "        self.word2index = word2index\n",
    "        self.priors = {} # Prior probability for each class, P(c)\n",
    "        self.likelihoods = {} # Likelihood for each token, P(d|c)\n",
    "    \n",
    "    def _set_priors(self, labels):\n",
    "        \"\"\"\n",
    "        Set prior probability for each class, P(c).\n",
    "        Count the number of each class and caculate P(c) for each class.\n",
    "        \"\"\"\n",
    "        # Count the number of each class\n",
    "        class_counts = defaultdict(int)\n",
    "        for label in tqdm(labels):\n",
    "            class_counts[label] += 1\n",
    "        \n",
    "        # For each class, calcuate P(c)\n",
    "        for label, count in class_counts.items():\n",
    "            self.priors[label] = class_counts[label] / len(labels)\n",
    "    \n",
    "    def _set_likelihoods(self, tokens, labels):\n",
    "        \"\"\"\n",
    "        Set likelihood for each token, P(d|c).\n",
    "        First, count the number of each class for each token.\n",
    "        Then, calculate P(d|c) for a given class and token.\n",
    "        \"\"\"\n",
    "        token_dists = {}\n",
    "        number_of_token_for_class = defaultdict(int)\n",
    "        \n",
    "        # Count the number of each class for each token\n",
    "        for i, label in enumerate(tqdm(labels)):\n",
    "            count = 0\n",
    "            for token in tokens[i]:\n",
    "                # 'token in self.word2index'부분은 안 들어가도 되지 않는가?\n",
    "                if token not in token_dists and token in self.word2index:\n",
    "                    token_dists[token] = {0:0, 1:0}\n",
    "                token_dists[token][label] += 1\n",
    "                count += 1\n",
    "            number_of_token_for_class[label] += count\n",
    "\n",
    "        for token, dist in tqdm(token_dists.items()):\n",
    "            if token not in self.likelihoods:\n",
    "                self.likelihoods[token] = {\n",
    "                    0: (token_dists[token][0] + self.k) / (number_of_token_for_class[0] + len(self.word2index) * self.k),\n",
    "                    1: (token_dists[token][1] + self.k) / (number_of_token_for_class[1] + len(self.word2index) * self.k),\n",
    "                }\n",
    "    \n",
    "    def train(self, input_tokens, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized train data.\n",
    "            labels (): train labels for each sentence/document.\n",
    "        \"\"\"\n",
    "        self._set_priors(labels)\n",
    "        self._set_likelihoods(input_tokens, labels)\n",
    "    \n",
    "    def inference(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized test data.\n",
    "        \"\"\"\n",
    "        log_prob_0 = 0.0\n",
    "        log_prob_1 = 0.0\n",
    "        \n",
    "        for token in input_tokens:\n",
    "            if token in self.likelihoods:\n",
    "                log_prob_0 += math.log(self.likelihoods[token][0])\n",
    "                log_prob_1 += math.log(self.likelihoods[token][1])\n",
    "        \n",
    "        log_prob_0 += math.log(self.priors[0])\n",
    "        log_prob_1 += math.log(self.priors[1])\n",
    "        \n",
    "        if log_prob_0 >= log_prob_1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 학습 데이터에 대해 문장 분류 모델을 학습시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 56258.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 문장 분류 모델 선언 및 학습\n",
    "classifier = NaiveBayesClassifier(word2index)\n",
    "classifier.train(tokenized_data['train'], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 test 데이터에 대해 정답값을 예측하고 Accuracy를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4164.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test data inference\n",
    "preds = []\n",
    "for test_tokens in tqdm(tokenized_data['test']):\n",
    "    pred = classifier.inference(test_tokens)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Bag-of-Words Encoding of Text Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번 챕터에서 본 Bag-of-Words를 다시 보겠습니다. \"John likes movies. Mary likes too.\"와 \"John also likes football.\"이란 두 문장이 주어졌을 때 각각 bag-of-words vector는 다음과 같습니다.\n",
    "\n",
    "![4-2-1](_image/4-2-1.PNG)\n",
    "\n",
    "각 단어를 사전으로 만들고 나타난 빈도수를 저장합니다. 그렇기에 행렬은 (키워드 개수) x (document 개수)의 형태로 나타납니다. 이 행렬을 term-document matrix(TDM)이라고 합니다. 순서 정보는 무시되는 단점이 있지만 많이 사용되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 topic은 가상의 document의 백 오브 워즈 벡터입니다. 그리고 백 오브 워즈 벡터의 값들을 정규화하면 합이 1이 되는 확률분포로 나타낼 수 있습니다. 그렇기에 토픽은 키워드들의 확률분포이자 키워드들의 가중치 조합이라고 할 수 있습니다. \n",
    "\n",
    "![4-2-2](_image/4-2-2.PNG)\n",
    "\n",
    "위 그림을 보면 맨 윗줄은 topic이고 밑에 있는 단어들은 그 topic에 속한 단어들입니다. topic의 제목은 프로그램이 자동으로 정해지며 군집에 속한 단어의 개수가 많으면 topic과 관련된 document라고 추측할 수 있습니다. 그리고 이를 바탕으로 document도 topic에 대해 군집을 만들 수 있습니다. \n",
    "\n",
    "밑의 그림은 topic modeling의 전반적 동작을 표현한 것입니다.\n",
    "\n",
    "<img src = \"https://iq.opengenus.org/content/images/2020/01/1_taTOiaCpd_CzGugx_PticQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Topic Modeling Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 동작하는 과정들을 상세히 보겠습니다.\n",
    "\n",
    "먼저 input document들을 bag-of-words vector들로 만듭니다. 이를 열벡터로 합쳐서 단어의 개수 x 문서의 개수 크기의 행렬 $A$를 만듭니다. 그리고 주어진 topic의 개수만큼 열벡터를 임의로 만듭니다. 이를 단어의 개수 x 토픽의 개수 크기의 행렬 $W$라고 하겠습니다. 이제 A를 잘 표현하도록 W에 곱할 행렬 H를 찾습니다. $H$는 토픽의 개수 x 문서의 개수 크기의 행렬일 것입니다. 이를 통해 H를 찾았다면 이제 H를 고정하고 W를 학습합니다. 이를 반복하며 최적의 W와 H를 찾는 것입니다. 이를 통해 행렬 W는 행렬 A가 가진 패턴들 중 빈도가 높은 패턴들로 학습되고 그 패턴들을 topic으로 가져가게 됩니다. \n",
    "\n",
    "이때 loss는 프로베니우스 놈으로 구합니다. \n",
    "\n",
    "$$\\lVert x \\rVert_F = (x_1^F + x_2^F + \\cdots + x_n^F)^{\\frac{1}{f}}$$\n",
    "\n",
    "벡터의 크기를 구할 때 자주 봤던 식입니다. 이를 통해 각 문서별 loss를 구하고 이것이 최소화되도록 H 안에 각 요소들을 바꿔줍니다. 이때 우리는 대체로 F = 2인 2놈을 자주 사용합니다. 이를 식으로 나타내면 다음과 같습니다. 이때 n은 단어의 개수입니다.\n",
    "\n",
    "$$\\underset{W, H \\geq 0}{arg \\; min} \\lVert A - WH \\rVert_2 = (A_1^2 - (W_1 H_1)^2 + \\cdots + A_n^2 - (W_nH_n)^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "이제 구해진 topic을 가지고 document를 분류합니다. 여러 topic들의 선형 결합으로 가장 잘 표현할 수 있는 document의 가중치를 찾고 가중치가 가장 높은 topic으로 분류합니다.\n",
    "\n",
    "밑의 그림은 이를 간단하게 표현한 것입니다.\n",
    "\n",
    "<img src = \"https://iq.opengenus.org/content/images/2020/01/1_2uj6t3gNv76SpHrWf5-z-A.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. 크롤링한 뉴스 데이터로 Topic Modeling하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 직접 크롤링한 뉴스 데이터에 대해서 topic modeling을 해보겠습니다. \n",
    "\n",
    "간단하게 전 과정을 살펴보면 먼저 네이버에서 뉴스 기사를 간단하게 크롤링합니다.  \n",
    "기본적인 전처리 이후, Term-Document Matrix를 만들고 이를 non-negative factorization을 이용해 행렬 분해 하여 topic modeling을 수행합니다.   \n",
    "\n",
    "그 후, t-distributed stochastic neighbor embedding(T-SNE) 기법을 통해 topic별 시각화를 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Crawiling News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링에 필요한 패키지 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import article\n",
    "from time import sleep, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 정적 페이지인 네이버 뉴스 신문 기사 웹페이지를 크롤링합니다. 정적 페이지와 HTML에 대해선 [자](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%A0%81_%EC%9B%B9_%ED%8E%98%EC%9D%B4%EC%A7%80)[료](https://opentutorials.org/course/2039)들을 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news(query: str=None, crawl_num: int=1000, workers: int=4):\n",
    "    \"\"\"crawl_news 뉴스 기사 텍스트가 담긴 list를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str, optional): 검색어. Defaults to None.\n",
    "        crawl_num (int, optional): 수집할 뉴스 기사의 개수. Defaults to 1000.\n",
    "        workers (int, optional): multi-processing 시, 사용할 thread의 개수. Defaults to 4.\n",
    "    \"\"\"\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "    articleList = []\n",
    "    crawled_url = set()\n",
    "    keyboard_interrupt = False\n",
    "    t = time()\n",
    "    idx = 0\n",
    "    page = 1\n",
    "    \n",
    "    # 서버에 url 요청 결과를 선언\n",
    "    res = requests.get(url.format(query))\n",
    "    sleep(0.5)\n",
    "    # res를 parsing할 parser를 선언\n",
    "    bs = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    with Pool(workers) as p:\n",
    "        while idx < crawl_num:\n",
    "            table = bs.find('ul', {'class': 'list_news'})\n",
    "            li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "            area_list = [li.find('div', {'class':'news_area'}) for li in li_list]\n",
    "            a_list = [area.find('a', {'class':'news_tit'}) for area in area_list]\n",
    "            \n",
    "            for n in a_list[:min(len(a_list), crawl_num - idx)]:\n",
    "                articleList.append(n.get('title'))\n",
    "                idx += 1\n",
    "            page += 1\n",
    "            \n",
    "            pages = bs.find('div', {'class':'sc_page_inner'})\n",
    "            next_page_url = [p for p in pages.find_all('a') if p.text == str(page)][0].get('href')\n",
    "            \n",
    "            req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "            bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return articleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '구글'\n",
    "articleList = crawl_news(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"공정위 새해도 '플랫폼 갑질' 겨눈다…구글·카카오·쿠팡 사정권\",\n",
       " '[신간] 구글은 어떻게 디자인하는가',\n",
       " \"구글, 인터넷뉴스서비스사업자 등록할까…여야 '법안 추진' 논의\",\n",
       " '비트코인도 뚫는 ‘무한대 성능’… 구글 “2029년 상업용 출시”',\n",
       " \"구글·카카오 등 플랫폼기업 '갑질' 칼빼든다\",\n",
       " '[CES 2022] MS·구글·아마존·메타도 안 나온다… 韓 독무대 된 세계 최대 IT쇼',\n",
       " '거친 운전에 차멀미가...구글 완전 자율주행차 타보니 [김성민의 실밸 레이더]',\n",
       " \"구글, UDC 스마트폰 특허 출원.. 차기 '픽셀7' 탑재될까?\",\n",
       " '구글 트렌드로 본 경제 키워드…‘블루 이코노미’에 주목하라',\n",
       " \"'적중률 70%' 미라클레터 올해도 10대기술 예측…구글 AR안경·테슬라 로봇\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleList[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tagger를 이용해 한글 명사와 알파벳만 추출해서 tdm을 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Okt 형태소 분석기 선언\n",
    "t = Okt()\n",
    "\n",
    "words_list_ = []\n",
    "vocab = Counter()\n",
    "tag_set = set(['Noun', 'Alpha'])\n",
    "stopwords = set(['글자'])\n",
    "\n",
    "for i, article in enumerate(articleList):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # tagger를 이용한 품사 태깅\n",
    "    words = t.pos(article, norm=True, stem=True)\n",
    "    \n",
    "    # 명사와 알파벳 tag를 가지며 철자 길이가 2이상이고 stopwords에 포함되지 않는 단어들로 리스트 생성\n",
    "    words = [w for w, t in words if t in tag_set and len(w) > 1 and w not in stopwords]\n",
    "    \n",
    "    vocab.update(words)\n",
    "    words_list_.append((words, article))\n",
    "    \n",
    "vocab = sorted([w for w, freq in vocab.most_common(10000)])\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "words_list = []\n",
    "for words, article in words_list_:\n",
    "    words = [w for w in words if w in word2id]\n",
    "    if len(words) > 10:\n",
    "        words_list.append((words, article))\n",
    "\n",
    "del words_list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Build document-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 document-term matrix를 만들어보겠습니다. 문서 개수 x 단어 개수의 형태를 가집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "dtm = np.zeros((len(words_list), len(vocab)), dtype=np.float32)\n",
    "for i, (words, article) in enumerate(words_list):\n",
    "    for word in words:\n",
    "        dtm[i, word2id[word]] += 1\n",
    "\n",
    "dtm = TfidfTransformer().fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 document-term matrix를 non-negative factorization(NMF)을 이용해 행렬 분해를 해보겠습니다. \n",
    "\n",
    "이때 NMF는 주어진 행렬 non-negative matrix X를 non-negative matrix W와 H로 행렬 분해하는 알고리즘입니다. 이어지는 코드를 통해 W와 H의 의미에 대해 파악하겠습니다. \n",
    "\n",
    "참고: [Non-negative Matrix Factorization](https://angeloyeo.github.io/2020/10/15/NMF.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-negative Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "K = 5\n",
    "nmf = NMF(n_components=K, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn의 NMF를 이용해 W와 H matrix를 구했습니다. \n",
    "\n",
    "W는 document length x K, H는 K x term length의 차원을 갖고 있습니다.  \n",
    "W 하나의 row는 각각의 feature에 얼만큼의 가중치를 줄 지에 대한 weight입니다.  \n",
    "H 하나의 row는 하나의 feature를 나타냅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "W = nmf.fit_transform(dtm)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 하나의 Topic(H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic\n",
      "결제 인앱 강제 금지법 정책 꼼수 애플 시간 겉도 제출 추가 요구 자료 허용 계획 이행 연내 구글 방통위 방지법 \n",
      "1th topic\n",
      "사전 레볼루션 사이트 넷마블 실시 공식 나이 세븐 등록 신작 대작 구글 게임 급등 최소 NFT 론칭 내년 오리진 이상 \n",
      "2th topic\n",
      "삼성 미국 전자 픽셀 모뎀 기관 조사 탑재 처음 마이크로소프트 부회장 영진 확인 동맹 이재용 아마존 시장 구글 사운드 공개 \n",
      "3th topic\n",
      "애플 규제 경쟁 위원장 저승사자 EU 규칙 촉구 플랫폼 시행 구글 공룡 방지 강화 엄격 적용 테크 넷플릭스 방통위 시급 \n",
      "4th topic\n",
      "검색 도전 시장 아마존 클라우드 비즈 확대 과감 위해 글로벌 투자 vs 전쟁 점화 사용자 국내 네이버 테크 구글 메타 \n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    print(f\"{k}th topic\")\n",
    "    for index in H[k].argsort()[::-1][:20]:\n",
    "        print(vocab[index], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 W에서 하나의 topic (W의 n번째 column)에 접근해서 해당 topic에 대해 값이 가장 높은 3개의 뉴스 기사 제목을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===0th topic===\n",
      "겉도는 인앱결제 강제금지법… 구글은 결제정책 꼼수, 애플은 시간끌기\n",
      "겉도는 인앱결제 강제금지법… 구글은 결제정책 꼼수, 애플은 시간끌기\n",
      "구글은 결제정책 꼼수, 애플은 모르쇠… 힘못쓰는 갑질방지법 [인앱결제강제 금지법 시행 100일]\n",
      "\n",
      "===1th topic===\n",
      "넷마블, 기대신작 세븐나이츠 레볼루션...'구글·공식 사이트 사전등록 실시'\n",
      "넷마블, 기대신작 '세븐나이츠 레볼루션' 구글/공식 사이트 사전등록 실시\n",
      "넷마블, 기대작 세븐나이츠 레볼루션 구글·공식 사이트 사전등록 실시\n",
      "\n",
      "===2th topic===\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "\n",
      "===3th topic===\n",
      "'구글‧애플 저승사자' EU경쟁위원장, 빅테크 규제 위한 규칙 시행 촉구\n",
      "'구글‧애플 저승사자' EU경쟁위원장, 빅테크 규제 위한 규칙 시행 촉구\n",
      "구글·애플·넷플릭스 ‘플랫폼 공룡’ 갑질 방지 강화… 방통위, 규제 엄격 적용\n",
      "\n",
      "===4th topic===\n",
      "구글 vs 네이버… 빅테크 ‘검색전쟁’ 재점화 국내시장 1위에 ‘사용자 친화 검색’으로 도전장\n",
      "구글 vs 네이버… 빅테크 ‘검색전쟁’ 재점화 국내시장 1위에 ‘사용자 친화 검색’으로 도전장\n",
      "[글로벌 비즈] 구글 클라우드, 시장 확대 위해 과감한 투자…아마존에 도전장\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    print(f\"==={k}th topic===\")\n",
    "    for index in W[:, k].argsort()[::-1][:3]:\n",
    "        print(words_list[index][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번째 topic에 대해 가장 높은 가중치를 갖는 제목 5개를 출력하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "[더벨][뉴삼성 차세대 리더십]소프트웨어 경쟁력 높인다…구글·MS출신 발탁승진\n"
     ]
    }
   ],
   "source": [
    "for index in W[:, 2].argsort()[::-1][:5]:\n",
    "    print(words_list[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 t-SNE를 이용해 topic별 시각화를 진행하겠습니다. \n",
    "\n",
    "t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 벡터를 데이터간 구조적 특징을 유지한 상태로 저차원(2~3차원) 벡터로 축소하는 방법 중 하나입니다. 주로 고차원 데이터의 시각화를 위해 사용됩니다.\n",
    "\n",
    "참고: [lovit: t-SNE](https://lovit.github.io/nlp/representation/2018/09/28/tsne/#:~:text=t%2DSNE%20%EB%8A%94%20%EA%B3%A0%EC%B0%A8%EC%9B%90%EC%9D%98,%EC%9D%98%20%EC%A7%80%EB%8F%84%EB%A1%9C%20%ED%91%9C%ED%98%84%ED%95%A9%EB%8B%88%EB%8B%A4.)\n",
    "\n",
    "참고: [ratsgo: t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 63 nearest neighbors...\n",
      "[t-SNE] Indexed 64 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 64 samples in 0.003s...\n",
      "[t-SNE] Computed conditional probabilities for sample 64 / 64\n",
      "[t-SNE] Mean sigma: 0.107609\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.561291\n",
      "[t-SNE] KL divergence after 1000 iterations: -0.062155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# n_components = 차원 수\n",
    "tsne = TSNE(n_components=2, init='pca', verbose=1)\n",
    "\n",
    "# W matrix에 대해 t-sne를 수행합니다.\n",
    "W2d = tsne.fit_transform(W)\n",
    "\n",
    "# 각 뉴스 기사 제목마다 가중치가 가장 높은 topic을 저장합니다.\n",
    "topicIndex = [v.argmax() for v in W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1131\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  const JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1131\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1131\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"5a684a58-54da-4364-ac07-102665a18197\" data-root-id=\"1132\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n    \n  const docs_json = {\"b4682a68-fa0e-4e7d-8772-58b06cbc8579\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1141\"}],\"center\":[{\"id\":\"1144\"},{\"id\":\"1148\"},{\"id\":\"1181\"}],\"height\":580,\"left\":[{\"id\":\"1145\"}],\"renderers\":[{\"id\":\"1168\"}],\"title\":{\"id\":\"1170\"},\"toolbar\":{\"id\":\"1156\"},\"width\":720,\"x_range\":{\"id\":\"1133\"},\"x_scale\":{\"id\":\"1137\"},\"y_range\":{\"id\":\"1135\"},\"y_scale\":{\"id\":\"1139\"}},\"id\":\"1132\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"label\":{\"field\":\"topic\"},\"renderers\":[{\"id\":\"1168\"}]},\"id\":\"1182\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"1163\"}},\"id\":\"1169\",\"type\":\"CDSView\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1173\"},\"group\":null,\"major_label_policy\":{\"id\":\"1174\"},\"ticker\":{\"id\":\"1146\"}},\"id\":\"1145\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data\":{\"color\":[\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#aec7e8\",\"#aec7e8\",\"#ffbb78\",\"#2ca02c\",\"#aec7e8\",\"#ffbb78\",\"#2ca02c\",\"#aec7e8\",\"#ff7f0e\",\"#2ca02c\",\"#ffbb78\",\"#aec7e8\",\"#ffbb78\",\"#aec7e8\",\"#ffbb78\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ff7f0e\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\"],\"document\":[\"'\\uc801\\uc911\\ub960 70%' \\ubbf8\\ub77c\\ud074\\ub808\\ud130 \\uc62c\\ud574\\ub3c4 10\\ub300\\uae30\\uc220 \\uc608\\uce21\\u2026\\uad6c\\uae00 AR\\uc548\\uacbd\\u00b7\\ud14c\\uc2ac\\ub77c \\ub85c\\ubd07\",\"\\\"\\uc62c\\ud574 \\uba54\\ud0c0\\ubc84\\uc2a4 \\uacbd\\uc7c1 \\ubcf8\\uaca9\\ud654\\\"...\\uba54\\ud0c0\\uac00 \\uc120\\ub450, \\uc560\\ud50c\\u00b7MS\\u00b7\\uad6c\\uae00\\uc774 \\ucd94\\uaca9\",\"\\uacf5\\uc815\\uc704\\uc6d0\\uc7a5 \\\"\\uad6c\\uae00 \\uc571\\ub9c8\\ucf13\\uc2dc\\uc7a5 \\uacbd\\uc7c1\\uc81c\\ud55c\\ud589\\uc704 \\uc804\\uc6d0\\ud68c\\uc758 \\uc2ec\\uc758 \\uc608\\uc815\\\"\",\"\\ud2f0\\uc564\\ucf00\\uc774\\ud329\\ud1a0\\ub9ac '\\uac80\\uc740\\uc655\\uad00:\\uba54\\uae30\\uc655\\uc758 \\ubd84\\ub178' \\uad6c\\uae00 \\ud50c\\ub808\\uc774 \\uc778\\uae30 \\uc21c\\uc704 1\\uc704 \\ub2ec\\uc131\",\"[\\uae00\\ub85c\\ubc8c \\ube44\\uc988] \\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc, \\uc2dc\\uc7a5 \\ud655\\ub300 \\uc704\\ud574 \\uacfc\\uac10\\ud55c \\ud22c\\uc790\\u2026\\uc544\\ub9c8\\uc874\\uc5d0 \\ub3c4\\uc804\\uc7a5\",\"\\uc560\\ud50c, \\uc5f0\\ub0b4 \\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c\\uae08\\uc9c0\\ubc95 \\uc774\\ud589\\uacc4\\ud68d \\uc81c\\ucd9c\\ud560\\uae4c?\\u2026\\ubc29\\ud1b5\\uc704, \\uad6c\\uae00\\uc5d0\\ub3c4 '\\uc81c3\\uc790 \\uacb0\\uc81c \\ud5c8\\uc6a9' \\ucd94\\uac00\\uc790\\ub8cc \\uc694\\uad6c\",\"\\uad6c\\uae00\\u00b7\\uc560\\ud50c\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4 \\u2018\\ud50c\\ub7ab\\ud3fc \\uacf5\\ub8e1\\u2019 \\uac11\\uc9c8 \\ubc29\\uc9c0 \\uac15\\ud654\\u2026 \\ubc29\\ud1b5\\uc704, \\uaddc\\uc81c \\uc5c4\\uaca9 \\uc801\\uc6a9\",\"[\\uae00\\ub85c\\ubc8c \\ube44\\uc988] \\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc, \\uc2dc\\uc7a5 \\ud655\\ub300 \\uc704\\ud574 \\uacfc\\uac10\\ud55c \\ud22c\\uc790\\u2026\\uc544\\ub9c8\\uc874\\uc5d0 \\ub3c4\\uc804\\uc7a5\",\"\\uc560\\ud50c, \\uc5f0\\ub0b4 \\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c\\uae08\\uc9c0\\ubc95 \\uc774\\ud589\\uacc4\\ud68d \\uc81c\\ucd9c\\ud560\\uae4c?\\u2026\\ubc29\\ud1b5\\uc704, \\uad6c\\uae00\\uc5d0\\ub3c4 '\\uc81c3\\uc790 \\uacb0\\uc81c \\ud5c8\\uc6a9' \\ucd94\\uac00\\uc790\\ub8cc \\uc694\\uad6c\",\"\\uad6c\\uae00\\u00b7\\uc560\\ud50c\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4 \\u2018\\ud50c\\ub7ab\\ud3fc \\uacf5\\ub8e1\\u2019 \\uac11\\uc9c8 \\ubc29\\uc9c0 \\uac15\\ud654\\u2026 \\ubc29\\ud1b5\\uc704, \\uaddc\\uc81c \\uc5c4\\uaca9 \\uc801\\uc6a9\",\"\\uad6c\\uae00 \\uacf5\\uc778 \\uad50\\uc721\\uc804\\ubb38\\uac00 \\uae40\\ub3d9\\uc6d0 \\ub300\\ud45c\\uc758 'Google \\ud65c\\uc6a9\\ubc95' \\ub124\\uc774\\ubc84 \\uc5d1\\uc2a4\\ud37c\\ud2b8 \\uc624\\ud508\",\"\\\"\\uc720\\ud29c\\ube0c\\ub3c4 \\ud1a0\\uc2a4\\ud398\\uc774\\ub85c\\\"\\u2026 \\ud1a0\\uc2a4\\ud398\\uc774\\uba3c\\uce20, \\uad6c\\uae00 \\uc81c\\ud734\\ub85c \\uac04\\ud3b8\\uacb0\\uc81c \\ud655\\uc7a5\",\"\\uc624\\ubbf8\\ud06c\\ub860 \\ud655\\uc0b0\\uc5d0 CES \\ube68\\uac04\\ubd88\\u2026\\ucd5c\\ud0dc\\uc6d0\\u00b7\\uc815\\uc758\\uc120 \\ucd9c\\uc7a5 \\uc7ac\\uac80\\ud1a0, \\uad6c\\uae00\\u00b7\\uba54\\ud0c0\\u00b7MS \\ubd88\\ucc38\",\"LG CNS, \\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988\\uc11c \\ub9ac\\ub4dc \\uc0dd\\uc131\\uacfc \\ubca0\\uc2a4\\ud2b8\\ud300 2\\uac1c\\ubd80\\ubb38 \\uc218\\uc0c1\",\"\\ubbf8\\ub974\\uc758 \\uc804\\uc1242: MOM, \\uad6c\\uae00 \\ub9e4\\ucd9c \\uc21c\\uc704 \\uc0c1\\uc2b9\\uc138, \\uc0c1\\uc704\\uad8c \\uc9c4\\uc785 \\ubc1c\\ud310 \\ub9c8\\ub828\",\"LG CNS, \\u2018\\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988\\u2019 3\\ud68c \\uc5f0\\uc18d \\uc218\\uc0c1\\u2026\\uc720\\uc77c 2\\uac1c\\ubd80\\ubb38 \\uc120\\uc815\",\"\\uc544\\ub9c8\\uc874. \\uba54\\ud0c0. \\ud2b8\\uc704\\ud130 \\uc774\\uc5b4 GM. \\uad6c\\uae00 \\uc6e8\\uc774\\ubaa8\\ub3c4 \\uc624\\ubbf8\\ud06c\\ub860 \\ud655\\uc0b0\\uc73c\\ub85c 'CES 2022' \\ucc38\\uac00 \\ucde8\\uc18c\",\"\\ud574\\ud53c\\uba38\\ub2c8, \\uad6c\\uae00\\uae30\\ud504\\ud2b8\\ucf54\\ub4dc \\uad6c\\ub9e4\\uace0\\uac1d \\ub300\\uc0c1 \\ub9ac\\ub2c8\\uc9c0W \\uc544\\uc774\\ud15c \\uc774\\ubca4\\ud2b8 \\uc9c4\\ud589\",\"\\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\ubaa8\\ub974\\uc1e0\\u2026 \\ud798\\ubabb\\uc4f0\\ub294 \\uac11\\uc9c8\\ubc29\\uc9c0\\ubc95 [\\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c \\uae08\\uc9c0\\ubc95 \\uc2dc\\ud589 100\\uc77c]\",\"\\ud1a0\\uc2a4\\ud398\\uc774\\uba3c\\uce20 \\uad6c\\uae00\\uacfc \\ud1a0\\uc2a4\\ud398\\uc774 \\uc81c\\ud734, \\uae40\\ubbfc\\ud45c \\\"\\uac04\\ud3b8\\ud55c \\uacb0\\uc81c\\uacbd\\ud5d8 \\uc9c0\\uc6d0\\\"\",\"\\uac89\\ub3c4\\ub294 \\uc778\\uc571\\uacb0\\uc81c \\uac15\\uc81c\\uae08\\uc9c0\\ubc95\\u2026 \\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\uc2dc\\uac04\\ub04c\\uae30\",\"\\uac89\\ub3c4\\ub294 \\uc778\\uc571\\uacb0\\uc81c \\uac15\\uc81c\\uae08\\uc9c0\\ubc95\\u2026 \\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\uc2dc\\uac04\\ub04c\\uae30\",\"[\\ud2b9\\uc9d5\\uc8fc] FSN, '\\ud2f1\\ud1a1' \\uad6c\\uae00 \\ub204\\ub974\\uace0 \\uc804\\uc138\\uacc41\\uc704 \\uae30\\ub85d\\u2026 \\uc804\\uc138\\uacc4 \\ud2f1\\ud1a1 \\uad11\\uace0\\ub300\\ud589 \\uad8c\\ud55c \\ubd80\\uac01\",\"[\\uc9d1\\uc911\\ucde8\\uc7acM] MS \\uad6c\\uae00 A\\ud559\\uc810, \\ub124\\uc774\\ubc84 \\uce74\\uce74\\uc624 F\\ud559\\uc810\\u2025IT\\uae30\\uc5c5\\ub4e4\\uc758 \\ud0c4\\uc18c \\uc131\\uc801\\ud45c\",\"[\\uae08\\uc77c \\uc0b0\\uc5c5\\uacc4 \\uc8fc\\uc694\\uae30\\uc0ac] \\\"\\uc624\\ubbf8\\ud06c\\ub860 \\ubcc0\\uc774 \\ud655\\uc0b0\\\" \\uad6c\\uae00\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uba54\\ud0c0\\u00b7\\ud2f1\\ud1a1 CES \\ubd88\\ucc38\\u2026 \\\"8K \\uc0dd\\ud0dc\\uacc4 \\ud655\\uc7a5 \\uac00\\uc18d\\ud654\\\" \\uc544\\ub9c8\\uc874, '8K \\ud611\\ud68c' \\ud569\\ub958 \\u5916\",\"[\\ub274\\uc2a4\\ud574\\uc124]\\uc560\\ud50c\\u00b7\\uad6c\\uae00 \\uc790\\uccb4\\ub4f1\\uae09\\ubd84\\ub958 \\ub0a8\\uc6a9...\\uae30\\uc900 \\uc815\\ube44 \\uc2dc\\uae09\",\"[\\ub274\\uc2a4\\ud574\\uc124]\\uc560\\ud50c\\u00b7\\uad6c\\uae00 \\uc790\\uccb4\\ub4f1\\uae09\\ubd84\\ub958 \\ub0a8\\uc6a9...\\uae30\\uc900 \\uc815\\ube44 \\uc2dc\\uae09\",\"LG\\uc804\\uc790, 2022\\ub144\\ud615 \\uc0ac\\uc6b4\\ub4dc \\ubc14 \\uacf5\\uac1c...\\uad6c\\uae00\\u00b7\\uc544\\ub9c8\\uc874, \\uc778\\uacf5\\uc9c0\\ub2a5 \\uc2a4\\ud53c\\ucee4 \\uc5f0\\ub3d9 \\ud1b5\\ud574 \\uc74c\\uc131\\ub9cc\\uc73c\\ub85c \\uc0ac\\uc6b4\\ub4dc \\ubc14 \\uc870\\uc791\",\"[AICON \\uad11\\uc8fc 2021] \\uad6c\\uae00 \\ucf54\\ub9ac\\uc544 \\uae40\\ud0dc\\uc6d0 \\uc804\\ubb34 \\\"\\ub514\\uc9c0\\ud138 \\uae30\\uc220\\uc740 \\uc138\\uc0c1\\uc758 \\ubb38\\uc81c\\ub97c \\ud574\\uacb0\\ud55c\\ub2e4\\\"\",\"\\uc120\\uc6b0\\uc724 \\uc640\\uadf8 \\ub300\\ud45c \\\"\\uad6c\\uae00 \\ud611\\uc5c5 \\uccab \\uad6d\\ub0b4 OTA\\u2026\\uc678\\uad6d\\uc778\\ub3c4 \\uc774\\uc81c \\uc6b0\\ub9ac \\uace0\\uac1d\\\" [\\uc778\\ud130\\ubdf0]\",\"\\uad11\\uc9c4\\uc708\\ud14d, 3D\\uc0ac\\uc6b4\\ub4dc \\ucd2c\\uc601 \\uc2e0\\uae30\\uc220 \\uac1c\\ubc1c...\\uad6c\\uae00 \\ub4f1 \\uc2a4\\ub9c8\\ud2b8\\ub514\\ubc14\\uc774\\uc2a4 \\uacb0\\ud569 1\\ucc28 \\uc0c1\\uc6a9\\ubaa8\\ub378 \\uc591\\uc0b0 \\ubaa9\\ud45c\",\"'\\uad6c\\uae00\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4' \\ub300\\uc0c1 \\ub9dd \\uc548\\uc815\\uc131 \\ud655\\ubcf4 \\uac00\\uc774\\ub4dc\\ub77c\\uc778 \\ub9c8\\ub828\\u2026 \\uc778\\ud130\\ub137 \\ud68c\\uc120 \\uc6a9\\ub7c9 \\ud655\\ubcf4 \\uc758\\ubb34\\ud654\",\"\\uad6c\\uae00 vs \\ub124\\uc774\\ubc84\\u2026 \\ube45\\ud14c\\ud06c \\u2018\\uac80\\uc0c9\\uc804\\uc7c1\\u2019 \\uc7ac\\uc810\\ud654 \\uad6d\\ub0b4\\uc2dc\\uc7a5 1\\uc704\\uc5d0 \\u2018\\uc0ac\\uc6a9\\uc790 \\uce5c\\ud654 \\uac80\\uc0c9\\u2019\\uc73c\\ub85c \\ub3c4\\uc804\\uc7a5\",\"\\uad6c\\uae00 vs \\ub124\\uc774\\ubc84\\u2026 \\ube45\\ud14c\\ud06c \\u2018\\uac80\\uc0c9\\uc804\\uc7c1\\u2019 \\uc7ac\\uc810\\ud654 \\uad6d\\ub0b4\\uc2dc\\uc7a5 1\\uc704\\uc5d0 \\u2018\\uc0ac\\uc6a9\\uc790 \\uce5c\\ud654 \\uac80\\uc0c9\\u2019\\uc73c\\ub85c \\ub3c4\\uc804\\uc7a5\",\"[\\uc704\\ud074\\ub9ac\\ub9ac\\ud3ec\\ud2b8 60\\ud638] '\\uc62c\\ud574 \\ud55c\\uad6d\\uc778\\uc774 \\uad6c\\uae00\\uc11c \\uac00\\uc7a5 \\ub9ce\\uc774 \\ucc3e\\uc740 \\uac80\\uc0c9\\uc5b4' 3\\uc704 \\uc624\\uc9d5\\uc5b4\\uac8c\\uc784, 2\\uc704 \\ubc31\\uc2e0\\uc608\\uc57d, 1\\uc704\\ub294\\u2026\",\"[\\uc704\\ud074\\ub9ac\\ub9ac\\ud3ec\\ud2b8 60\\ud638] '\\uc62c\\ud574 \\ud55c\\uad6d\\uc778\\uc774 \\uad6c\\uae00\\uc11c \\uac00\\uc7a5 \\ub9ce\\uc774 \\ucc3e\\uc740 \\uac80\\uc0c9\\uc5b4' 3\\uc704 \\uc624\\uc9d5\\uc5b4\\uac8c\\uc784, 2\\uc704 \\ubc31\\uc2e0\\uc608\\uc57d, 1\\uc704\\ub294\\u2026\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc791 \\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158 \\uad6c\\uae00\\u00b7\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc\",\"\\uc5d4\\ub3cc\\ud540\\ucee4\\ub125\\ud2b8 \\uc2e0\\uc791 '\\uc2a4\\ud0d1 \\ubc14\\uc774: \\uc0b0\\ud0c0 \\ub808\\uc774\\uc2a4' \\uad6c\\uae00 \\ud50c\\ub808\\uc774 \\uc2a4\\ud1a0\\uc5b4 \\ucd9c\\uc2dc\",\"\\ubbf8, \\uad6c\\uae00\\u00b7\\uba54\\ud0c0 '\\ud0dc\\ud3c9\\uc591 \\uad11\\ucf00\\uc774\\ube14' \\uc0ac\\uc6a9\\uc2b9\\uc778 \\uad8c\\uace0\\u2026\\\"\\uc548\\ubcf4 \\uc6b0\\ub824\\ub85c \\ud64d\\ucf69 \\uad6c\\uac04\\uc740 \\uc81c\\uc678\\\"\",\"\\uc2e4\\uc801 \\uc804\\ub9dd \\uc5c7\\uac08\\ub9ac\\ub294 \\u7f8e \\ube45\\ud14c\\ud06c, MS\\u00b7\\uad6c\\uae00\\u00b7\\uba54\\ud0c0 \\u2018\\ub9d1\\uc74c\\u2019 vs \\uc560\\ud50c\\u00b7\\uc544\\ub9c8\\uc874 \\u2018\\uae00\\uc384\\u2019\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc2e0\\uc791 \\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158...'\\uad6c\\uae00\\u00b7\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc'\",\"\\uc544\\uc774\\uc2a4\\ubc84\\ub4dc \\uac8c\\uc784\\uc988 '\\ubbf8\\ub974\\uc758 \\uc804\\uc1242: MOM', \\uad6c\\uae00 \\ubb34\\ub8cc \\uc778\\uae30 \\uc21c\\uc704 2\\uc704 \\ub2ec\\uc131\",\"\\ud55c\\uad6d\\uacf5\\uc608\\ub514\\uc790\\uc778\\ubb38\\ud654\\uc9c4\\ud765\\uc6d0, \\uad6c\\uae00 \\uc544\\ud2b8 \\uc564 \\uceec\\ucc98\\uc640 \\ud611\\uc5c5...\\ud55c\\uad6d\\uc758 \\uacf5\\uc608\\ubb38\\ud654 \\uc120\\ubcf4\\uc5ec\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc2e0\\uc791 '\\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158' \\uad6c\\uae00/\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc\",\"[\\ub354\\ubca8][\\ub274\\uc0bc\\uc131 \\ucc28\\uc138\\ub300 \\ub9ac\\ub354\\uc2ed]\\uc18c\\ud504\\ud2b8\\uc6e8\\uc5b4 \\uacbd\\uc7c1\\ub825 \\ub192\\uc778\\ub2e4\\u2026\\uad6c\\uae00\\u00b7MS\\ucd9c\\uc2e0 \\ubc1c\\ud0c1\\uc2b9\\uc9c4\",\"\\ud3ec\\uc2a4\\ud14d, \\uc2a4\\ud3ec\\uce20 \\uc778\\uacf5\\uc9c0\\ub2a5(AI) \\ubd84\\uc57c...\\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc \\uc544\\u2027\\ud0dc \\uacf5\\uacf5\\ubd80\\ubb38 \\ud30c\\ud2b8\\ub108 \\uc120\\uc815\",\"[\\uc810\\uc2ec \\ube0c\\ub9ac\\ud551]\\uc554\\ud638\\ud654\\ud3d0 \\ud558\\ub77d\\uc138\\u00b7\\u00b7\\u00b7\\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uad6c\\uae00 \\uc804 \\ud68c\\uc7a5, \\uccb4\\uc778\\ub9c1\\ud06c \\ud504\\ub85c\\uc81d\\ud2b8 \\ud569\\ub958\",\"\\ub124\\uc624\\ub9ac\\uc9c4, \\uc2e0\\uc791 \\uac8c\\uc784 \\uad6c\\uae00 \\ub4f1 3\\ub300 \\uc571 \\ub9c8\\ucf13\\ub860\\uce6d\\u2026\\u201c\\ub0b4\\ub144 \\ucd5c\\uc18c 2\\uac1c \\uc774\\uc0c1 \\uc2e0\\uc791\\uacfc NFT\\uac8c\\uc784 \\uacf5\\uac1c \\uc608\\uc815\\u201d\",\"[AICON \\uad11\\uc8fc 2021] \\uad6c\\uae00 \\ucf54\\ub9ac\\uc544 \\uae40\\ud0dc\\uc6d0 \\uc804\\ubb34 \\uae30\\uc870 \\uac15\\uc5f0 \\ub098\\uc11c... \\\"\\ub514\\uc9c0\\ud138 \\uae30\\uc220 \\ud1b5\\ud574 \\ub2f9\\ub300\\uc758 \\uad00\\uc2ec\\uc0ac\\uc640 \\ubcc0\\ud654 \\ud30c\\uc545\\ud560 \\uc218 \\uc788\\uc5b4!\\\"\",\"\\uc601\\uc6c5 \\uc218\\uc9d1 \\ud30c\\ud2f0 \\ubc29\\uce58\\ud615 RPG '\\ub9d0\\ub2e8 \\ubcd1\\uc0ac\\uc5d0\\uc11c \\uad70\\uc8fc\\uae4c\\uc9c0 - \\uad70\\uc8fc \\ud0a4\\uc6b0\\uae30' \\uad6c\\uae00\\ud50c\\ub808\\uc774 \\ucd9c\\uc2dc\",\"[\\ud2b9\\uc9d5\\uc8fc]NHN\\ubc85\\uc2a4, \\uad6c\\uae00\\u00b7\\uc560\\ud50c\\uc5d0 \\uc2f8\\uc774\\uc6d4\\ub4dc \\uc571 \\uc2e0\\uccad\\u2026\\ub124\\uc774\\ubc84\\u00b7\\uce74\\uce74\\uc624 \\uc787\\ub294 K-\\ud50c\\ub7ab\\ud3fc \\ucd9c\\uc0ac\\ud45c\",\"\\uad6c\\uae00, 12\\uc6d4 '\\ud53d\\uc140 \\ud53c\\ucc98 \\ub4dc\\ub86d' \\uc5c5\\ub370\\uc774\\ud2b8 \\uacf5\\uac1c.. '\\ud53d\\uc1406' \\uc9c0\\ubb38 \\uc13c\\uc11c \\uc131\\ub2a5 \\uac1c\\uc120\",\"\\uc870\\uc0ac\\uae30\\uad00 \\u201c\\uad6c\\uae00 \\ud53d\\uc1406\\uc5d0 \\uc0bc\\uc131\\uc804\\uc790 5G\\ubaa8\\ub380 \\ud0d1\\uc7ac, \\ubbf8\\uad6d\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucc98\\uc74c\\\"\",\"\\uc870\\uc0ac\\uae30\\uad00 \\u201c\\uad6c\\uae00 \\ud53d\\uc1406\\uc5d0 \\uc0bc\\uc131\\uc804\\uc790 5G\\ubaa8\\ub380 \\ud0d1\\uc7ac, \\ubbf8\\uad6d\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucc98\\uc74c\\\"\",\"[\\uae09\\ub4f1\\uc655\\uc758 '\\uae09\\ub4f1\\ub9e5 \\ud544\\uc0b4\\uae30'] NHN\\ubc85\\uc2a4, \\uad6c\\uae00\\u00b7\\uc560\\ud50c \\uc571 \\ub4f1\\ub85d \\uc2ec\\uc0ac \\uc2e0\\uccad \\uc18c\\uc2dd\\uc5d0 \\uc0c1\\uc2b9! GO? STOP?!\",\"\\uad6c\\uae00 \\uc790\\uccb4 \\uac1c\\ubc1c \\uc571\\ud50c\\ub808\\uc774\\uc5b4 'Google Play Games' \\uacf5\\uac1c, \\uc571 \\ud50c\\ub808\\uc774\\uc5b4 \\uc0dd\\ud0dc\\uacc4 \\uc9c0\\uac01\\ubcc0\\ub3d9 \\uc62c\\uae4c\",\"\\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uc804 \\uad6c\\uae00 CEO, \\ube14\\ub85d\\uccb4\\uc778 \\uc624\\ub77c\\ud074 \\uc194\\ub8e8\\uc158 \\uac1c\\ubc1c\\uc0ac '\\uccb4\\uc778\\ub9c1\\ud06c' \\ud569\\ub958\",\"[\\uc5ec\\uc758\\uc625] \\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uc804 \\uad6c\\uae00 CEO, \\ube14\\ub85d\\uccb4\\uc778 \\uc624\\ub77c\\ud074 \\uc194\\ub8e8\\uc158 \\uac1c\\ubc1c\\uc0ac \\uc804\\uaca9 \\ud569\\ub958\",\"\\ub098\\uc2a4\\ubbf8\\ub514\\uc5b4, '\\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988 2021' \\uc218\\uc0c1\\u2026\\ube0c\\ub79c\\ub4dc \\uc9c0\\uc18d\\uac00\\ub2a5 \\uc131\\uc7a5\\uc5d0 \\uae30\\uc5ec\",\"[\\uc0bc\\uc815KPMG \\ubcf4\\uace0\\uc11c] \\ud14c\\ud06c \\uae30\\uc5c5, ESG \\ub9ac\\uc2a4\\ud06c \\uc9c1\\uba74\\u2026 \\\"\\uc6b0\\uc120 \\uacfc\\uc81c \\ub3c4\\ucd9c\\ud574\\uc57c\\\"\",\"\\uc774\\uc7ac\\uc6a9 \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ud68c\\uc7a5, \\ubbf8\\uad6d\\uc5d0\\uc11c '\\ub274\\uc0bc\\uc131' \\ub3d9\\ub9f9 \\ud655\\uc778..\\ub9c8\\uc774\\ud06c\\ub85c\\uc18c\\ud504\\ud2b8\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uad6c\\uae00 \\uacbd\\uc601\\uc9c4 \\uc787\\ub530\\ub77c \\ub9cc\\ub098\",\"\\uc774\\uc7ac\\uc6a9 \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ud68c\\uc7a5, \\ubbf8\\uad6d\\uc5d0\\uc11c '\\ub274\\uc0bc\\uc131' \\ub3d9\\ub9f9 \\ud655\\uc778..\\ub9c8\\uc774\\ud06c\\ub85c\\uc18c\\ud504\\ud2b8\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uad6c\\uae00 \\uacbd\\uc601\\uc9c4 \\uc787\\ub530\\ub77c \\ub9cc\\ub098\",\"'\\uad6c\\uae00\\u2027\\uc560\\ud50c \\uc800\\uc2b9\\uc0ac\\uc790' EU\\uacbd\\uc7c1\\uc704\\uc6d0\\uc7a5, \\ube45\\ud14c\\ud06c \\uaddc\\uc81c \\uc704\\ud55c \\uaddc\\uce59 \\uc2dc\\ud589 \\ucd09\\uad6c\",\"'\\uad6c\\uae00\\u2027\\uc560\\ud50c \\uc800\\uc2b9\\uc0ac\\uc790' EU\\uacbd\\uc7c1\\uc704\\uc6d0\\uc7a5, \\ube45\\ud14c\\ud06c \\uaddc\\uc81c \\uc704\\ud55c \\uaddc\\uce59 \\uc2dc\\ud589 \\ucd09\\uad6c\"],\"id\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],\"topic\":[\"3\",\"3\",\"3\",\"3\",\"4\",\"0\",\"3\",\"4\",\"0\",\"3\",\"4\",\"0\",\"4\",\"4\",\"3\",\"4\",\"4\",\"3\",\"0\",\"0\",\"0\",\"0\",\"3\",\"4\",\"4\",\"3\",\"3\",\"2\",\"3\",\"4\",\"3\",\"3\",\"4\",\"4\",\"3\",\"3\",\"1\",\"1\",\"3\",\"4\",\"1\",\"3\",\"4\",\"1\",\"2\",\"4\",\"3\",\"1\",\"3\",\"1\",\"3\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"2\",\"2\",\"3\",\"3\"],\"x\":{\"__ndarray__\":\"UUTfQvR4IMNRyKnCwjTvQkkeQsPhb6zD6XBBw0keQsPhb6zD6XBBwwXJ+0GEqkfD6mQ+wlJo40G/IBZDFX+JQcrbJcKNS0BD3UTPwwmrG8NxhLzDcYS8w5Qzd0OsNApDX9wLwr9lZ8O/ZWfD9GJhQya9LEP8qKNCQ65XQ630wkGkAAvDpAALw95jXsLeY17CigIwQqyuk0MKNZ1CUkDJwih+w0J/Z2ZDDDgFQyh+w0I2apRDgvLWwglboEIld5RDy845QwVsQ0OalyDDPPCMQ0Tg1UNE4NVDx9TkQvPYA0NBqHBCyM4EQVRIo0IESaFB9OHUQ/Th1EM7ToTDO06Eww==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[64]},\"y\":{\"__ndarray__\":\"0JeTQkGdF8OK0S3DfpkvQ+YfZ0OaZgdCaDOGw+YfZ0OaZgdCaDOGw9teMEM0s3pC//+QQm+60cIb+O9Cdy8twyU5E0Oc9HbCBC/qQajNBEKEpu7BhKbuwdCrjcFsItzCktm/wivJGMMryRjD7F0fw5Q8gELu1Q3DeIXbQu9Y18Hb+oVD2/qFQ16pW8FeqVvBvkusQzd8k0L/ePJCgW3SwoLOpENspDZCD0S3QYLOpEM1kSXDh6/1QllyKMC4/RRDIxuPP5a0MEO7KJrCf5zSwkjb/MFI2/zBzLBZw8IQDsLsDUdCAYPuQQ+XkcILj8ZCvwjBwr8IwcLZuYHD2bmBww==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[64]}},\"selected\":{\"id\":\"1179\"},\"selection_policy\":{\"id\":\"1178\"}},\"id\":\"1163\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1177\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1152\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1153\",\"type\":\"ResetTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1155\"}},\"id\":\"1150\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1151\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1133\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis\":{\"id\":\"1141\"},\"coordinates\":null,\"group\":null,\"ticker\":null},\"id\":\"1144\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1176\"},\"group\":null,\"major_label_policy\":{\"id\":\"1177\"},\"ticker\":{\"id\":\"1142\"}},\"id\":\"1141\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Topic\",\"@topic\"],[\"id\",\"@id\"],[\"Article\",\"@document\"]]},\"id\":\"1149\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1167\",\"type\":\"Circle\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1163\"},\"glyph\":{\"id\":\"1165\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1167\"},\"nonselection_glyph\":{\"id\":\"1166\"},\"view\":{\"id\":\"1169\"}},\"id\":\"1168\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1155\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1146\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1154\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1182\"}],\"location\":\"top_left\"},\"id\":\"1181\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1137\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1135\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1173\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1139\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\"},\"hatch_color\":{\"field\":\"color\"},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1165\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1178\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1174\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1142\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1176\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1179\",\"type\":\"Selection\"},{\"attributes\":{\"tools\":[{\"id\":\"1149\"},{\"id\":\"1150\"},{\"id\":\"1151\"},{\"id\":\"1152\"},{\"id\":\"1153\"},{\"id\":\"1154\"}]},\"id\":\"1156\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis\":{\"id\":\"1145\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null},\"id\":\"1148\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"group\":null},\"id\":\"1170\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1166\",\"type\":\"Circle\"}],\"root_ids\":[\"1132\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n  const render_items = [{\"docid\":\"b4682a68-fa0e-4e7d-8772-58b06cbc8579\",\"root_ids\":[\"1132\"],\"roots\":{\"1132\":\"5a684a58-54da-4364-ac07-102665a18197\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1132"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "# 사용할 툴들\n",
    "tools_to_show = 'hover, box_zoom, pan, save, reset, wheel_zoom'\n",
    "p = figure(plot_width=720, plot_height=580, tools=tools_to_show)\n",
    "\n",
    "source = ColumnDataSource(data={\n",
    "    'x': W2d[:, 0],\n",
    "    'y': W2d[:, 1],\n",
    "    'id': [i for i in range(W.shape[0])],\n",
    "    'document': [article for words, article in words_list],\n",
    "    'topic': [str(i) for i in topicIndex],  # 토픽 번호\n",
    "    'color': [Category20[K][i] for i in topicIndex]\n",
    "})\n",
    "p.circle(\n",
    "    'x', 'y',\n",
    "    source=source,\n",
    "    legend='topic',\n",
    "    color='color'\n",
    ")\n",
    "\n",
    "# interaction\n",
    "p.legend.location = \"top_left\"\n",
    "hover = p.select({'type': HoverTool})\n",
    "hover.tooltips = [(\"Topic\", \"@topic\"), ('id', '@id'), (\"Article\", \"@document\")]\n",
    "hover.mode = 'mouse'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
