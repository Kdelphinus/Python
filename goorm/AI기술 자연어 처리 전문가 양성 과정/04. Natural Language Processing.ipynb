{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Intro\n",
    "1. Bag-of-Words\n",
    "2. Topic Modeling\n",
    "3. Word Embedding\n",
    "4. RNNs with Attention\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.1 NLP란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP(Natural Language Processing)란 자연어를 컴퓨터와 같은 기계를 이용해서 묘사하도록 연구하는 인공지능 분야입니다. 이는 다시 NLU(Natural Language Understanding)과 NLG(Natural Language Generation)으로 나뉩니다. \n",
    "\n",
    "NLU는 문장이나 문단이 주어졌을 때, 이를 이해하고 질의응답이 가능하도록 만드는 분야입니다. NLG는 자동 번역 등 자연스럽게 말이 이어지도록 생성하는 과정입니다. 현재 NLU는 어느 정도 연구가 진행되었다고 여겨지고 NLG에 대한 연구가 더 활발하게 이루어지고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.2 NLP Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP를 사용하는 분야는 많이 있습니다. 이를 하나씩 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam detection, sentiment analysis 등 받은 자연어들로 분류하는 분야입니다. 스팸 메일을 분류하거나 평점을 가지고 긍부정을 나누는 등에 사용됩니다. \n",
    "\n",
    "![text_classification](_image/text_classification.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) QA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA(Question Answering)는 말 그대로 질문과 context(위키 등 웹페이지들)가 input으로 주어졌을 때, output으로 답을 내보내는 것입니다. 대표적으로 search engine 등이 있습니다. 밑에서 구글을 보면 질문에 대해 위키에 답을 진하게 표현한 것을 볼 수 있습니다.\n",
    "\n",
    "![QA](_image/QA.PNG)\n",
    "\n",
    "이때 답을 찾는 방법은 크게 세 가지가 있습니다.\n",
    "\n",
    "1. Context 내에서 뽑아냄(위 이미지가 그 예시)\n",
    "2. 직접 문장을 생성하여 출력\n",
    "3. 객관식(Multi-Choice)를 사용하여 출력\n",
    "\n",
    "2번 같은 경우는 context가 주어지지 않고 질문만 주어지는 common sense reasoning입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Macine Translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Translation은 기존 언어(source language)를 번역할 언어(target language)로 바꿔주는 것입니다. 현재 연구, 실무 분야에서 가장 활발하게 진행되고 있습니다. 파파고, 구글 번역 등이 그 예시입니다.\n",
    "\n",
    "1. pair 사용(같은 의미, 다른 언어의 쌍)\n",
    "2. unsupervised learning(pair가 없이 학습)\n",
    "\n",
    "학습 방법은 위에 두 가지 방법이 있습니다. pair는 직관적이지만 하나하나 학습 데이터를 만들기 어렵고 데이터가 너무 커집니다. 그래서 최근 비지도 학습을 이용한 학습이 새로운 방법으로 나오고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot은 자연어가 input으로 주어졌을 때, 대화가 이어지도록 output이 이어서 주어지는 프로그램입니다. 대화가 계속 이어지기 때문에 맥락을 잘 보는 것이 필요합니다. 그렇기에 직전 한 문장만 input으로 받지 않고 어느 정도 문단을 같이 가져옵니다. input을 template와 generation 중 어떤 것으로 받을지 선택할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Personal Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal Assistant는 빅스비, 시리와 같이 특정 요청을 수행하는 것들을 말합니다. 아직은 노래 요청, 날씨 등 제한적인 사용만 가능하지만 점차 개발되어지고 있습니다. 음성으로 작동하기 때문에 특정 음성(하이 빅스비, 시리야 등)을 들어야 음성 모드로 들어가게 만들어서 여러 소음에 작동하지 않도록 하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6) Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization은 말그대로 글들을 요약해주는 것입니다. 예를 들어 뉴스가 있다면 제목과 내용을 보고 요약해주는 것을 말합니다. \n",
    "\n",
    "![text_summarization](_image/text_summarization.PNG)\n",
    "\n",
    "요약하는 방식은 extractive와 generative로 나뉩니다. extractive는 주어진 내용들을 통해 요약을 해주는 반면 generative는 본문에 없는 말로도 요약할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0.3 NLP 연구 필드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 분야는 ACL, EMNLP, NAACL 등 유력한 컨퍼런스들이 있습니다. (AI는 급변하는 분야이기에 저널을 잘 보지 않습니다.) 이를 더욱 세분화하여 분야를 나눌 수 있습니다.\n",
    "\n",
    "#### **Low-level parsing**\n",
    "- Tokenization, stemming(과거형이나 미래형을 기본형으로 바꾸는 것)\n",
    "\n",
    "#### **Word and pharse level**\n",
    "- Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution $\\Rightarrow$ (언어학자가 만든 체계가 컴퓨터에게는 도움이 되지 않음이 확인되면서 연구가 사그라듬)\n",
    "- Semantic relation extraction\n",
    "\n",
    "#### **Sentence level**\n",
    "- Sentiment analysis, machine translation\n",
    "\n",
    "#### **Multi-sentence and paragraph level**\n",
    "- Entailment prediction, question answering, dialog systems, summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 text mining, information retrieval 등이 있습니다.(PDF 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding이란 단어들을 embedding이란 과정을 거쳐 컴퓨터가 이해할 수 있는 숫자로 바꾸는 과정을 말합니다. 이러면 단어들은 벡터가 되는데 이 벡터를 embedding vector라고 합니다.\n",
    "\n",
    "|구분|Bag-of_Words|언어 모델|분포 가정|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|내용|어떤 단어가 많이 쓰였는가|단어가 어떤 순서로 쓰였는가|어떤 단어가 같이 쓰였는가|\n",
    "|대표 통계량|TF-IDF|-|PMI|\n",
    "|대표 모델|Deep Averaging Network|ELMO, GPT|Word2Vec|\n",
    "\n",
    "단어를 임베딩하여 나오는 벡터들은 어떤 가정을 따르냐에 따라서 위와 같이 구분됩니다. 이를 더 자세히 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 백 오브 워즈 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백 오브 워즈는 저자의 의도가 단어 사용 여부나 그 빈도에서 드러난다고 보는 과정입니다. 그렇기에 단어의 순서는 고려대상이 아닙니다. 순서에 상관없이 하나의 가방에 단어들을 모두 넣고 빈도수를 확인하는 것입니다. 이때 대표적으로 TF-IDF가 사용됩니다. \n",
    "\n",
    "#### **TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
    "어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지고 반대의 경우 작아집니다. w는 단어, N은 문서의 개수, TF(Term Frequency)는 단락에서의 빈도, DF(Document Frequency)는 문서에서의 빈도입니다. \n",
    "\n",
    "$$TF - IDF(w) = TF(w) \\times log(\\frac{N}{DF(w)})$$\n",
    "\n",
    "그렇기에 문서 전체에서 빈도가 높은 조사들은 가중치가 줄어들고 특정 문장에서 빈번하게 나오는 단어들은 가중치가 증가합니다. 이에 대해 뒤에서 더 자세히 알아보겠습니다.\n",
    "\n",
    "\n",
    "#### **Deep Averaging Network(lyyer et al. 2015)**\n",
    "문장에 속한 단어의 임베딩의 평균을 구해 문장의 임베딩을 만드는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) 언어 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여하는 방법입니다. 백 오브 워즈와 달리 등장 순서에 영향을 받기에 '나는 밥을 먹었다'와 '나는 먹었다 밥을'을 다른 문장으로 해석합니다. \n",
    "\n",
    "$$P(w_i) \\Rightarrow P(w_i|w_{i-1}, w_{i-2}, \\cdots, w_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 분포 가정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 의미를 주변 문맥을 통해 유추하는 방법입니다. 가까운 단어들을 통해 의미를 유추합니다.\n",
    "\n",
    "#### **PMI(Pointwise Mutual Information)**\n",
    "두 단어 A, B가 얼마나 자주 같이 등장하는지 정보를 수치화하여 유추합니다.\n",
    "\n",
    "$$ PMI(A, B) = log \\frac{P(A, B)}{P(A) \\times P(B)}$$\n",
    "\n",
    "#### **Word2Vec**\n",
    "특정 단어 주변의 문맥, 즉 분포 정보를 함축하며 벡터로 만들어 사용하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Bag-of-Words Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 문법과 어순은 무시하지만 다중성을 유지하면서 단어의 가방에 단어들을 집어넣습니다. 그리고 이를 고유한 단어의 vocabulary를 만들거나 각 단어들을 one-hot vector로 만들어 사용하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes Classifier은 Bayes' theorem을 이용하여 간단한 분류를 하는 classifier입니다. \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\; (\\text{Bayes'\\, theorem})$$\n",
    "\n",
    "이제 이를 이용해 Naïve Bayes Classifier을 유추해보겠습니다. 각 document d는 class c를 가지고 있습니다. P(c|d)가 d가 c에 속할 확률입니다. Bayes' theorem을 적용하면 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "근데 여기서 분모는 항상 같기 때문에 분모를 제거해도 괜찮습니다. 그렇기에 아래와 같은 식이 나옵니다.\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c)$$\n",
    "\n",
    "$d$는 $words w_1, w_2, \\cdots, w_n$로 이루어져 있습니다. 그렇기에 위의 식은 다시 쓰면 다음과 같습니다.\n",
    "\n",
    "$$P(d|c)P(c) = P(w_1, w_2, \\cdots, w_n|c)P(c)$$\n",
    "\n",
    "이때 우리가 관심이 있는 것은 $P(c|d) = P(c|w_1, w_2, \\cdots, w_n)$이다. 이제 여기에 chain rule까지 적용하면 다음과 같이 나옵니다. 이때 확률에 관한 chain rule은 다음과 같습니다.\n",
    "\n",
    "$$\\begin{aligned} P(X_4, X_3, X_2, X_1) &= P(X_4|X_3, X_2, x_1) \\cdot P(X_3, X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2, X_1) \\\\\n",
    "&= P(X_4|X_3, X_2, X_1) \\cdot P(X_3|X_2, X_1) \\cdot P(X_2|X_1) \\cdot P(X_1) \\end{aligned}$$\n",
    "\n",
    "$$P(c|d) = P(d|c)P(c) = P(c) \\prod_{w_i \\in W} P(w_i|c)$$\n",
    "\n",
    "예시를 통해 알아보겠습니다. 밑에처럼 주어진 document와 word, class가 있습니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:skyblue\">?</span>|\n",
    "\n",
    "위와 같이 샘플이 주어져 있고 7번의 문장이 spam인지 아닌지 확인해야 합니다. 우리가 구한 식을 이용하여 식을 세워보겠습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) = P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})$$\n",
    "$$P(c_{Inbox}|d_7) = P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})$$\n",
    "\n",
    "만약 $P(c_{spam}|d_7) > P(c_{inbox}|d_7)$이면 스팸이고 $P(c_{spam}|d_7) < P(c_{inbox}|d_7)$이면 inbox로 분류될 것이다. 각 단어의 개수와 클래스 개수를 통해 결과를 보면 다음과 같습니다. \n",
    "\n",
    "![spam_result](_image/spam_result.PNG)\n",
    "\n",
    "이제 결과를 통해 종합적으로 계산하면 다음과 같습니다.\n",
    "\n",
    "$$P(c_{spam}|d_7) \n",
    "= P(c_{spam})P(w_{you}|c_{spam})P(w_{free}|c_{spam})P(w_{lottery}|c_{spam})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{3}{10} \\times \\frac{2}{10} = \\frac{6}{1000}$$\n",
    "$$P(c_{Inbox}|d_7) \n",
    "= P(c_{Inbox})P(w_{you}|c_{Inbox})P(w_{free}|c_{Inbox})P(w_{lottery}|c_{Inbox})\n",
    "= \\frac{1}{2} \\times \\frac{2}{10} \\times \\frac{2}{10} \\times \\frac{0}{10} = 0$$\n",
    "\n",
    "그러므로 7번 문장은 spam입니다.\n",
    "\n",
    "||No.|Document($d$)|Class($c$)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Training|1|me free lottery|Spam|\n",
    "||2|free get free you|Spam|\n",
    "||3|you free scholarship|Inbox|\n",
    "||4|free to contact me|Inbox|\n",
    "||5|you won award|Inbox|\n",
    "||6|you ticket loterry|Spam|\n",
    "|<span style=\"color:skyblue\">Test</span>|<span style=\"color:skyblue\">7</span>|<span style=\"color:skyblue\">you free loterry</span>|<span style=\"color:red\">Spam</span>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실습1. Naïve Bayes Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 필요한 라이브러리들을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# POS(Part of Speech) tagger\n",
    "from konlpy import tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data와 test data를 준비하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# training data. input text와 정답 label (긍정(1), 부정(0))으로 구성\n",
    "data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n",
    "                {'text': \"기대했던 것보단 별로였네요.\"},\n",
    "                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n",
    "                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n",
    "                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n",
    "                {'text': \"연기가 좀 별로였습니다.\"},\n",
    "                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n",
    "                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n",
    "                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n",
    "                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n",
    "                ]\n",
    "# test data\n",
    "data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n",
    "                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n",
    "                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n",
    "                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n",
    "                ]\n",
    "\n",
    "train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "test_labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 선언\n",
    "morph_analyzer = tag.Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization 함수 정의\n",
    "def tokenization(data, morph_analyzer):\n",
    "    \"\"\"tokenization \n",
    "\n",
    "    Args:\n",
    "        data (list): list of data examples.\n",
    "        morph_analyzer (konlpy.tag._kkma.Kkma): morphological analyzer.\n",
    "\n",
    "    Returns:\n",
    "        tokenized_data (list): list of tokenized data examples.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for example in tqdm(data):\n",
    "        tokens = morph_analyzer.morphs(example['text'])\n",
    "        tokenized_data.append(tokens)\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 15.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenization 함수를 이용한 데이터 tokenization\n",
    "tokenized_data = {}\n",
    "\n",
    "tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n",
    "tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n",
       " ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n",
       " ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n",
       " ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n",
       " ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n",
       " ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n",
       " ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n",
       " ['기념일',\n",
       "  '에',\n",
       "  '방문',\n",
       "  '하',\n",
       "  '었',\n",
       "  '는데',\n",
       "  '연기',\n",
       "  '도',\n",
       "  '연출',\n",
       "  '도',\n",
       "  '다',\n",
       "  '좋',\n",
       "  '았',\n",
       "  '습니다',\n",
       "  '.'],\n",
       " ['전반적',\n",
       "  '으로',\n",
       "  '지루',\n",
       "  '하',\n",
       "  '었',\n",
       "  '습니다',\n",
       "  '.',\n",
       "  '저',\n",
       "  '는',\n",
       "  '별',\n",
       "  '로',\n",
       "  '이',\n",
       "  '었',\n",
       "  '네요',\n",
       "  '.'],\n",
       " ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_data 확인\n",
    "tokenized_data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tokenization 결과를 이용해서 word to index dictionary를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<00:00, 55964.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n",
    "tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i]]\n",
    "unique_train_tokens = set(tokens)\n",
    "\n",
    "# Naïve Bayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n",
    "word2index = defaultdict() # key: word, value: index of word\n",
    "idx = 0\n",
    "for token in tqdm(unique_train_tokens):\n",
    "    word2index[token] = idx\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Naïve Bayes Classifier 모델 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, word2index, k=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word2index (dict): mapping a word to a pre-assigned index.\n",
    "            k (float, optional): constant for smoothing. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        self.k = k # for smoothing\n",
    "        self.word2index = word2index\n",
    "        self.priors = {} # Prior probability for each class, P(c)\n",
    "        self.likelihoods = {} # Likelihood for each token, P(d|c)\n",
    "    \n",
    "    def _set_priors(self, labels):\n",
    "        \"\"\"\n",
    "        Set prior probability for each class, P(c).\n",
    "        Count the number of each class and caculate P(c) for each class.\n",
    "        \"\"\"\n",
    "        # Count the number of each class\n",
    "        class_counts = defaultdict(int)\n",
    "        for label in tqdm(labels):\n",
    "            class_counts[label] += 1\n",
    "        \n",
    "        # For each class, calcuate P(c)\n",
    "        for label, count in class_counts.items():\n",
    "            self.priors[label] = class_counts[label] / len(labels)\n",
    "    \n",
    "    def _set_likelihoods(self, tokens, labels):\n",
    "        \"\"\"\n",
    "        Set likelihood for each token, P(d|c).\n",
    "        First, count the number of each class for each token.\n",
    "        Then, calculate P(d|c) for a given class and token.\n",
    "        \"\"\"\n",
    "        token_dists = {}\n",
    "        number_of_token_for_class = defaultdict(int)\n",
    "        \n",
    "        # Count the number of each class for each token\n",
    "        for i, label in enumerate(tqdm(labels)):\n",
    "            count = 0\n",
    "            for token in tokens[i]:\n",
    "                # 'token in self.word2index'부분은 안 들어가도 되지 않는가?\n",
    "                if token not in token_dists and token in self.word2index:\n",
    "                    token_dists[token] = {0:0, 1:0}\n",
    "                token_dists[token][label] += 1\n",
    "                count += 1\n",
    "            number_of_token_for_class[label] += count\n",
    "\n",
    "        for token, dist in tqdm(token_dists.items()):\n",
    "            if token not in self.likelihoods:\n",
    "                self.likelihoods[token] = {\n",
    "                    0: (token_dists[token][0] + self.k) / (number_of_token_for_class[0] + len(self.word2index) * self.k),\n",
    "                    1: (token_dists[token][1] + self.k) / (number_of_token_for_class[1] + len(self.word2index) * self.k),\n",
    "                }\n",
    "    \n",
    "    def train(self, input_tokens, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized train data.\n",
    "            labels (): train labels for each sentence/document.\n",
    "        \"\"\"\n",
    "        self._set_priors(labels)\n",
    "        self._set_likelihoods(input_tokens, labels)\n",
    "    \n",
    "    def inference(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tokens (list): list of tokenized test data.\n",
    "        \"\"\"\n",
    "        log_prob_0 = 0.0\n",
    "        log_prob_1 = 0.0\n",
    "        \n",
    "        for token in input_tokens:\n",
    "            if token in self.likelihoods:\n",
    "                log_prob_0 += math.log(self.likelihoods[token][0])\n",
    "                log_prob_1 += math.log(self.likelihoods[token][1])\n",
    "        \n",
    "        log_prob_0 += math.log(self.priors[0])\n",
    "        log_prob_1 += math.log(self.priors[1])\n",
    "        \n",
    "        if log_prob_0 >= log_prob_1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 학습 데이터에 대해 문장 분류 모델을 학습시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 56258.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 문장 분류 모델 선언 및 학습\n",
    "classifier = NaiveBayesClassifier(word2index)\n",
    "classifier.train(tokenized_data['train'], train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 test 데이터에 대해 정답값을 예측하고 Accuracy를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4164.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test data inference\n",
    "preds = []\n",
    "for test_tokens in tqdm(tokenized_data['test']):\n",
    "    pred = classifier.inference(test_tokens)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy 측정\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Bag-of-Words Encoding of Text Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번 챕터에서 본 Bag-of-Words를 다시 보겠습니다. \"John likes movies. Mary likes too.\"와 \"John also likes football.\"이란 두 문장이 주어졌을 때 각각 bag-of-words vector는 다음과 같습니다.\n",
    "\n",
    "![4-2-1](_image/4-2-1.PNG)\n",
    "\n",
    "각 단어를 사전으로 만들고 나타난 빈도수를 저장합니다. 그렇기에 행렬은 (키워드 개수) x (document 개수)의 형태로 나타납니다. 이 행렬을 term-document matrix(TDM)이라고 합니다. 순서 정보는 무시되는 단점이 있지만 많이 사용되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 topic은 가상의 document의 백 오브 워즈 벡터입니다. 그리고 백 오브 워즈 벡터의 값들을 정규화하면 합이 1이 되는 확률분포로 나타낼 수 있습니다. 그렇기에 토픽은 키워드들의 확률분포이자 키워드들의 가중치 조합이라고 할 수 있습니다. \n",
    "\n",
    "![4-2-2](_image/4-2-2.PNG)\n",
    "\n",
    "위 그림을 보면 맨 윗줄은 topic이고 밑에 있는 단어들은 그 topic에 속한 단어들입니다. topic의 제목은 프로그램이 자동으로 정해지며 군집에 속한 단어의 개수가 많으면 topic과 관련된 document라고 추측할 수 있습니다. 그리고 이를 바탕으로 document도 topic에 대해 군집을 만들 수 있습니다. \n",
    "\n",
    "밑의 그림은 topic modeling의 전반적 동작을 표현한 것입니다.\n",
    "\n",
    "<img src = \"https://iq.opengenus.org/content/images/2020/01/1_taTOiaCpd_CzGugx_PticQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Topic Modeling Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 동작하는 과정들을 상세히 보겠습니다.\n",
    "\n",
    "먼저 input document들을 bag-of-words vector들로 만듭니다. 이를 열벡터로 합쳐서 단어의 개수 x 문서의 개수 크기의 행렬 $A$를 만듭니다. 그리고 주어진 topic의 개수만큼 열벡터를 임의로 만듭니다. 이를 단어의 개수 x 토픽의 개수 크기의 행렬 $W$라고 하겠습니다. 이제 A를 잘 표현하도록 W에 곱할 행렬 H를 찾습니다. $H$는 토픽의 개수 x 문서의 개수 크기의 행렬일 것입니다. 이를 통해 H를 찾았다면 이제 H를 고정하고 W를 학습합니다. 이를 반복하며 최적의 W와 H를 찾는 것입니다. 이를 통해 행렬 W는 행렬 A가 가진 패턴들 중 빈도가 높은 패턴들로 학습되고 그 패턴들을 topic으로 가져가게 됩니다. \n",
    "\n",
    "이때 loss는 프로베니우스 놈으로 구합니다. \n",
    "\n",
    "$$\\lVert x \\rVert_F = (x_1^F + x_2^F + \\cdots + x_n^F)^{\\frac{1}{f}}$$\n",
    "\n",
    "벡터의 크기를 구할 때 자주 봤던 식입니다. 이를 통해 각 문서별 loss를 구하고 이것이 최소화되도록 H 안에 각 요소들을 바꿔줍니다. 이때 우리는 대체로 F = 2인 2놈을 자주 사용합니다. 이를 식으로 나타내면 다음과 같습니다. 이때 n은 단어의 개수입니다.\n",
    "\n",
    "$$\\underset{W, H \\geq 0}{arg \\; min} \\lVert A - WH \\rVert_2 = (A_1^2 - (W_1 H_1)^2 + \\cdots + A_n^2 - (W_nH_n)^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "이제 구해진 topic을 가지고 document를 분류합니다. 여러 topic들의 선형 결합으로 가장 잘 표현할 수 있는 document의 가중치를 찾고 가중치가 가장 높은 topic으로 분류합니다.\n",
    "\n",
    "밑의 그림은 이를 간단하게 표현한 것입니다.\n",
    "\n",
    "<img src = \"https://iq.opengenus.org/content/images/2020/01/1_2uj6t3gNv76SpHrWf5-z-A.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. 크롤링한 뉴스 데이터로 Topic Modeling하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 직접 크롤링한 뉴스 데이터에 대해서 topic modeling을 해보겠습니다. \n",
    "\n",
    "간단하게 전 과정을 살펴보면 먼저 네이버에서 뉴스 기사를 간단하게 크롤링합니다.  \n",
    "기본적인 전처리 이후, Term-Document Matrix를 만들고 이를 non-negative factorization을 이용해 행렬 분해 하여 topic modeling을 수행합니다.   \n",
    "\n",
    "그 후, t-distributed stochastic neighbor embedding(T-SNE) 기법을 통해 topic별 시각화를 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Crawiling News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링에 필요한 패키지 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import article\n",
    "from time import sleep, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 정적 페이지인 네이버 뉴스 신문 기사 웹페이지를 크롤링합니다. 정적 페이지와 HTML에 대해선 [자](https://ko.wikipedia.org/wiki/%EC%A0%95%EC%A0%81_%EC%9B%B9_%ED%8E%98%EC%9D%B4%EC%A7%80)[료](https://opentutorials.org/course/2039)들을 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news(query: str=None, crawl_num: int=1000, workers: int=4):\n",
    "    \"\"\"crawl_news 뉴스 기사 텍스트가 담긴 list를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str, optional): 검색어. Defaults to None.\n",
    "        crawl_num (int, optional): 수집할 뉴스 기사의 개수. Defaults to 1000.\n",
    "        workers (int, optional): multi-processing 시, 사용할 thread의 개수. Defaults to 4.\n",
    "    \"\"\"\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
    "    articleList = []\n",
    "    crawled_url = set()\n",
    "    keyboard_interrupt = False\n",
    "    t = time()\n",
    "    idx = 0\n",
    "    page = 1\n",
    "    \n",
    "    # 서버에 url 요청 결과를 선언\n",
    "    res = requests.get(url.format(query))\n",
    "    sleep(0.5)\n",
    "    # res를 parsing할 parser를 선언\n",
    "    bs = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    with Pool(workers) as p:\n",
    "        while idx < crawl_num:\n",
    "            table = bs.find('ul', {'class': 'list_news'})\n",
    "            li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
    "            area_list = [li.find('div', {'class':'news_area'}) for li in li_list]\n",
    "            a_list = [area.find('a', {'class':'news_tit'}) for area in area_list]\n",
    "            \n",
    "            for n in a_list[:min(len(a_list), crawl_num - idx)]:\n",
    "                articleList.append(n.get('title'))\n",
    "                idx += 1\n",
    "            page += 1\n",
    "            \n",
    "            pages = bs.find('div', {'class':'sc_page_inner'})\n",
    "            next_page_url = [p for p in pages.find_all('a') if p.text == str(page)][0].get('href')\n",
    "            \n",
    "            req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
    "            bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return articleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '구글'\n",
    "articleList = crawl_news(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"공정위 새해도 '플랫폼 갑질' 겨눈다…구글·카카오·쿠팡 사정권\",\n",
       " '[신간] 구글은 어떻게 디자인하는가',\n",
       " \"구글, 인터넷뉴스서비스사업자 등록할까…여야 '법안 추진' 논의\",\n",
       " '비트코인도 뚫는 ‘무한대 성능’… 구글 “2029년 상업용 출시”',\n",
       " \"구글·카카오 등 플랫폼기업 '갑질' 칼빼든다\",\n",
       " '[CES 2022] MS·구글·아마존·메타도 안 나온다… 韓 독무대 된 세계 최대 IT쇼',\n",
       " '거친 운전에 차멀미가...구글 완전 자율주행차 타보니 [김성민의 실밸 레이더]',\n",
       " \"구글, UDC 스마트폰 특허 출원.. 차기 '픽셀7' 탑재될까?\",\n",
       " '구글 트렌드로 본 경제 키워드…‘블루 이코노미’에 주목하라',\n",
       " \"'적중률 70%' 미라클레터 올해도 10대기술 예측…구글 AR안경·테슬라 로봇\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articleList[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tagger를 이용해 한글 명사와 알파벳만 추출해서 tdm을 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Okt 형태소 분석기 선언\n",
    "t = Okt()\n",
    "\n",
    "words_list_ = []\n",
    "vocab = Counter()\n",
    "tag_set = set(['Noun', 'Alpha'])\n",
    "stopwords = set(['글자'])\n",
    "\n",
    "for i, article in enumerate(articleList):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # tagger를 이용한 품사 태깅\n",
    "    words = t.pos(article, norm=True, stem=True)\n",
    "    \n",
    "    # 명사와 알파벳 tag를 가지며 철자 길이가 2이상이고 stopwords에 포함되지 않는 단어들로 리스트 생성\n",
    "    words = [w for w, t in words if t in tag_set and len(w) > 1 and w not in stopwords]\n",
    "    \n",
    "    vocab.update(words)\n",
    "    words_list_.append((words, article))\n",
    "    \n",
    "vocab = sorted([w for w, freq in vocab.most_common(10000)])\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "words_list = []\n",
    "for words, article in words_list_:\n",
    "    words = [w for w in words if w in word2id]\n",
    "    if len(words) > 10:\n",
    "        words_list.append((words, article))\n",
    "\n",
    "del words_list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Build document-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 document-term matrix를 만들어보겠습니다. 문서 개수 x 단어 개수의 형태를 가집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "dtm = np.zeros((len(words_list), len(vocab)), dtype=np.float32)\n",
    "for i, (words, article) in enumerate(words_list):\n",
    "    for word in words:\n",
    "        dtm[i, word2id[word]] += 1\n",
    "\n",
    "dtm = TfidfTransformer().fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 document-term matrix를 non-negative factorization(NMF)을 이용해 행렬 분해를 해보겠습니다. \n",
    "\n",
    "이때 NMF는 주어진 행렬 non-negative matrix X를 non-negative matrix W와 H로 행렬 분해하는 알고리즘입니다. 이어지는 코드를 통해 W와 H의 의미에 대해 파악하겠습니다. \n",
    "\n",
    "참고: [Non-negative Matrix Factorization](https://angeloyeo.github.io/2020/10/15/NMF.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-negative Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "K = 5\n",
    "nmf = NMF(n_components=K, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn의 NMF를 이용해 W와 H matrix를 구했습니다. \n",
    "\n",
    "W는 document length x K, H는 K x term length의 차원을 갖고 있습니다.  \n",
    "W 하나의 row는 각각의 feature에 얼만큼의 가중치를 줄 지에 대한 weight입니다.  \n",
    "H 하나의 row는 하나의 feature를 나타냅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "W = nmf.fit_transform(dtm)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 하나의 Topic(H의 n번째 row)에 접근해서 해당 topic에 대해 값이 가장 높은 20개의 단어를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic\n",
      "결제 인앱 강제 금지법 정책 꼼수 애플 시간 겉도 제출 추가 요구 자료 허용 계획 이행 연내 구글 방통위 방지법 \n",
      "1th topic\n",
      "사전 레볼루션 사이트 넷마블 실시 공식 나이 세븐 등록 신작 대작 구글 게임 급등 최소 NFT 론칭 내년 오리진 이상 \n",
      "2th topic\n",
      "삼성 미국 전자 픽셀 모뎀 기관 조사 탑재 처음 마이크로소프트 부회장 영진 확인 동맹 이재용 아마존 시장 구글 사운드 공개 \n",
      "3th topic\n",
      "애플 규제 경쟁 위원장 저승사자 EU 규칙 촉구 플랫폼 시행 구글 공룡 방지 강화 엄격 적용 테크 넷플릭스 방통위 시급 \n",
      "4th topic\n",
      "검색 도전 시장 아마존 클라우드 비즈 확대 과감 위해 글로벌 투자 vs 전쟁 점화 사용자 국내 네이버 테크 구글 메타 \n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    print(f\"{k}th topic\")\n",
    "    for index in H[k].argsort()[::-1][:20]:\n",
    "        print(vocab[index], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 W에서 하나의 topic (W의 n번째 column)에 접근해서 해당 topic에 대해 값이 가장 높은 3개의 뉴스 기사 제목을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===0th topic===\n",
      "겉도는 인앱결제 강제금지법… 구글은 결제정책 꼼수, 애플은 시간끌기\n",
      "겉도는 인앱결제 강제금지법… 구글은 결제정책 꼼수, 애플은 시간끌기\n",
      "구글은 결제정책 꼼수, 애플은 모르쇠… 힘못쓰는 갑질방지법 [인앱결제강제 금지법 시행 100일]\n",
      "\n",
      "===1th topic===\n",
      "넷마블, 기대신작 세븐나이츠 레볼루션...'구글·공식 사이트 사전등록 실시'\n",
      "넷마블, 기대신작 '세븐나이츠 레볼루션' 구글/공식 사이트 사전등록 실시\n",
      "넷마블, 기대작 세븐나이츠 레볼루션 구글·공식 사이트 사전등록 실시\n",
      "\n",
      "===2th topic===\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "\n",
      "===3th topic===\n",
      "'구글‧애플 저승사자' EU경쟁위원장, 빅테크 규제 위한 규칙 시행 촉구\n",
      "'구글‧애플 저승사자' EU경쟁위원장, 빅테크 규제 위한 규칙 시행 촉구\n",
      "구글·애플·넷플릭스 ‘플랫폼 공룡’ 갑질 방지 강화… 방통위, 규제 엄격 적용\n",
      "\n",
      "===4th topic===\n",
      "구글 vs 네이버… 빅테크 ‘검색전쟁’ 재점화 국내시장 1위에 ‘사용자 친화 검색’으로 도전장\n",
      "구글 vs 네이버… 빅테크 ‘검색전쟁’ 재점화 국내시장 1위에 ‘사용자 친화 검색’으로 도전장\n",
      "[글로벌 비즈] 구글 클라우드, 시장 확대 위해 과감한 투자…아마존에 도전장\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    print(f\"==={k}th topic===\")\n",
    "    for index in W[:, k].argsort()[::-1][:3]:\n",
    "        print(words_list[index][1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번째 topic에 대해 가장 높은 가중치를 갖는 제목 5개를 출력하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "이재용 삼성전자 부회장, 미국에서 '뉴삼성' 동맹 확인..마이크로소프트·아마존·구글 경영진 잇따라 만나\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "조사기관 “구글 픽셀6에 삼성전자 5G모뎀 탑재, 미국시장에서 처음\"\n",
      "[더벨][뉴삼성 차세대 리더십]소프트웨어 경쟁력 높인다…구글·MS출신 발탁승진\n"
     ]
    }
   ],
   "source": [
    "for index in W[:, 2].argsort()[::-1][:5]:\n",
    "    print(words_list[index][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 t-SNE를 이용해 topic별 시각화를 진행하겠습니다. \n",
    "\n",
    "t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 벡터를 데이터간 구조적 특징을 유지한 상태로 저차원(2~3차원) 벡터로 축소하는 방법 중 하나입니다. 주로 고차원 데이터의 시각화를 위해 사용됩니다.\n",
    "\n",
    "참고: [lovit: t-SNE](https://lovit.github.io/nlp/representation/2018/09/28/tsne/#:~:text=t%2DSNE%20%EB%8A%94%20%EA%B3%A0%EC%B0%A8%EC%9B%90%EC%9D%98,%EC%9D%98%20%EC%A7%80%EB%8F%84%EB%A1%9C%20%ED%91%9C%ED%98%84%ED%95%A9%EB%8B%88%EB%8B%A4.)\n",
    "\n",
    "참고: [ratsgo: t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 63 nearest neighbors...\n",
      "[t-SNE] Indexed 64 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 64 samples in 0.003s...\n",
      "[t-SNE] Computed conditional probabilities for sample 64 / 64\n",
      "[t-SNE] Mean sigma: 0.107609\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.561291\n",
      "[t-SNE] KL divergence after 1000 iterations: -0.062155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# n_components = 차원 수\n",
    "tsne = TSNE(n_components=2, init='pca', verbose=1)\n",
    "\n",
    "# W matrix에 대해 t-sne를 수행합니다.\n",
    "W2d = tsne.fit_transform(W)\n",
    "\n",
    "# 각 뉴스 기사 제목마다 가중치가 가장 높은 topic을 저장합니다.\n",
    "topicIndex = [v.argmax() for v in W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1131\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  const JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1131\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1131\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"5a684a58-54da-4364-ac07-102665a18197\" data-root-id=\"1132\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n    \n  const docs_json = {\"b4682a68-fa0e-4e7d-8772-58b06cbc8579\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1141\"}],\"center\":[{\"id\":\"1144\"},{\"id\":\"1148\"},{\"id\":\"1181\"}],\"height\":580,\"left\":[{\"id\":\"1145\"}],\"renderers\":[{\"id\":\"1168\"}],\"title\":{\"id\":\"1170\"},\"toolbar\":{\"id\":\"1156\"},\"width\":720,\"x_range\":{\"id\":\"1133\"},\"x_scale\":{\"id\":\"1137\"},\"y_range\":{\"id\":\"1135\"},\"y_scale\":{\"id\":\"1139\"}},\"id\":\"1132\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"label\":{\"field\":\"topic\"},\"renderers\":[{\"id\":\"1168\"}]},\"id\":\"1182\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"1163\"}},\"id\":\"1169\",\"type\":\"CDSView\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1173\"},\"group\":null,\"major_label_policy\":{\"id\":\"1174\"},\"ticker\":{\"id\":\"1146\"}},\"id\":\"1145\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data\":{\"color\":[\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#1f77b4\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ffbb78\",\"#ffbb78\",\"#aec7e8\",\"#aec7e8\",\"#ffbb78\",\"#2ca02c\",\"#aec7e8\",\"#ffbb78\",\"#2ca02c\",\"#aec7e8\",\"#ff7f0e\",\"#2ca02c\",\"#ffbb78\",\"#aec7e8\",\"#ffbb78\",\"#aec7e8\",\"#ffbb78\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#2ca02c\",\"#2ca02c\",\"#ff7f0e\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\"],\"document\":[\"'\\uc801\\uc911\\ub960 70%' \\ubbf8\\ub77c\\ud074\\ub808\\ud130 \\uc62c\\ud574\\ub3c4 10\\ub300\\uae30\\uc220 \\uc608\\uce21\\u2026\\uad6c\\uae00 AR\\uc548\\uacbd\\u00b7\\ud14c\\uc2ac\\ub77c \\ub85c\\ubd07\",\"\\\"\\uc62c\\ud574 \\uba54\\ud0c0\\ubc84\\uc2a4 \\uacbd\\uc7c1 \\ubcf8\\uaca9\\ud654\\\"...\\uba54\\ud0c0\\uac00 \\uc120\\ub450, \\uc560\\ud50c\\u00b7MS\\u00b7\\uad6c\\uae00\\uc774 \\ucd94\\uaca9\",\"\\uacf5\\uc815\\uc704\\uc6d0\\uc7a5 \\\"\\uad6c\\uae00 \\uc571\\ub9c8\\ucf13\\uc2dc\\uc7a5 \\uacbd\\uc7c1\\uc81c\\ud55c\\ud589\\uc704 \\uc804\\uc6d0\\ud68c\\uc758 \\uc2ec\\uc758 \\uc608\\uc815\\\"\",\"\\ud2f0\\uc564\\ucf00\\uc774\\ud329\\ud1a0\\ub9ac '\\uac80\\uc740\\uc655\\uad00:\\uba54\\uae30\\uc655\\uc758 \\ubd84\\ub178' \\uad6c\\uae00 \\ud50c\\ub808\\uc774 \\uc778\\uae30 \\uc21c\\uc704 1\\uc704 \\ub2ec\\uc131\",\"[\\uae00\\ub85c\\ubc8c \\ube44\\uc988] \\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc, \\uc2dc\\uc7a5 \\ud655\\ub300 \\uc704\\ud574 \\uacfc\\uac10\\ud55c \\ud22c\\uc790\\u2026\\uc544\\ub9c8\\uc874\\uc5d0 \\ub3c4\\uc804\\uc7a5\",\"\\uc560\\ud50c, \\uc5f0\\ub0b4 \\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c\\uae08\\uc9c0\\ubc95 \\uc774\\ud589\\uacc4\\ud68d \\uc81c\\ucd9c\\ud560\\uae4c?\\u2026\\ubc29\\ud1b5\\uc704, \\uad6c\\uae00\\uc5d0\\ub3c4 '\\uc81c3\\uc790 \\uacb0\\uc81c \\ud5c8\\uc6a9' \\ucd94\\uac00\\uc790\\ub8cc \\uc694\\uad6c\",\"\\uad6c\\uae00\\u00b7\\uc560\\ud50c\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4 \\u2018\\ud50c\\ub7ab\\ud3fc \\uacf5\\ub8e1\\u2019 \\uac11\\uc9c8 \\ubc29\\uc9c0 \\uac15\\ud654\\u2026 \\ubc29\\ud1b5\\uc704, \\uaddc\\uc81c \\uc5c4\\uaca9 \\uc801\\uc6a9\",\"[\\uae00\\ub85c\\ubc8c \\ube44\\uc988] \\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc, \\uc2dc\\uc7a5 \\ud655\\ub300 \\uc704\\ud574 \\uacfc\\uac10\\ud55c \\ud22c\\uc790\\u2026\\uc544\\ub9c8\\uc874\\uc5d0 \\ub3c4\\uc804\\uc7a5\",\"\\uc560\\ud50c, \\uc5f0\\ub0b4 \\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c\\uae08\\uc9c0\\ubc95 \\uc774\\ud589\\uacc4\\ud68d \\uc81c\\ucd9c\\ud560\\uae4c?\\u2026\\ubc29\\ud1b5\\uc704, \\uad6c\\uae00\\uc5d0\\ub3c4 '\\uc81c3\\uc790 \\uacb0\\uc81c \\ud5c8\\uc6a9' \\ucd94\\uac00\\uc790\\ub8cc \\uc694\\uad6c\",\"\\uad6c\\uae00\\u00b7\\uc560\\ud50c\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4 \\u2018\\ud50c\\ub7ab\\ud3fc \\uacf5\\ub8e1\\u2019 \\uac11\\uc9c8 \\ubc29\\uc9c0 \\uac15\\ud654\\u2026 \\ubc29\\ud1b5\\uc704, \\uaddc\\uc81c \\uc5c4\\uaca9 \\uc801\\uc6a9\",\"\\uad6c\\uae00 \\uacf5\\uc778 \\uad50\\uc721\\uc804\\ubb38\\uac00 \\uae40\\ub3d9\\uc6d0 \\ub300\\ud45c\\uc758 'Google \\ud65c\\uc6a9\\ubc95' \\ub124\\uc774\\ubc84 \\uc5d1\\uc2a4\\ud37c\\ud2b8 \\uc624\\ud508\",\"\\\"\\uc720\\ud29c\\ube0c\\ub3c4 \\ud1a0\\uc2a4\\ud398\\uc774\\ub85c\\\"\\u2026 \\ud1a0\\uc2a4\\ud398\\uc774\\uba3c\\uce20, \\uad6c\\uae00 \\uc81c\\ud734\\ub85c \\uac04\\ud3b8\\uacb0\\uc81c \\ud655\\uc7a5\",\"\\uc624\\ubbf8\\ud06c\\ub860 \\ud655\\uc0b0\\uc5d0 CES \\ube68\\uac04\\ubd88\\u2026\\ucd5c\\ud0dc\\uc6d0\\u00b7\\uc815\\uc758\\uc120 \\ucd9c\\uc7a5 \\uc7ac\\uac80\\ud1a0, \\uad6c\\uae00\\u00b7\\uba54\\ud0c0\\u00b7MS \\ubd88\\ucc38\",\"LG CNS, \\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988\\uc11c \\ub9ac\\ub4dc \\uc0dd\\uc131\\uacfc \\ubca0\\uc2a4\\ud2b8\\ud300 2\\uac1c\\ubd80\\ubb38 \\uc218\\uc0c1\",\"\\ubbf8\\ub974\\uc758 \\uc804\\uc1242: MOM, \\uad6c\\uae00 \\ub9e4\\ucd9c \\uc21c\\uc704 \\uc0c1\\uc2b9\\uc138, \\uc0c1\\uc704\\uad8c \\uc9c4\\uc785 \\ubc1c\\ud310 \\ub9c8\\ub828\",\"LG CNS, \\u2018\\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988\\u2019 3\\ud68c \\uc5f0\\uc18d \\uc218\\uc0c1\\u2026\\uc720\\uc77c 2\\uac1c\\ubd80\\ubb38 \\uc120\\uc815\",\"\\uc544\\ub9c8\\uc874. \\uba54\\ud0c0. \\ud2b8\\uc704\\ud130 \\uc774\\uc5b4 GM. \\uad6c\\uae00 \\uc6e8\\uc774\\ubaa8\\ub3c4 \\uc624\\ubbf8\\ud06c\\ub860 \\ud655\\uc0b0\\uc73c\\ub85c 'CES 2022' \\ucc38\\uac00 \\ucde8\\uc18c\",\"\\ud574\\ud53c\\uba38\\ub2c8, \\uad6c\\uae00\\uae30\\ud504\\ud2b8\\ucf54\\ub4dc \\uad6c\\ub9e4\\uace0\\uac1d \\ub300\\uc0c1 \\ub9ac\\ub2c8\\uc9c0W \\uc544\\uc774\\ud15c \\uc774\\ubca4\\ud2b8 \\uc9c4\\ud589\",\"\\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\ubaa8\\ub974\\uc1e0\\u2026 \\ud798\\ubabb\\uc4f0\\ub294 \\uac11\\uc9c8\\ubc29\\uc9c0\\ubc95 [\\uc778\\uc571\\uacb0\\uc81c\\uac15\\uc81c \\uae08\\uc9c0\\ubc95 \\uc2dc\\ud589 100\\uc77c]\",\"\\ud1a0\\uc2a4\\ud398\\uc774\\uba3c\\uce20 \\uad6c\\uae00\\uacfc \\ud1a0\\uc2a4\\ud398\\uc774 \\uc81c\\ud734, \\uae40\\ubbfc\\ud45c \\\"\\uac04\\ud3b8\\ud55c \\uacb0\\uc81c\\uacbd\\ud5d8 \\uc9c0\\uc6d0\\\"\",\"\\uac89\\ub3c4\\ub294 \\uc778\\uc571\\uacb0\\uc81c \\uac15\\uc81c\\uae08\\uc9c0\\ubc95\\u2026 \\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\uc2dc\\uac04\\ub04c\\uae30\",\"\\uac89\\ub3c4\\ub294 \\uc778\\uc571\\uacb0\\uc81c \\uac15\\uc81c\\uae08\\uc9c0\\ubc95\\u2026 \\uad6c\\uae00\\uc740 \\uacb0\\uc81c\\uc815\\ucc45 \\uaf3c\\uc218, \\uc560\\ud50c\\uc740 \\uc2dc\\uac04\\ub04c\\uae30\",\"[\\ud2b9\\uc9d5\\uc8fc] FSN, '\\ud2f1\\ud1a1' \\uad6c\\uae00 \\ub204\\ub974\\uace0 \\uc804\\uc138\\uacc41\\uc704 \\uae30\\ub85d\\u2026 \\uc804\\uc138\\uacc4 \\ud2f1\\ud1a1 \\uad11\\uace0\\ub300\\ud589 \\uad8c\\ud55c \\ubd80\\uac01\",\"[\\uc9d1\\uc911\\ucde8\\uc7acM] MS \\uad6c\\uae00 A\\ud559\\uc810, \\ub124\\uc774\\ubc84 \\uce74\\uce74\\uc624 F\\ud559\\uc810\\u2025IT\\uae30\\uc5c5\\ub4e4\\uc758 \\ud0c4\\uc18c \\uc131\\uc801\\ud45c\",\"[\\uae08\\uc77c \\uc0b0\\uc5c5\\uacc4 \\uc8fc\\uc694\\uae30\\uc0ac] \\\"\\uc624\\ubbf8\\ud06c\\ub860 \\ubcc0\\uc774 \\ud655\\uc0b0\\\" \\uad6c\\uae00\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uba54\\ud0c0\\u00b7\\ud2f1\\ud1a1 CES \\ubd88\\ucc38\\u2026 \\\"8K \\uc0dd\\ud0dc\\uacc4 \\ud655\\uc7a5 \\uac00\\uc18d\\ud654\\\" \\uc544\\ub9c8\\uc874, '8K \\ud611\\ud68c' \\ud569\\ub958 \\u5916\",\"[\\ub274\\uc2a4\\ud574\\uc124]\\uc560\\ud50c\\u00b7\\uad6c\\uae00 \\uc790\\uccb4\\ub4f1\\uae09\\ubd84\\ub958 \\ub0a8\\uc6a9...\\uae30\\uc900 \\uc815\\ube44 \\uc2dc\\uae09\",\"[\\ub274\\uc2a4\\ud574\\uc124]\\uc560\\ud50c\\u00b7\\uad6c\\uae00 \\uc790\\uccb4\\ub4f1\\uae09\\ubd84\\ub958 \\ub0a8\\uc6a9...\\uae30\\uc900 \\uc815\\ube44 \\uc2dc\\uae09\",\"LG\\uc804\\uc790, 2022\\ub144\\ud615 \\uc0ac\\uc6b4\\ub4dc \\ubc14 \\uacf5\\uac1c...\\uad6c\\uae00\\u00b7\\uc544\\ub9c8\\uc874, \\uc778\\uacf5\\uc9c0\\ub2a5 \\uc2a4\\ud53c\\ucee4 \\uc5f0\\ub3d9 \\ud1b5\\ud574 \\uc74c\\uc131\\ub9cc\\uc73c\\ub85c \\uc0ac\\uc6b4\\ub4dc \\ubc14 \\uc870\\uc791\",\"[AICON \\uad11\\uc8fc 2021] \\uad6c\\uae00 \\ucf54\\ub9ac\\uc544 \\uae40\\ud0dc\\uc6d0 \\uc804\\ubb34 \\\"\\ub514\\uc9c0\\ud138 \\uae30\\uc220\\uc740 \\uc138\\uc0c1\\uc758 \\ubb38\\uc81c\\ub97c \\ud574\\uacb0\\ud55c\\ub2e4\\\"\",\"\\uc120\\uc6b0\\uc724 \\uc640\\uadf8 \\ub300\\ud45c \\\"\\uad6c\\uae00 \\ud611\\uc5c5 \\uccab \\uad6d\\ub0b4 OTA\\u2026\\uc678\\uad6d\\uc778\\ub3c4 \\uc774\\uc81c \\uc6b0\\ub9ac \\uace0\\uac1d\\\" [\\uc778\\ud130\\ubdf0]\",\"\\uad11\\uc9c4\\uc708\\ud14d, 3D\\uc0ac\\uc6b4\\ub4dc \\ucd2c\\uc601 \\uc2e0\\uae30\\uc220 \\uac1c\\ubc1c...\\uad6c\\uae00 \\ub4f1 \\uc2a4\\ub9c8\\ud2b8\\ub514\\ubc14\\uc774\\uc2a4 \\uacb0\\ud569 1\\ucc28 \\uc0c1\\uc6a9\\ubaa8\\ub378 \\uc591\\uc0b0 \\ubaa9\\ud45c\",\"'\\uad6c\\uae00\\u00b7\\ub137\\ud50c\\ub9ad\\uc2a4' \\ub300\\uc0c1 \\ub9dd \\uc548\\uc815\\uc131 \\ud655\\ubcf4 \\uac00\\uc774\\ub4dc\\ub77c\\uc778 \\ub9c8\\ub828\\u2026 \\uc778\\ud130\\ub137 \\ud68c\\uc120 \\uc6a9\\ub7c9 \\ud655\\ubcf4 \\uc758\\ubb34\\ud654\",\"\\uad6c\\uae00 vs \\ub124\\uc774\\ubc84\\u2026 \\ube45\\ud14c\\ud06c \\u2018\\uac80\\uc0c9\\uc804\\uc7c1\\u2019 \\uc7ac\\uc810\\ud654 \\uad6d\\ub0b4\\uc2dc\\uc7a5 1\\uc704\\uc5d0 \\u2018\\uc0ac\\uc6a9\\uc790 \\uce5c\\ud654 \\uac80\\uc0c9\\u2019\\uc73c\\ub85c \\ub3c4\\uc804\\uc7a5\",\"\\uad6c\\uae00 vs \\ub124\\uc774\\ubc84\\u2026 \\ube45\\ud14c\\ud06c \\u2018\\uac80\\uc0c9\\uc804\\uc7c1\\u2019 \\uc7ac\\uc810\\ud654 \\uad6d\\ub0b4\\uc2dc\\uc7a5 1\\uc704\\uc5d0 \\u2018\\uc0ac\\uc6a9\\uc790 \\uce5c\\ud654 \\uac80\\uc0c9\\u2019\\uc73c\\ub85c \\ub3c4\\uc804\\uc7a5\",\"[\\uc704\\ud074\\ub9ac\\ub9ac\\ud3ec\\ud2b8 60\\ud638] '\\uc62c\\ud574 \\ud55c\\uad6d\\uc778\\uc774 \\uad6c\\uae00\\uc11c \\uac00\\uc7a5 \\ub9ce\\uc774 \\ucc3e\\uc740 \\uac80\\uc0c9\\uc5b4' 3\\uc704 \\uc624\\uc9d5\\uc5b4\\uac8c\\uc784, 2\\uc704 \\ubc31\\uc2e0\\uc608\\uc57d, 1\\uc704\\ub294\\u2026\",\"[\\uc704\\ud074\\ub9ac\\ub9ac\\ud3ec\\ud2b8 60\\ud638] '\\uc62c\\ud574 \\ud55c\\uad6d\\uc778\\uc774 \\uad6c\\uae00\\uc11c \\uac00\\uc7a5 \\ub9ce\\uc774 \\ucc3e\\uc740 \\uac80\\uc0c9\\uc5b4' 3\\uc704 \\uc624\\uc9d5\\uc5b4\\uac8c\\uc784, 2\\uc704 \\ubc31\\uc2e0\\uc608\\uc57d, 1\\uc704\\ub294\\u2026\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc791 \\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158 \\uad6c\\uae00\\u00b7\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc\",\"\\uc5d4\\ub3cc\\ud540\\ucee4\\ub125\\ud2b8 \\uc2e0\\uc791 '\\uc2a4\\ud0d1 \\ubc14\\uc774: \\uc0b0\\ud0c0 \\ub808\\uc774\\uc2a4' \\uad6c\\uae00 \\ud50c\\ub808\\uc774 \\uc2a4\\ud1a0\\uc5b4 \\ucd9c\\uc2dc\",\"\\ubbf8, \\uad6c\\uae00\\u00b7\\uba54\\ud0c0 '\\ud0dc\\ud3c9\\uc591 \\uad11\\ucf00\\uc774\\ube14' \\uc0ac\\uc6a9\\uc2b9\\uc778 \\uad8c\\uace0\\u2026\\\"\\uc548\\ubcf4 \\uc6b0\\ub824\\ub85c \\ud64d\\ucf69 \\uad6c\\uac04\\uc740 \\uc81c\\uc678\\\"\",\"\\uc2e4\\uc801 \\uc804\\ub9dd \\uc5c7\\uac08\\ub9ac\\ub294 \\u7f8e \\ube45\\ud14c\\ud06c, MS\\u00b7\\uad6c\\uae00\\u00b7\\uba54\\ud0c0 \\u2018\\ub9d1\\uc74c\\u2019 vs \\uc560\\ud50c\\u00b7\\uc544\\ub9c8\\uc874 \\u2018\\uae00\\uc384\\u2019\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc2e0\\uc791 \\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158...'\\uad6c\\uae00\\u00b7\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc'\",\"\\uc544\\uc774\\uc2a4\\ubc84\\ub4dc \\uac8c\\uc784\\uc988 '\\ubbf8\\ub974\\uc758 \\uc804\\uc1242: MOM', \\uad6c\\uae00 \\ubb34\\ub8cc \\uc778\\uae30 \\uc21c\\uc704 2\\uc704 \\ub2ec\\uc131\",\"\\ud55c\\uad6d\\uacf5\\uc608\\ub514\\uc790\\uc778\\ubb38\\ud654\\uc9c4\\ud765\\uc6d0, \\uad6c\\uae00 \\uc544\\ud2b8 \\uc564 \\uceec\\ucc98\\uc640 \\ud611\\uc5c5...\\ud55c\\uad6d\\uc758 \\uacf5\\uc608\\ubb38\\ud654 \\uc120\\ubcf4\\uc5ec\",\"\\ub137\\ub9c8\\ube14, \\uae30\\ub300\\uc2e0\\uc791 '\\uc138\\ube10\\ub098\\uc774\\uce20 \\ub808\\ubcfc\\ub8e8\\uc158' \\uad6c\\uae00/\\uacf5\\uc2dd \\uc0ac\\uc774\\ud2b8 \\uc0ac\\uc804\\ub4f1\\ub85d \\uc2e4\\uc2dc\",\"[\\ub354\\ubca8][\\ub274\\uc0bc\\uc131 \\ucc28\\uc138\\ub300 \\ub9ac\\ub354\\uc2ed]\\uc18c\\ud504\\ud2b8\\uc6e8\\uc5b4 \\uacbd\\uc7c1\\ub825 \\ub192\\uc778\\ub2e4\\u2026\\uad6c\\uae00\\u00b7MS\\ucd9c\\uc2e0 \\ubc1c\\ud0c1\\uc2b9\\uc9c4\",\"\\ud3ec\\uc2a4\\ud14d, \\uc2a4\\ud3ec\\uce20 \\uc778\\uacf5\\uc9c0\\ub2a5(AI) \\ubd84\\uc57c...\\uad6c\\uae00 \\ud074\\ub77c\\uc6b0\\ub4dc \\uc544\\u2027\\ud0dc \\uacf5\\uacf5\\ubd80\\ubb38 \\ud30c\\ud2b8\\ub108 \\uc120\\uc815\",\"[\\uc810\\uc2ec \\ube0c\\ub9ac\\ud551]\\uc554\\ud638\\ud654\\ud3d0 \\ud558\\ub77d\\uc138\\u00b7\\u00b7\\u00b7\\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uad6c\\uae00 \\uc804 \\ud68c\\uc7a5, \\uccb4\\uc778\\ub9c1\\ud06c \\ud504\\ub85c\\uc81d\\ud2b8 \\ud569\\ub958\",\"\\ub124\\uc624\\ub9ac\\uc9c4, \\uc2e0\\uc791 \\uac8c\\uc784 \\uad6c\\uae00 \\ub4f1 3\\ub300 \\uc571 \\ub9c8\\ucf13\\ub860\\uce6d\\u2026\\u201c\\ub0b4\\ub144 \\ucd5c\\uc18c 2\\uac1c \\uc774\\uc0c1 \\uc2e0\\uc791\\uacfc NFT\\uac8c\\uc784 \\uacf5\\uac1c \\uc608\\uc815\\u201d\",\"[AICON \\uad11\\uc8fc 2021] \\uad6c\\uae00 \\ucf54\\ub9ac\\uc544 \\uae40\\ud0dc\\uc6d0 \\uc804\\ubb34 \\uae30\\uc870 \\uac15\\uc5f0 \\ub098\\uc11c... \\\"\\ub514\\uc9c0\\ud138 \\uae30\\uc220 \\ud1b5\\ud574 \\ub2f9\\ub300\\uc758 \\uad00\\uc2ec\\uc0ac\\uc640 \\ubcc0\\ud654 \\ud30c\\uc545\\ud560 \\uc218 \\uc788\\uc5b4!\\\"\",\"\\uc601\\uc6c5 \\uc218\\uc9d1 \\ud30c\\ud2f0 \\ubc29\\uce58\\ud615 RPG '\\ub9d0\\ub2e8 \\ubcd1\\uc0ac\\uc5d0\\uc11c \\uad70\\uc8fc\\uae4c\\uc9c0 - \\uad70\\uc8fc \\ud0a4\\uc6b0\\uae30' \\uad6c\\uae00\\ud50c\\ub808\\uc774 \\ucd9c\\uc2dc\",\"[\\ud2b9\\uc9d5\\uc8fc]NHN\\ubc85\\uc2a4, \\uad6c\\uae00\\u00b7\\uc560\\ud50c\\uc5d0 \\uc2f8\\uc774\\uc6d4\\ub4dc \\uc571 \\uc2e0\\uccad\\u2026\\ub124\\uc774\\ubc84\\u00b7\\uce74\\uce74\\uc624 \\uc787\\ub294 K-\\ud50c\\ub7ab\\ud3fc \\ucd9c\\uc0ac\\ud45c\",\"\\uad6c\\uae00, 12\\uc6d4 '\\ud53d\\uc140 \\ud53c\\ucc98 \\ub4dc\\ub86d' \\uc5c5\\ub370\\uc774\\ud2b8 \\uacf5\\uac1c.. '\\ud53d\\uc1406' \\uc9c0\\ubb38 \\uc13c\\uc11c \\uc131\\ub2a5 \\uac1c\\uc120\",\"\\uc870\\uc0ac\\uae30\\uad00 \\u201c\\uad6c\\uae00 \\ud53d\\uc1406\\uc5d0 \\uc0bc\\uc131\\uc804\\uc790 5G\\ubaa8\\ub380 \\ud0d1\\uc7ac, \\ubbf8\\uad6d\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucc98\\uc74c\\\"\",\"\\uc870\\uc0ac\\uae30\\uad00 \\u201c\\uad6c\\uae00 \\ud53d\\uc1406\\uc5d0 \\uc0bc\\uc131\\uc804\\uc790 5G\\ubaa8\\ub380 \\ud0d1\\uc7ac, \\ubbf8\\uad6d\\uc2dc\\uc7a5\\uc5d0\\uc11c \\ucc98\\uc74c\\\"\",\"[\\uae09\\ub4f1\\uc655\\uc758 '\\uae09\\ub4f1\\ub9e5 \\ud544\\uc0b4\\uae30'] NHN\\ubc85\\uc2a4, \\uad6c\\uae00\\u00b7\\uc560\\ud50c \\uc571 \\ub4f1\\ub85d \\uc2ec\\uc0ac \\uc2e0\\uccad \\uc18c\\uc2dd\\uc5d0 \\uc0c1\\uc2b9! GO? STOP?!\",\"\\uad6c\\uae00 \\uc790\\uccb4 \\uac1c\\ubc1c \\uc571\\ud50c\\ub808\\uc774\\uc5b4 'Google Play Games' \\uacf5\\uac1c, \\uc571 \\ud50c\\ub808\\uc774\\uc5b4 \\uc0dd\\ud0dc\\uacc4 \\uc9c0\\uac01\\ubcc0\\ub3d9 \\uc62c\\uae4c\",\"\\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uc804 \\uad6c\\uae00 CEO, \\ube14\\ub85d\\uccb4\\uc778 \\uc624\\ub77c\\ud074 \\uc194\\ub8e8\\uc158 \\uac1c\\ubc1c\\uc0ac '\\uccb4\\uc778\\ub9c1\\ud06c' \\ud569\\ub958\",\"[\\uc5ec\\uc758\\uc625] \\uc5d0\\ub9ad \\uc288\\ubbf8\\ud2b8 \\uc804 \\uad6c\\uae00 CEO, \\ube14\\ub85d\\uccb4\\uc778 \\uc624\\ub77c\\ud074 \\uc194\\ub8e8\\uc158 \\uac1c\\ubc1c\\uc0ac \\uc804\\uaca9 \\ud569\\ub958\",\"\\ub098\\uc2a4\\ubbf8\\ub514\\uc5b4, '\\uad6c\\uae00 \\ud504\\ub9ac\\ubbf8\\uc5b4 \\ud30c\\ud2b8\\ub108 \\uc5b4\\uc6cc\\uc988 2021' \\uc218\\uc0c1\\u2026\\ube0c\\ub79c\\ub4dc \\uc9c0\\uc18d\\uac00\\ub2a5 \\uc131\\uc7a5\\uc5d0 \\uae30\\uc5ec\",\"[\\uc0bc\\uc815KPMG \\ubcf4\\uace0\\uc11c] \\ud14c\\ud06c \\uae30\\uc5c5, ESG \\ub9ac\\uc2a4\\ud06c \\uc9c1\\uba74\\u2026 \\\"\\uc6b0\\uc120 \\uacfc\\uc81c \\ub3c4\\ucd9c\\ud574\\uc57c\\\"\",\"\\uc774\\uc7ac\\uc6a9 \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ud68c\\uc7a5, \\ubbf8\\uad6d\\uc5d0\\uc11c '\\ub274\\uc0bc\\uc131' \\ub3d9\\ub9f9 \\ud655\\uc778..\\ub9c8\\uc774\\ud06c\\ub85c\\uc18c\\ud504\\ud2b8\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uad6c\\uae00 \\uacbd\\uc601\\uc9c4 \\uc787\\ub530\\ub77c \\ub9cc\\ub098\",\"\\uc774\\uc7ac\\uc6a9 \\uc0bc\\uc131\\uc804\\uc790 \\ubd80\\ud68c\\uc7a5, \\ubbf8\\uad6d\\uc5d0\\uc11c '\\ub274\\uc0bc\\uc131' \\ub3d9\\ub9f9 \\ud655\\uc778..\\ub9c8\\uc774\\ud06c\\ub85c\\uc18c\\ud504\\ud2b8\\u00b7\\uc544\\ub9c8\\uc874\\u00b7\\uad6c\\uae00 \\uacbd\\uc601\\uc9c4 \\uc787\\ub530\\ub77c \\ub9cc\\ub098\",\"'\\uad6c\\uae00\\u2027\\uc560\\ud50c \\uc800\\uc2b9\\uc0ac\\uc790' EU\\uacbd\\uc7c1\\uc704\\uc6d0\\uc7a5, \\ube45\\ud14c\\ud06c \\uaddc\\uc81c \\uc704\\ud55c \\uaddc\\uce59 \\uc2dc\\ud589 \\ucd09\\uad6c\",\"'\\uad6c\\uae00\\u2027\\uc560\\ud50c \\uc800\\uc2b9\\uc0ac\\uc790' EU\\uacbd\\uc7c1\\uc704\\uc6d0\\uc7a5, \\ube45\\ud14c\\ud06c \\uaddc\\uc81c \\uc704\\ud55c \\uaddc\\uce59 \\uc2dc\\ud589 \\ucd09\\uad6c\"],\"id\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],\"topic\":[\"3\",\"3\",\"3\",\"3\",\"4\",\"0\",\"3\",\"4\",\"0\",\"3\",\"4\",\"0\",\"4\",\"4\",\"3\",\"4\",\"4\",\"3\",\"0\",\"0\",\"0\",\"0\",\"3\",\"4\",\"4\",\"3\",\"3\",\"2\",\"3\",\"4\",\"3\",\"3\",\"4\",\"4\",\"3\",\"3\",\"1\",\"1\",\"3\",\"4\",\"1\",\"3\",\"4\",\"1\",\"2\",\"4\",\"3\",\"1\",\"3\",\"1\",\"3\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\",\"3\",\"4\",\"4\",\"2\",\"2\",\"3\",\"3\"],\"x\":{\"__ndarray__\":\"UUTfQvR4IMNRyKnCwjTvQkkeQsPhb6zD6XBBw0keQsPhb6zD6XBBwwXJ+0GEqkfD6mQ+wlJo40G/IBZDFX+JQcrbJcKNS0BD3UTPwwmrG8NxhLzDcYS8w5Qzd0OsNApDX9wLwr9lZ8O/ZWfD9GJhQya9LEP8qKNCQ65XQ630wkGkAAvDpAALw95jXsLeY17CigIwQqyuk0MKNZ1CUkDJwih+w0J/Z2ZDDDgFQyh+w0I2apRDgvLWwglboEIld5RDy845QwVsQ0OalyDDPPCMQ0Tg1UNE4NVDx9TkQvPYA0NBqHBCyM4EQVRIo0IESaFB9OHUQ/Th1EM7ToTDO06Eww==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[64]},\"y\":{\"__ndarray__\":\"0JeTQkGdF8OK0S3DfpkvQ+YfZ0OaZgdCaDOGw+YfZ0OaZgdCaDOGw9teMEM0s3pC//+QQm+60cIb+O9Cdy8twyU5E0Oc9HbCBC/qQajNBEKEpu7BhKbuwdCrjcFsItzCktm/wivJGMMryRjD7F0fw5Q8gELu1Q3DeIXbQu9Y18Hb+oVD2/qFQ16pW8FeqVvBvkusQzd8k0L/ePJCgW3SwoLOpENspDZCD0S3QYLOpEM1kSXDh6/1QllyKMC4/RRDIxuPP5a0MEO7KJrCf5zSwkjb/MFI2/zBzLBZw8IQDsLsDUdCAYPuQQ+XkcILj8ZCvwjBwr8IwcLZuYHD2bmBww==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[64]}},\"selected\":{\"id\":\"1179\"},\"selection_policy\":{\"id\":\"1178\"}},\"id\":\"1163\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1177\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1152\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1153\",\"type\":\"ResetTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1155\"}},\"id\":\"1150\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1151\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1133\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis\":{\"id\":\"1141\"},\"coordinates\":null,\"group\":null,\"ticker\":null},\"id\":\"1144\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1176\"},\"group\":null,\"major_label_policy\":{\"id\":\"1177\"},\"ticker\":{\"id\":\"1142\"}},\"id\":\"1141\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Topic\",\"@topic\"],[\"id\",\"@id\"],[\"Article\",\"@document\"]]},\"id\":\"1149\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1167\",\"type\":\"Circle\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1163\"},\"glyph\":{\"id\":\"1165\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1167\"},\"nonselection_glyph\":{\"id\":\"1166\"},\"view\":{\"id\":\"1169\"}},\"id\":\"1168\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1155\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1146\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1154\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1182\"}],\"location\":\"top_left\"},\"id\":\"1181\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1137\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1135\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1173\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1139\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\"},\"hatch_color\":{\"field\":\"color\"},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1165\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1178\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1174\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1142\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1176\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1179\",\"type\":\"Selection\"},{\"attributes\":{\"tools\":[{\"id\":\"1149\"},{\"id\":\"1150\"},{\"id\":\"1151\"},{\"id\":\"1152\"},{\"id\":\"1153\"},{\"id\":\"1154\"}]},\"id\":\"1156\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis\":{\"id\":\"1145\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null},\"id\":\"1148\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"group\":null},\"id\":\"1170\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1166\",\"type\":\"Circle\"}],\"root_ids\":[\"1132\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n  const render_items = [{\"docid\":\"b4682a68-fa0e-4e7d-8772-58b06cbc8579\",\"root_ids\":[\"1132\"],\"roots\":{\"1132\":\"5a684a58-54da-4364-ac07-102665a18197\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1132"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "# 사용할 툴들\n",
    "tools_to_show = 'hover, box_zoom, pan, save, reset, wheel_zoom'\n",
    "p = figure(plot_width=720, plot_height=580, tools=tools_to_show)\n",
    "\n",
    "source = ColumnDataSource(data={\n",
    "    'x': W2d[:, 0],\n",
    "    'y': W2d[:, 1],\n",
    "    'id': [i for i in range(W.shape[0])],\n",
    "    'document': [article for words, article in words_list],\n",
    "    'topic': [str(i) for i in topicIndex],  # 토픽 번호\n",
    "    'color': [Category20[K][i] for i in topicIndex]\n",
    "})\n",
    "p.circle(\n",
    "    'x', 'y',\n",
    "    source=source,\n",
    "    legend='topic',\n",
    "    color='color'\n",
    ")\n",
    "\n",
    "# interaction\n",
    "p.legend.location = \"top_left\"\n",
    "hover = p.select({'type': HoverTool})\n",
    "hover.tooltips = [(\"Topic\", \"@topic\"), ('id', '@id'), (\"Article\", \"@document\")]\n",
    "hover.mode = 'mouse'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding은 단어가 가지는 고유한 벡터를 가지고 의미가 유사하면 유사도가 높도록(내적값이 커지도록) 의미가 작아지면 유사도가 작아지도록(내적값이 작아지도록) 해주는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec에는 CBOW와 Skip-Grad 두 가지 방식이 있습니다. CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법입니다. 반대로, Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법입니다. 매커니즘 자체는 거의 비슷합니다. 우선 CBOW에 대해서 알아보겠습니다. 이해를 위해 매우 간소화된 형태의 CBOW로 설명합니다.\n",
    "\n",
    "**예문: \"The fat cat sat on the mat\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) CBOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어 코퍼스에 위와 같은 문장이 있습니다. {\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"}으로부터 \"sat\"을 예측하는 것이 CBOW가 할 일입니다. 이 때 예측하는 단어 sat을 중심 단어(center word)라고 하고, 예측에 사용되는 단어들을 주변 단어(context word)라고 합니다.\n",
    "\n",
    "중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지 결정했다면 이 범위를 윈도우(window)라고 합니다. 예를 들어 윈도우 크기가 2이고 예측하고자 하는 중심 단어가 sat이라고 한다면 앞의 두 단어인 fat과 cat, 그리고 뒤의 두 단어인 on, the를 참고합니다. 윈도우의 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n이 됩니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG\">\n",
    "\n",
    "윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만들 수 있는데, 이 방법을 슬라이딩 윈도우(sliding window)라고 합니다. \n",
    "\n",
    "위 그림에서 좌측의 중심 단어와 주변 단어의 변화는 윈도우 크기가 2일 때, 슬라이딩 윈도우가 어떤 식으로 이루어지면서 데이터 셋을 만드는지 보여줍니다. 또한 Word2Vec에서 입력은 원-핫 벡터가 되어야 하는데, 우측 그림은 중심 단어와 주변 단어를 어떻게 선택했을 때에 따라서 각각 어떤 원-핫 벡터가 되는지를 보여줍니다. 밑의 그림은 결국 CBOW를 위한 전체 데이터 셋을 보여주는 것입니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG\">\n",
    "\n",
    "CBOW의 인공 신경망을 간단히 도식화하면 위와 같습니다. 입력층(Input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고, 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 필요합니다. \n",
    "\n",
    "또한 위 그림에서 알 수 있는 사실은, Word2Vec은 딥 러닝 모델(Deep Learning Model)은 아니라는 점입니다. 보통 딥 러닝이라함은, 입력층과 출력층 사이의 은닉층의 개수가 충분히 쌓인 신경망을 학습할 때를 말하는데 Word2Vec는 입력층과 출력층 사이에 하나의 은닉층만이 존재합니다. 이렇게 은닉층이 1개인 경우에는 일반적으로 심층신경망(Deep Neural Network)이 아니라 얕은신경망(Shallow Neural Network)이라고 부릅니다. 또한 Word2Vec의 은닉층은 일반적인 은닉층과 달리 활성화 함수가 존재하지 않으며 룩업 테이블이라는 연산을 담당하는 층으로 일반적인 은닉층과 구분하기 위해 투사층(projection layer)이라고 부르기도 합니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG\">\n",
    "\n",
    "CBOW의 인공신경망을 더 확대하여 알아보겠습니다. 이 그림에서 투사층의 크기가 M이라는 것과 입력층과 투사층 사이의 가중치 W는 V x M이고 투사층과 출력층 사이 가중치 W'의 크기는 M x V임을 주목해야 합니다.\n",
    "\n",
    "먼저 CBOW에서 투사층의 크기 M은 임베딩하고 난 벡터의 차원이 됩니다. 다시 말해, 위 그림에서 투사층의 크기는 M = 5이기 때문에 CBOW를 수행하고 나서 얻는 각 단어의 임베딩 벡터의 차원은 5입니다.\n",
    "\n",
    "두번째로 V는 단어 집합의 크기를 의미합니다. 즉, 위의 그림처럼 원-핫 벡터의 차원이 7이고, M은 5라면 가중치 W는 7 x 5 행렬이고, W'는 5 x 7 행렬이 될 것입니다. 이때 W와 W'는 동일한 행렬을 전치한 것이 아니라 서로 다른 행렬입니다. 인공 신경망의 훈련 전에 이 가중치 행렬 W와 W'는 대게 굉장히 작은 랜덤 값을 가지게 됩니다. CBOW는 주변 단어로 중심 단어를 더 정확히 맞추기 위해서 W와 W'를 학습해가는 구조입니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG\">\n",
    "\n",
    "입력으로 들어오는 주변 단어의 원-핫 벡터와 가중치 W 행렬의 곱이 어떻게 이루어지는지 보겠습니다. 위 그림에서 각 주변 단어의 원-핫 벡터를 $x$로 표기했습니다. 입력 데이터는 원-핫 벡터입니다. 입력 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 같기에 이 작업을 룩업 테이블(lookup table)이라고 합니다. 여기서 lookup해온 W의 각 행벡터가 사실 Word2Vec을 수행한 후의 각 단어의 M차원 크기를 갖는 임베딩 벡터들입니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG\">\n",
    "\n",
    "이렇게 각 주변 단어의 원-핫 벡터에 대해서 가중치 W가 곱해서 생겨진 결과 벡터들은 투사층에서 만나 이들의 평균인 벡터를 구합니다. CBOW는 투사층에서 벡터의 평균을 구하지만 뒤에서 볼 Skip-Gram은 입력이 중심 단어 하나이기에 투사층에서 벡터의 평균을 구하지 않습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG\">\n",
    "\n",
    "이제 투사층에서 구해진 평균 벡터는 두번째 가중치 행렬 W'와 곱해집니다. 곱셈의 결과는 원-핫 벡터들과 동일한 차원을 가진 벡터로 나옵니다. \n",
    "\n",
    "이 벡터에서 CBOW는 소프트맥스 함수를 사용합니다. 출력된 벡터의 총 합은 1이고 이렇게 나온 벡터를 스코어 벡터(score vector)라고 합니다. 스코어 벡터는 각 인덱스번째 단어가 중심 단어일 확률을 나타냅니다. 그리고 스코어 벡터가 우리가 가진 답의 원-핫 벡터와 가까워져야 합니다. \n",
    "\n",
    "스코어 벡터를 $\\hat{y}$, 중심 단어를 $y$라고 했을 때, CBOW의 손실 함수로 cross-entropy 함수를 사용합니다.\n",
    "\n",
    "$$H(\\hat{y}, y) = -\\sum_{j=1}^{\\lvert V \\rvert} y_j log(\\hat{y}_j)$$\n",
    "\n",
    "cross-entropy 함수에 실제 중심 단어인 원-핫 벡터와 스코어 벡터를 입력값으로 넣고, 이를 식으로 표현하면 위와 같습니다. \n",
    "\n",
    "$$H(\\hat{y}, y) = -y_i log(\\hat{y_i})$$\n",
    "\n",
    "그런데 y가 원-핫 벡터임을 고려하면, 식을 위와 같이 간소화할 수 있습니다. c를 중심 단어에서 1을 가진 차원의 값의 인덱스라고 한다면 $\\hat{y}_c = 1$는 $\\hat{y}$가 $y$를 정확하게 예측한 경우가 됩니다. 이를 식에 대입하면 $-1log(1) = 0$이기에 결과적으로 정확히 예측한 경우의 cross-entropy 값은 0이 됩니다. 따라서 위 식을 최소화하는 방향으로 학습해야 하며 loss function으로 사용해도 됩니다.\n",
    "\n",
    "이제 역전파를 수행하면 W와 W'가 학습되는데, 학습이 다 되었다면 M차원의 크기를 갖는 W의 행이나 W'의 열로부터 어떤 것을 임베딩 벡터로 사용할지 결정하면 됩니다. 떄로는 W와 W'의 평균치를 가지고 임베딩 벡터를 선택하기도 합니다.\n",
    "\n",
    "Word2Vec의 알고리즘을 다시 요약해보면 다음과 같습니다.\n",
    "\n",
    "![4-3-1](_image/4-3-1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Skip-Gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram은 CBOW를 이해했다면, 매커니즘 자체는 동일하기 때문에 쉽게 이해할 수 있습니다. 앞서 CBOW에선 주변 단어를 통해 중심 단어를 예측했다면, skip-gram은 중심 단어에서 주변 단어를 예측하겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG\">\n",
    "\n",
    "앞서 언급한 동일한 예문에 대해서 인공 신경망을 도식화해보면 위와 같습니다. 이제 중심 단어에 대해서 주변 단어를 예측하기 때문에, 투사층에서 벡터들의 평균을 구하는 과정은 없습니다. \n",
    "\n",
    "여러 논문에서 성능 비교를 진행했을 때, 전반적으로 skip-gram이 CBOW보다 성능이 좋다고 알려져 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Negative Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대체적으로 Word2Vec를 사용한다고 하면 SGNS(Skip-Gram with Negative Sampling)을 사용합니다. 이는 skip-gram 방법에, 네거티브 샘플링이란 방법까지 추가로 사용하는 것입니다. 그렇기에 skip-gram을 전제로 네거티브 샘플링에 대해서 알아보겠습니다.\n",
    "\n",
    "위에서 배운 Word2Vec 모델은 속도가 문제입니다. 마지막 단계를 봅시다. 출력층에 있는 소프트맥스 함수는 단어 집합 크기의 벡터 내의 모든 값을 0과 1 사이의 값이면서 모두 더하면 1이 되도록 바꾸는 작업을 수행합니다. 그리고 이에 대한 오차를 구하고 모든 단어에 대한 임베딩을 조정합니다. 그 단어가 중심 단어나 주변 단어와 전혀 상관없는 단어라도 마찬가지입니다. 그런데 만약 단어 집합의 크기가 수백만에 달한다면 이 작업은 굉장히 무거워집니다.\n",
    "\n",
    "여기서 핵심은 모든 단어 집합에 대해서 소프트맥스 함수를 수행하고, 역전파를 수행하므로 주변 단어와 상관 없는 모든 단어까지 워드 임베딩 조정 작업을 수행한다는 것입니다. 만약 '강아지'와 '고양이'와 같은 단어에 집중한다면 '돈가스'나 '컴퓨터'와 같은 연관 관계가 없는 수많은 단어의 임베딩을 조정할 필요가 없습니다. \n",
    "\n",
    "그렇다면 전체 단어 집합이 아니라 일부 단어 집합에 대해서만 고려할 수는 없을까요? '강아지', '고양이' 같은 주변 단어들로 일부 단어 집합을 만듭니다. 그리고 여기에 '돈가스', '컴퓨터', '회의실' 같은 무작위로 선택된 주변 단어가 아닌 상관없는 단어들을 일부만 갖고옵니다. 이렇게 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단께를 이진 분류 문제로 바꿔버립니다. 즉, Word2Vec은 주변 단어들을 긍정(positive)으로 두고 무작위로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행합니다. \n",
    "\n",
    "이는 기존의 다중 클래스 분류 문제를 이진 분류 문제로 바꾸면서도 연산량에 있어서 훨씬 효율적입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Word2Vec Property**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-3-2](_image/4-3-2.PNG)\n",
    "링크: https://ronxin.github.io/wevi/\n",
    "\n",
    "위 그림은 Word2Vec이 동작하는 것을 보여주는 페이지에 결과입니다. apple과 juice는 관계가 있기에 apple input vector와 juice output vector의 값이 모두 양수가 되는 것을 확인할 수 있습니다. 반대로 rice input vector와 juice output vector는 음수가 많이 나오는 것을 확인할 수 있습니다. \n",
    "\n",
    "또한 이렇게 구해진 embedding vector들은 유사한 관계를 가진 벡터쌍끼리 비슷한 관계를 가집니다. 예를 들어 (대한민국, 서울)과 (일본, 도쿄)는 (나라, 수도)의 관계를 가지고 있습니다. 그렇기에 대한민국 - 서울 = 일본 - 도쿄의 식이 성립됩니다. 그 외에도 여러 유사 관계를 밑의 그림에서 확인할 수 있습니다.\n",
    "\n",
    "![4-3-3](_image/4-3-3.PNG)\n",
    "\n",
    "이외에도 거의 대부분의 NLP분야에서 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 자료: https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/  \n",
    "참고 자료: https://simonezz.tistory.com/35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA(Latent Semantic Analysis)는 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력 받아 차원을 축소(Truncated SVD)하여 잠재된 의미를 끌어내는 방법론입니다. 반면, Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론입니다. \n",
    "\n",
    "LSA는 카운트 기반이기에 전체적인 통계 정보를 고려하기는 하지만, '왕:남자 = 여왕:?'과 같은 단어 의미의 유추 작업(Analogy task)에는 성능이 떨어집니다. Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못합니다. GloVe는 이러한 기존 방법론들의 각각의 한계를 지적하며 LSA의 매커니즘이었던 카운트 기반의 방법과 Word2Vec의 매커니즘이었던 예측 기반의 방법론을 두 가지 모두 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 GloVe에 대해 간단하게 살펴보겠습니다. GloVe는 위에서 이야기했듯 Word2Vec에 카운터 기반의 방법을 합친 것입니다. 그렇기에 단어들의 쌍들이 나오는 개수를 먼저 count하여 행렬로 만듭니다. 그리고 $W_1 \\cdot W_2$가 구한 행렬과 같아지도록 학습시키는 것입니다. 이때 the 등의 관사는 빈도수가 너무 높기에 이를 억제하기 위해 log항을 추가합니다. 또한 빈도수에 따라 가중치를 주지만 이 역시 너무 커지지 않도록 f라는 함수를 사용합니다. 임베딩 벡터로는 Word2Vec과 마찬가지로 $W_1$을 사용하거나 두 개의 평균을 임베딩 벡터로 사용합니다. \n",
    "\n",
    "밑의 그림은 loss function(bias = 0)과 f함수를 보여줍니다.\n",
    "\n",
    "![4-3-4](_image/4-3-4.PNG)\n",
    "\n",
    "이제 구체적으로 GloVe의 동작을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Windoe based Co-occurrence Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 단어 쌍의 빈도수의 정보를 가지는 윈도우 기반 동시 등장 행렬에 대해 보겠습니다.\n",
    "\n",
    "단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들을 구성하고, i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말합니다. 예제를 보겠습니다.\n",
    "\n",
    "Ex)  \n",
    "I like deep learning  \n",
    "I like NLP  \n",
    "I enjoy flying  \n",
    "\n",
    "윈도우 크기가 N일 때는 좌, 우에 존재하는 N개의 단어만 참고하게 됩니다. 윈도우 크기가 1일 때, 위 텍스트를 가지고 동시 등장 행렬은 다음과 같습니다.\n",
    "\n",
    "|카운트|I|like|enjoy|deep|learning|NLP|flying|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|I|0|2|1|0|0|0|0|\n",
    "|like|2|0|0|1|0|1|0|\n",
    "|enjoy|1|0|0|0|0|0|1|\n",
    "|deep|0|1|0|0|1|0|0|\n",
    "|learning|0|0|0|1|0|0|0|\n",
    "|NLP|0|1|0|0|0|0|0|\n",
    "|flying|0|0|1|0|0|0|0|\n",
    "\n",
    "위 행렬은 행렬을 전치(Transpose)해도 동일한 행렬이 된다는 특징이 있습니다. 그 이유는 i 단어의 윈도우 크기 내에서 k 단어가 등장한 빈도는 반대로 k 단어의 윈도우 크기 내에서 i 단어가 등장한 빈도와 동일하기 때문입니다. \n",
    "\n",
    "참고 자료: http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Co-occurrence Probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 동시 등장 확률에 대해서 알아보겠습니다. 아래의 표는 어떤 동시 등장 행렬을 가지고 정리한 동시 등장 확률(Co-occurrence Probability)을 보여줍니다. 여기서 이야기하는 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률입니다. \n",
    "\n",
    "$P(k|i)$에서 i를 중심 단어(Center word), k를 주변 단어(Context word)라고 했을 때, 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모로 하고 i행 k열의 값을 분자로 한 값이라고 볼 수 있겠습니다. 다음은 GloVe의 제안 논문에서 가져온 동시 등장 확률을 표로 정리한 하나의 예입니다.\n",
    "\n",
    "|동시 등장 확률과 크기 관계 비(ratio)|k=solid|k=gas|k=water|k=fasion|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|$P(k \\vert ice)$|0.00019|0.000066|0.003|0.000017|\n",
    "|$P(k \\vert steam)$|0.000022|0.00078|0.0022|0.000018|\n",
    "|$\\frac{P(k \\vert ice)}{P(k \\vert steam)}$|8.9|0.085|1.36|0.96|\n",
    "\n",
    "위의 표를 통해 알 수 있는 사실은 solid가 등장했을 때, ice가 등장할 확률은 0.00019은 solid가 등장했을 때 steam이 등장할 확률인 0.000022보다 약 8.9배 크다는 사실입니다. 그도 그럴 것이 solid는 '단단한'이라는 의미를 가졌으니까 '증기'라는 의미를 가지는 steam보다는 당연히 '얼음'이라는 의미를 가지는 ice라는 단어와 더 자주 등장할 겁니다.\n",
    "\n",
    "수식적으로 다시 정리하면 k가 solid일 때, $\\frac{P(solid \\vert ice)}{P(solid \\vert steam)}$를 계산한 값은 8.9가 나옵니다. 이는 1보다 매우 큰 값입니다. 왜냐면 $P(solid \\vert ice)$의 값은 크고, $P(solid \\vert steam)$의 값은 작기 때문입니다.\n",
    "\n",
    "그런데 k를 solid가 아니라 gas로 바꾸면 이야기는 완전히 달라집니다. gas는 ice보다는 steam과 더 자주 등장하므로, $\\frac{P(gas \\vert ice)}{P(gas \\vert steam)}$를 계산한 값은 1보다 훨씬 작은 값인 0.085가 나옵니다. 반면, k가 water인 경우에는 solid와 steam 두 단어 모두와 동시 등장하는 경우가 많으므로 1에 가까운 값이 나오고, k가 fasion인 경우에는 solid와 steam 두 단어 모두와 동시 등장하는 경우가 적으므로 1에 가까운 값이 나옵니다. 보기 쉽도록 조금 단순화해서 표현한 표는 다음과 같습니다.\n",
    "\n",
    "|동시 등장 확률과 크기 관계 비(ratio)|k=solid|k=gas|k=water|k=fasion|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|$P(k \\vert ice)$|큰 값|작은 값|큰 값|작은 값|\n",
    "|$P(k \\vert steam)$|작은 값|큰 값|큰 값|작은 값|\n",
    "|$\\frac{P(k \\vert ice)}{P(k \\vert steam)}$|큰 값|작은 값|1에 가까움|1에 가까움|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 손실 함수를 설명하기 전에 각 용어를 정리하겠습니다.\n",
    "\n",
    "- $X$: 동시 등장 행렬(Co-occurrence Matrix)\n",
    "- $X_{ij}$: 중심 단어 i가 등장했을 때, 윈도우 내 주변 단어 j가 등장하는 횟수\n",
    "- $X_i$: $\\sum_j X_{ij}$: 동시 등장 행렬에서 i행의 값을 모두 더한 값\n",
    "- $P_{ik}$: $P(k \\vert i) = \\frac{X_{ik}}{X_i}$: 중심 단어 i가 등장했을 때, 윈도우 내 주변 단어 k가 등장할 확률  \n",
    "    Ex) $P(solid \\vert ice)$ = 단어 ice가 등장했을 때, 단어 solid가 등장할 확률\n",
    "- $\\frac{P_{ik}}{P_{jk}}$: $P_{ik}$를 $P_{jk}$로 나눠준 값  \n",
    "    Ex) $\\frac{P(solid \\vert ice)}{P(solid \\vert steam)}$ = 8.9\n",
    "- $w_i$: 중심 단어 i의 임베딩 벡터\n",
    "- $\\bar{w_k}$: 주변 단어 k의 임베딩 벡터\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe의 아이디어를 한 줄로 요약하면 **'임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것'** 입니다. 즉, 이를 만족하도록 임베딩 벡터를 만드는 것이 목표입니다. 이를 식으로 표현하면 다음과 같습니다.\n",
    "\n",
    "$dot product(w_i, \\bar{w_k}) \\approx P(k \\vert i) = P_{ik}$\n",
    "\n",
    "뒤에서 보겠지만, 더 정확히는 GloVe는 아래와 같은 관계를 가지도록 임베딩 벡터를 설계합니다.\n",
    "\n",
    "$dot product(w_i, \\bar{w_k}) \\approx log P(k \\vert i) = log P_{ik}$\n",
    "\n",
    "임베딩 벡터들을 만들기 위한 손실 함수를 처음부터 차근차근 설계해보겠습니다. 가장 중요한 것은 단어 간의 관계를 잘 표현하는 함수여야 한다는 것입니다. 이를 위해 앞서 배운 개념인 $P_{ik}/P_{jk}$를 식에 사용합니다. GloVe의 연구진들은 벡터 $w_i, w_j, \\bar{w_k}$를 가지고 어떤 함수 $F$를 수행하면 $P_{ik}/P_{jk}$가 나온다는 초기 식으로부터 전개를 시작합니다.\n",
    "\n",
    "$$F(w_i, w_j, \\bar{w_k}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "아직 이 함수가 $F$가 어떤 식을 가지고 있는지는 정해진 것이 없습니다. 위의 목적에 맞게 근사할 수 있는 함수식은 무수히 많겠으나 최적의 식에 다가가기 위해서 단계별로 디테일을 추가하겠습니다. 함수 $F$는 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩하는 것이 목적입니다. 이를 위해 GloVe 연구진들은 $w_i$와 $w_j$라는 두 벡터의 차이를 함수 $F$의 입력으로 사용하는 것을 제안합니다.\n",
    "\n",
    "$$F(w_i - w_j, \\bar{w_k}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "그런데 우변은 스칼라값이고 좌변은 벡터값입니다. 이를 성립하기 위해서 함수 $F$의 두 입력에 내적(dot product)을 수행합니다.\n",
    "\n",
    "$$F((w_i - w_j)^T \\bar{w_k}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "정리하면, 선형 공간(Linear space)에서 단어의 의미 관계를 표현하기 위해 뺄셈과 내적을 택했습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 함수 $F$가 만족해야 할 필수 조건이 있습니다. 중심 단어 $w$와 주변 단어 $\\bar{w}$라는 선택 기준은 실제로는 무작위 선택이므로 이 둘의 관계는 자유롭게 교환될 수 있도록 해야합니다. 이것이 성립되게 하기 위해서 GloVe 연구진은 함수 $F$가 실수의 덧셈과 양수의 곱셈에 대해서 **준동형(Homomorphism)** 을 만족하도록 합니다. 이를 쉽게 정리하면 $a$와 $b$에 대해서 함수 $F$가 $F(a + b)$가 $F(a)F(b)$와 같도록 만족시켜야 한다는 의미입니다.\n",
    "\n",
    "식으로 나타내면 다음과 같습니다.\n",
    "\n",
    "$$F(a + b) = F(a)F(b), \\forall a, b \\in \\mathbb{R}$$\n",
    "\n",
    "이제 이 준동형식을 현재 전개하던 GloVe 식에 적용할 수 있도록 조금씩 바꿔보겠습니다. 전개하던 GloVe 식에 따르면, 함수 $F$는 결과값으로 스칼라 값($\\frac{P_{ik}}{P_{jk}}$)이 나와야 합니다. 준동형식에서 $a$와 $b$가 각각 벡터값이라면 함수 $F$의 결과값으로는 스칼라 값이 나올 수 없지만, $a$와 $b$가 각각 사실 두 벡터의 내적값이라고 하면 결과값으로 스칼라 값이 나올 수 있습니다. 그러므로 위의 준동형식을 아래와 같이 바꿔보겠습니다. 여기서 $v_1, v_2, v_3, v_4$는 각각 벡터값입니다. 아래의 $V$는 벡터를 의미합니다.\n",
    "\n",
    "$$F(v_1^T v_2 + v_3^T v_4) = F(v_1^T v_2)F(v_3^T v_4), \\forall v_1, v_2, v_3, v_4 \\in V$$\n",
    "\n",
    "그런데 앞서 작성한 GloVe 식에서는 $w_i$와 $w_j$라는 두 벡터의 차이를 함수 $F$의 입력으로 받았습니다. GloVe식에 바로 적용을 위해 준동형식을 뺄셈에 대한 준동형식으로 변경합니다. 그러면 곱셈도 나눗셈으로 바뀌게 됩니다.\n",
    "\n",
    "$$F(v_1^T v_2 - v_3^T v_4) = \\frac{F(v_1^T v_2)}{F(v_3^T v_4)}, \\forall v_1, v_2, v_3, v_4 \\in V$$\n",
    "\n",
    "이제 이 준동형식을 GloVe 식에 적용하겠습니다. 우선, 함수 $F$의 우변은 다음과 같이 바뀌어야 합니다.\n",
    "\n",
    "$$F((w_i - w_j)^T \\bar{w_k}) = \\frac{F(w_i^T \\bar{w_k})}{F(w_j^T \\bar{w_k})}$$\n",
    "\n",
    "그런데 이전의 식에 따르면 우변은 본래 $\\frac{P_{ik}}{P_{jk}}$였으므로, 결과적으로 다음과 같습니다.\n",
    "\n",
    "$$\\frac{P_{ik}}{P_{jk}} = \\frac{F(w_i^T \\bar{w_k})}{F(w_j^T \\bar{w_k})}$$\n",
    "\n",
    "$$F(w_i^T \\bar{w_k}) = P_{ik} = \\frac{X_{ik}}{X_i}$$\n",
    "\n",
    "좌변을 풀어쓰면 다음과 같습니다.\n",
    "\n",
    "$$F(w_i^T \\bar{w_k} - w_j^T \\bar{w_k}) = \\frac{F(w_i^T \\bar{w_k})}{F(w_j^T \\bar{w_k})}$$\n",
    "\n",
    "이는 뺄셈에 대한 준동형식의 형태와 정확히 일치합니다. 이제 이를 만족하는 함수 $F$를 찾아야 합니다. 그리고 이를 정확하게 만족시키는 함수가 있는데 바로 지수 함수입니다. $F$를 지수 함수 **exp** 라고 해봅시다.\n",
    "\n",
    "$$exp(w_i^T \\bar{w_k} - w_j^T \\bar{w_k}) = \\frac{exp(w_i^T \\bar{w_k})}{exp(w_j^T \\bar{w_k})}$$\n",
    "\n",
    "$$exo(w_i^T \\bar{w_k}) = P_{ik} = \\frac{X_{ik}}{X_i}$$\n",
    "\n",
    "위의 두 번째 식으로부터 다음과 같은 식을 얻을 수 있습니다.\n",
    "\n",
    "$$w_i^T \\bar{w_k} = log P_{ik} = log(\\frac{X_{ik}}{X_i}) = log X_{ik} - log X_i$$\n",
    "\n",
    "그런데 여기서 상기해야할 것은 앞서 언급했듯이, 사실 $w_i$와 $\\bar{w_k}$는 두 값의 위치를 서로 바꾸어도 식이 성립해야 합니다. $X_{ik}$의 정의를 생각해보면 $X_{ki}$와도 같습니다. 그런데 이게 성립되려면 위의 식에서 $log X_i$ 항이 걸림돌입니다. 이 부분만 없다면 이를 성립시킬 수 있습니다. 그래서 GloVe 연구팀은 이 $log X_i$항을 $w_i$에 대한 편향 $b_i$라는 상수항으로 대체하기로 합니다. 같은 이유로 $\\bar{w_k}$에 대한 편향 $\\bar{b_k}$를 추가합니다.\n",
    "\n",
    "$$w_i^T \\bar{w_k} + b_i + \\bar{b_k} = log X_{ik}$$\n",
    "\n",
    "이 식이 손실 함수의 핵심이 되는 식입니다. 우변의 값과 차이를 최소화하는 방향으로 좌변의 4개의 항이 학습을 통해 바뀌게 됩니다. 즉, 손실 함수는 다음과 같이 일반화될 수 있습니다.\n",
    "\n",
    "$$Loss \\; function = \\sum_{m, n = 1}^V (w_m^T \\bar{w_n} + b_m + \\bar{b_n} - log X_{mn})^2$$\n",
    "\n",
    "여기서 $V$는 단어 집합의 크기를 의미합니다. 그런데 아직 최적의 손실 함수라기에는 부족합니다. GloVe 연구진은 $log X_{ik}$에서 $X_{ik}$값이 0이 될 수 있음을 지적합니다. 대안 중 하나는 $log X_{ik}$항을 $log (1 + X_{ik})$로 변경하는 것입니다. 하지만 이렇게 해도 여전히 해결되지 않는 문제가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 동시 등장 행렬 $X$는 마치 DTM처럼 희소 행렬일 가능성이 다분하다는 점입니다. 동시 등장 행렬 $X$에는 많은 값이 0이거나, 동시 등장 빈도가 적어서 많은 값이 작은 수치를 가지는 경우가 많습니다. 앞서 빈도수를 가지고 가중치를 주는 고민을 하는 TF-ID나 LSA와 같은 몇 가지 방법들을 본 적이 있습니다. GloVe의 연구진은 동시 등장 행렬에서 동시 등장 빈도의 값 $X_{ik}$이 굉장히 낮은 경우에는 정보에 거의 도움이 되지 않는다고 판단합니다. 그래서 이에 대한 가중치를 주는 고민을 하게 되는데 GloVe 연구팀이 선택한 것은 바로 $X_{ik}$의 값에 영향을 받는 가중치 함수 $f(X_{ik})$를 손실 함수에 도입하는 것입니다. \n",
    "\n",
    "GloVe에 도입되는 $f(X_{ik})$의 그래프를 그려보겠습니다.\n",
    "\n",
    "<img src = \"https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG\">\n",
    "\n",
    "$X_{ik}$의 값이 작으면 상대적으로 함수의 값은 작도록 하고, 값이 크면 함수의 값은 상대적으로 크도록 합니다. 하지만 $X_{ik}$가 지나치게 높다고해서 지나친 가중치를 주지 않기 위해 함수의 최대값은 정해져 있습니다. (최대값은 1) 예를 들어 'It is'와 같은 불용어의 동시 등장 빈도수가 높다고해서 지나친 가중을 받아서는 안 됩니다. 이 함수의 값을 손실 함수에 곱해주면 가중치의 역할을 할 수 있습니다.\n",
    "\n",
    "이 함수 $f(x)$의 식은 다음과 같이 정의됩니다. \n",
    "\n",
    "$$f(x) = min(1, (\\frac{x}{x_{max}}^{\\frac{3}{4}}))$$\n",
    "\n",
    "최종적으로 다음과 같은 일반화 된 손실 함수를 얻어낼 수 있습니다.\n",
    "\n",
    "$$Loss \\; function = \\sum_{m, n = 1}^V f(X_{mn})(w_m^T \\bar{w_n} + b_m + \\bar{b_n} - log X_{mn})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) Pre-trained dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GloVe](https://nlp.stanford.edu/projects/glove/)에선 이미 학습된 임베딩 벡터를 제공합니다. 자료마다 뒤에 설명이 적혀있으니 참고하여 필요한 데이터를 사용하면 됩니다. 예를 들어 Wikipedia 2014 + Gigaword5에 대한 설명은 다음과 같습니다. \n",
    "\n",
    "- 6B tokens: 60억개의 토큰(중복 단어 허용)\n",
    "- 400k vocab: 40만개의 고유 단어(40만개의 one-hot vector)\n",
    "- uncased: 대소문자 구분 안 함\n",
    "- 50d, 100d, 200d, & 300d vectors: 50, 100, 200, 300을 target dimension으로 학습된 임베딩 벡터가 있음을 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Doc2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec(Paragraph2Vec)는 기존 Word2Vec에 paragraph vector를 더해 확장한 문서 임베딩 모델입니다. 타켓 단어와 이전 단어 k개가 주어졌을 때, 이전 단어들과 해당 문서의 아이디로 타켓 단어를 예측합니다. 그리고 이 과정에서 문맥이 비슷한 문서 벡터와 단어 벡터가 유사하게 임베딩됩니다.\n",
    "\n",
    "Doc2Vec은 다량의 코퍼스를 문서 임베딩 할 때 훌룡한 성능을 보여줍니다. Word2Vec이 CBOW와 Skip-Gram우로 나뉘었듯 Doc2Vec도 PV-DM(Distributed Memory version of Paragraph Vector)과 PV-DBOW(Distributed Bag of Words version of Paragraph Vector)로 나눠집니다. 아래와 같은 예시가 있을 때 동작하는 것을 살펴보겠습니다.\n",
    "\n",
    "- sentence: the cat sat on the mat\n",
    "- window size: k = 3\n",
    "- [$\\text{paragraph}_1$, the, cat, sat] - on\n",
    "- [$\\text{paragraph}_1$, cat, sat, on] - the\n",
    "- [$\\text{paragraph}_1$, sat, on, the] - mat\n",
    "\n",
    "Doc2Vec은 paragraph에서 단어를 예측하며 로그 확률 평균을 최대화하는 과정에서 학습됩니다. $\\text{paragraph}_{id}$가 학습의 입력 데이터로 들어가기에 문맥이나 단어가 paragraph 벡터에 녹아든다고 볼 수 있습니다. \n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbOvRfL%2FbtqBzPhTjCD%2FNeKAov500OG05vrsvraZV0%2Fimg.png\" width = \"600px\" height = \"300px\"> \n",
    "\n",
    "각 문서 paragraph는 별도의 (문서의 수 x d 차원) 크기의 행렬에 담깁니다. 학습이 완료된 후, 이 행렬을 이용하여 paragraph의 임베딩된 벡터를 사용합니다. 즉, paragraph의 정보돠 이전 단어들을 통해 다음 단어를 유추하는 것입니다. 이를 PV-DM이라고 합니다. \n",
    "\n",
    "반대로 하나의 $\\text{paragraph}_{id}$로 해당 문서 내 단어들을 유추하는 것을 PV-DBOW라고 합니다. \n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbx8qpy%2FbtqBzjXPOfy%2FNPzuVPVD1RGp4TxKtIXQoK%2Fimg.png\">\n",
    "\n",
    "이제 예시를 통해 살펴보겠습니다. \n",
    "\n",
    "![4-3-5](_image/4-3-5.PNG)\n",
    "\n",
    "위 그림을 보면 input에 (study, female, 10s)가 들어가 있는 것을 볼 수 있습니다. 이는 단어인 'study'외에도 paragraph와 관련된 정보들이 추가로 들어가 있기 때문입니다. input을 통해 10대 여자가 썼다는 정보를 얻을 수 있습니다. \n",
    "\n",
    "![4-3-6](_image/4-3-6.PNG)\n",
    "\n",
    "입력이 여러개이기에 연산 과정에서 데이터들이 각 가중치에 곱해지고 합해집니다. 이 과정을 통해 단어뿐만 아니라 추가적으로 들어오는 paragraph의 정보들도 함께 학습합니다. 위 상황은 (study, female, 10s)라는 input을 통해 (math)라는 output을 얻은 것입니다. \n",
    "\n",
    "반대도 가능합니다. \n",
    "\n",
    "![4-3-7](_image/4-3-7.PNG)\n",
    "\n",
    "위 그림의 경우 단어만 주어지고 이를 통해 다음 단어뿐만 아니라 성별과 나이까지 유추합니다. 그렇기에 loss도 각각 따로 구하고 다른 가중치를 곱하여 합하는 것을 볼 수 있습니다. 소프트맥스를 할 때도 마찬가지로 (i, study, math), (male, female), (10s, 20s, 30s, 40s) 세 분류로 나누어 각각 소프트맥스해줍니다. \n",
    "\n",
    "![4-3-8](_image/4-3-8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Other Applications of Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec을 다른 방향으로 사용할 수도 있습니다. 예를 들어 밑의 그림을 보겠습니다. \n",
    "\n",
    "![4-3-9](_image/4-3-9.PNG)\n",
    "\n",
    "(John, Jane, Michael)이란 사람이 input으로 주어지고 그들이 구매하거나 관심있는 (Galaxy, iPhone, Macbook, iPad)가 output으로 주어집니다. 이를 통해 그 사람이 관심있는 상품들을 묶어 학습시킬 수도 있습니다. 즉 $W_1$은 사용자에 대한 임베딩 벡터, $W_2$는 사용자가 구매한 상품에 대한 임베딩 벡터가 되는 것입니다.\n",
    "\n",
    "또한 임베딩 벡터를 통하여 비슷한 제품군을 묶고 관련 상품으로 추천할 수도 있습니다.\n",
    "\n",
    "![4-3-10](_image/4-3-10.PNG)\n",
    "\n",
    "위 그림을 보면 비슷한 옷끼리 가까운 거리에 위치하는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. Word2Vec 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab, Twitter, Okt, Kkma\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) 데이터 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    \"정말 맛있습니다. 추천합니다.\",\n",
    "    \"기대했던 것보단 별로였네요.\",\n",
    "    \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
    "    \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
    "    \"음식도 서비스도 다 만족스러웠습니다.\",\n",
    "    \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
    "    \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
    "    \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
    "    \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
    "    \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
    "]\n",
    "\n",
    "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization과 vocab을 만드는 과정은 지난 챕터 실습들과 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized(data):\n",
    "    tokenized = []\n",
    "    for sent in tqdm(data):\n",
    "        tokens = tokenizer.morphs(sent, stem=True)\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = make_tokenized(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "\n",
    "for tokens in tqdm(train_tokenized):\n",
    "    for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(list(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "w2i = {}\n",
    "for pair in tqdm(word_count):\n",
    "    if pair[0] not in w2i:\n",
    "        w2i[pair[0]] = len(w2i)\n",
    "\n",
    "i2w = {v:k for k, v in w2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
      "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Dataset 클래스 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Word2Vec을 학습시키는 대표적인 방법인 Skip-Gram과 CBOW로 구현하겠습니다. 간단하게 요약하자면 다음과 같은 특징을 가지고 있었습니다.\n",
    "\n",
    "- CBOW는 주변단어를 이용해 주어진 단어를 예측하는 방법입니다.\n",
    "- Skip-Gram은 중심 단어를 이용하여 주변 단어를 예측하는 방법입니다. \n",
    "- 즉, 두 방법은 input과 output을 어떻게 설정하는지에 대한 차이가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 모델에 들어가기 위한 input을 만들기 위해 Dataset 클래스를 먼저 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, train_tokenized, window_size=2):\n",
    "        self.x = [] # input word\n",
    "        self.y = [] # output word\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i - window_size >= 0 and i + window_size < len(token_ids):\n",
    "                    self.x.append(token_ids[i - window_size:i] + token_ids[i + 1: i + window_size + 1])\n",
    "                    self.y.append(id)\n",
    "        self.x = torch.LongTensor(self.x) # (전체 데이터 개수, 2* window_size)\n",
    "        self.y = torch.LongTensor(self.y) # (전체 데이터 개수)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, train_tokenized, window_size=2):\n",
    "        self.x = [] # input word\n",
    "        self.y = [] # output word\n",
    "        \n",
    "        for tokens in tqdm(train_tokenized):\n",
    "            token_ids = [w2i[token] for token in tokens]\n",
    "            for i, id in enumerate(token_ids):\n",
    "                if i - window_size >= 0 and i + window_size < len(token_ids):\n",
    "                    self.y += (token_ids[i - window_size:i] + token_ids[i + 1: i + window_size + 1])\n",
    "                    self.x += [id] * 2 * window_size # id로 window_size 내 주변 단어를 모두 유추해야함\n",
    "        \n",
    "        self.x = torch.LongTensor(self.x) # (전체 데이터 개수)\n",
    "        self.y = torch.LongTensor(self.y) # (전체 데이터 개수)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 모델에 맞는 Dataset 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9988.82it/s]\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cbow_set = CBOWDataset(train_tokenized)\n",
    "skipgram_set = SkipGramDataset(train_tokenized)\n",
    "print(list(skipgram_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) 모델 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차례대로 두 가지 Word2Vec 모델을 구현하겠습니다.\n",
    "\n",
    "- self.embedding: vocab_size 크기의 one-hot vector를 특정 크기의 dim차원으로 embedding 시키는 layer\n",
    "- self.linear: 변환된 embedding vector를 다시 원래 vocab_size로 바꾸는 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "    \n",
    "    # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
    "    def forward(self, x): # x:(B, 2W)\n",
    "        embeddings = self.embedding(x) # (B, 2W, d_w)\n",
    "        embeddings = torch.sum(embeddings, dim=1) # (B, d_w)\n",
    "        output = self.linear(embeddings) # (B, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "    \n",
    "    # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
    "    def forward(self, x): # x:(B)\n",
    "        embeddings = self.embedding(x) # (B, d_w)\n",
    "        output = self.linear(embeddings) # (B, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
    "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 모델 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 하이퍼파라미터를 설정하고 DataLoader 객체를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
    "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 CBOW 모델을 학습하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 202.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.517364501953125\n",
      "Train loss: 5.722186088562012\n",
      "Train loss: 4.509489059448242\n",
      "Train loss: 4.71565055847168\n",
      "Train loss: 5.350027084350586\n",
      "Train loss: 4.911778926849365\n",
      "Train loss: 3.9825315475463867\n",
      "Train loss: 5.044991970062256\n",
      "Train loss: 4.438126564025879\n",
      "Train loss: 4.177204132080078\n",
      "Train loss: 4.603321075439453\n",
      "Train loss: 5.182714462280273\n",
      "Train loss: 4.771857738494873\n",
      "Train loss: 4.823988437652588\n",
      "Train loss: 4.422669410705566\n",
      "Train loss: 3.94539213180542\n",
      "##################################################\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 372.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.3733344078063965\n",
      "Train loss: 5.599576473236084\n",
      "Train loss: 4.3914265632629395\n",
      "Train loss: 4.599921703338623\n",
      "Train loss: 5.2240891456604\n",
      "Train loss: 4.6652021408081055\n",
      "Train loss: 3.808961868286133\n",
      "Train loss: 4.932811737060547\n",
      "Train loss: 4.332380294799805\n",
      "Train loss: 4.0147881507873535\n",
      "Train loss: 4.421172142028809\n",
      "Train loss: 4.863188743591309\n",
      "Train loss: 4.64410924911499\n",
      "Train loss: 4.702084064483643\n",
      "Train loss: 4.267542362213135\n",
      "Train loss: 3.8243963718414307\n",
      "##################################################\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 400.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.231768608093262\n",
      "Train loss: 5.478585243225098\n",
      "Train loss: 4.275032043457031\n",
      "Train loss: 4.485116004943848\n",
      "Train loss: 5.098927021026611\n",
      "Train loss: 4.427643775939941\n",
      "Train loss: 3.6395773887634277\n",
      "Train loss: 4.822075366973877\n",
      "Train loss: 4.22916316986084\n",
      "Train loss: 3.858006238937378\n",
      "Train loss: 4.246267795562744\n",
      "Train loss: 4.555294990539551\n",
      "Train loss: 4.518178939819336\n",
      "Train loss: 4.5822834968566895\n",
      "Train loss: 4.115230560302734\n",
      "Train loss: 3.706716537475586\n",
      "##################################################\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.092677116394043\n",
      "Train loss: 5.359189510345459\n",
      "Train loss: 4.160326957702637\n",
      "Train loss: 4.371255874633789\n",
      "Train loss: 4.97455358505249\n",
      "Train loss: 4.199728012084961\n",
      "Train loss: 3.4745688438415527\n",
      "Train loss: 4.712802410125732\n",
      "Train loss: 4.12832498550415\n",
      "Train loss: 3.706940174102783\n",
      "Train loss: 4.078704357147217\n",
      "Train loss: 4.260048866271973\n",
      "Train loss: 4.394067764282227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 355.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 4.464550018310547\n",
      "Train loss: 3.9659085273742676\n",
      "Train loss: 3.592409372329712\n",
      "##################################################\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 410.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.956080436706543\n",
      "Train loss: 5.241368293762207\n",
      "Train loss: 4.047332763671875\n",
      "Train loss: 4.258368492126465\n",
      "Train loss: 4.850984573364258\n",
      "Train loss: 3.9820339679718018\n",
      "Train loss: 3.314148187637329\n",
      "Train loss: 4.605008125305176\n",
      "Train loss: 4.029695510864258\n",
      "Train loss: 3.5615880489349365\n",
      "Train loss: 3.9187049865722656\n",
      "Train loss: 3.978358268737793\n",
      "Train loss: 4.271775245666504\n",
      "Train loss: 4.348851680755615\n",
      "Train loss: 3.8197689056396484\n",
      "Train loss: 3.4815196990966797\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cbow.train()\n",
    "cbow = cbow.to(device)\n",
    "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs + 1):\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Epoch: {e}\")\n",
    "    for batch in tqdm(cbow_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = cbow(x) # (B, V)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        print(f\"Train loss: {loss.item()}\")\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 Skip-Gram 모델을 학습하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 441.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.7030582427978516\n",
      "##################################################\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 481.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.6635866165161133\n",
      "##################################################\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 586.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.624561309814453\n",
      "##################################################\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 735.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.585994243621826\n",
      "##################################################\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 646.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.5478954315185547\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skipgram.train()\n",
    "skipgram = skipgram.to(device)\n",
    "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(1, num_epochs + 1):\n",
    "    print(\"#\" * 50)\n",
    "    print(f\"Epoch: {e}\")\n",
    "    for batch in tqdm(skipgram_loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
    "        output = skipgram(x) # (B, V)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    print(f\"Train loss: {loss.item()}\")\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5) 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 음식\n",
      "tensor([-7.8031e-01,  3.4020e-01,  2.3965e-01,  1.1466e+00, -4.7382e-01,\n",
      "        -1.1625e-01,  2.5119e+00,  6.0728e-01,  2.4680e+00,  1.2630e-01,\n",
      "        -2.7188e-02,  3.9423e-01,  9.8465e-01, -5.1491e-01,  9.3773e-01,\n",
      "        -2.0908e+00,  9.9917e-01, -2.5360e-01, -4.9429e-01,  3.4974e-01,\n",
      "        -7.9115e-01, -3.7272e-01, -7.2416e-01,  7.1037e-01,  1.3163e-01,\n",
      "         5.8675e-01, -9.7430e-02,  1.3822e+00, -2.8502e-01, -7.8537e-01,\n",
      "        -9.5123e-01, -1.0286e+00,  7.6688e-01, -3.2761e-01,  1.3801e-01,\n",
      "        -4.0390e-01, -8.6721e-01, -1.2883e+00,  1.1042e+00,  2.2619e+00,\n",
      "         9.7791e-02,  9.4427e-01,  8.9367e-01,  1.3734e+00,  2.7213e-01,\n",
      "        -7.3901e-01,  3.9341e-01,  6.8423e-01,  1.5472e+00, -1.0421e-01,\n",
      "        -7.3628e-01, -2.1654e+00,  2.4462e-01,  9.1695e-01, -4.6950e-02,\n",
      "         5.0610e-03, -4.6465e-01, -7.3774e-01, -1.1797e+00, -8.8331e-01,\n",
      "         4.4442e-01,  9.0978e-01,  2.5616e+00,  1.6349e-01,  9.5964e-02,\n",
      "        -8.8922e-02,  1.5621e+00,  5.1451e-01, -5.3047e-01, -7.1057e-01,\n",
      "         1.9713e+00,  1.9023e+00,  9.7000e-01,  1.6177e+00, -4.8513e-01,\n",
      "        -5.9215e-01, -6.2762e-01,  6.5475e-02, -7.2082e-01, -1.6420e-01,\n",
      "        -2.2280e-01,  1.2401e+00,  3.4138e-01, -1.2840e+00, -1.6350e+00,\n",
      "        -6.5896e-01, -3.0616e-01, -1.3532e+00,  1.0170e+00,  3.7837e-01,\n",
      "        -2.6662e-01,  1.8364e+00,  2.4980e-01,  1.3723e+00, -1.7900e-02,\n",
      "        -1.4385e-01,  1.9547e+00, -5.1830e-01,  3.8538e-01,  9.2184e-01,\n",
      "        -5.4997e-01, -4.7631e-02, -1.0276e-01,  1.2959e-01, -3.1721e-01,\n",
      "         2.7689e-01,  6.1655e-01, -2.1437e+00, -1.4720e+00, -8.0351e-01,\n",
      "        -3.1372e-01,  1.7194e+00, -1.9934e+00,  1.7667e-02, -4.0639e-02,\n",
      "         3.7951e-01,  2.5669e-01, -1.6582e+00, -1.1023e-03,  1.9758e+00,\n",
      "        -1.6434e+00, -8.2522e-03, -8.5538e-01,  9.7460e-01,  1.5014e+00,\n",
      "         1.0897e-01,  2.0340e-01, -9.7940e-02,  2.3217e+00, -2.8914e-01,\n",
      "         1.0236e+00, -4.7788e-01,  1.3557e+00, -9.1947e-01, -9.4932e-01,\n",
      "        -1.3046e+00,  2.5262e+00,  1.1113e+00, -1.8813e-01, -7.9594e-01,\n",
      "        -8.4787e-01,  1.6889e+00,  3.4893e-01,  1.2047e-01,  3.1440e-01,\n",
      "         5.1613e-01,  9.5289e-01, -3.4682e-02,  4.3395e-01, -1.2423e+00,\n",
      "        -1.5560e-01,  3.4323e-02, -1.8101e+00, -9.7124e-01, -4.2865e-01,\n",
      "         5.4273e-01,  4.7482e-01, -1.2503e+00,  6.8545e-01,  3.9855e-01,\n",
      "         1.6989e-01,  1.4513e+00, -2.6038e-01,  2.2006e-01, -5.2606e-01,\n",
      "         1.6740e-01,  7.6959e-01, -9.7723e-01,  1.3092e+00,  1.7030e+00,\n",
      "        -7.1313e-01,  5.6531e-02,  4.8902e-01,  3.2708e-02,  8.6103e-02,\n",
      "        -1.0802e+00, -1.0031e+00, -1.4843e-02, -5.7045e-01,  1.8234e-01,\n",
      "        -1.0143e+00, -4.3484e-01, -8.4060e-01, -3.5553e-01, -1.0867e+00,\n",
      "        -4.1910e-01,  2.9517e-01,  7.2969e-02,  9.1472e-01, -4.2685e-01,\n",
      "         1.2463e+00, -8.6465e-01, -1.0092e+00,  8.5423e-01,  1.2691e+00,\n",
      "         2.9808e+00, -3.2207e-01, -5.6910e-01, -8.9806e-01, -3.3563e-01,\n",
      "        -9.4357e-01,  3.5381e-01, -2.9078e-01, -1.2577e+00,  4.7150e-01,\n",
      "        -3.9397e-01, -1.1603e+00, -1.4829e+00,  7.2038e-01, -7.6397e-02,\n",
      "         3.6751e-01,  7.5057e-01,  2.9799e-01,  1.8045e-02,  1.1808e+00,\n",
      "         2.4704e+00,  1.2356e+00, -9.0351e-01, -6.5906e-01,  1.7421e+00,\n",
      "        -3.6901e-01, -4.2717e-01,  6.7428e-01, -1.3162e+00, -1.4041e+00,\n",
      "        -4.2978e-01, -3.7173e-01,  1.1054e+00, -6.6969e-02, -8.8388e-01,\n",
      "        -1.3366e+00, -1.6624e-01, -1.0108e+00, -1.7806e-01,  3.7672e-01,\n",
      "        -3.2116e-01, -1.0270e+00,  1.8875e+00,  2.1317e+00, -1.3088e+00,\n",
      "         9.4271e-01, -1.7540e+00,  1.6453e+00,  4.4708e-01, -1.1201e-01,\n",
      "         3.8833e-01,  1.1898e+00,  1.0451e+00, -1.2772e+00, -1.4186e-02,\n",
      "        -4.9965e-01, -1.4359e-01,  7.3072e-01,  1.1311e+00,  8.0893e-01,\n",
      "        -4.7645e-01], grad_fn=<SqueezeBackward1>)\n",
      "Word: 맛\n",
      "tensor([ 1.6901e-01, -1.0049e+00,  1.3480e+00,  1.5568e-01, -8.4420e-01,\n",
      "        -2.6936e-01,  3.4812e-01,  1.6002e-01, -1.4371e+00,  2.5747e-02,\n",
      "        -7.1336e-01,  1.6903e+00, -7.1813e-02,  1.6065e-01,  1.1732e+00,\n",
      "        -1.1890e-01,  1.3415e+00, -5.9285e-01, -4.0044e-02,  6.3289e-01,\n",
      "         1.7741e+00,  1.5503e+00,  1.6715e-01, -7.3411e-01,  9.1571e-01,\n",
      "        -2.4276e-02,  1.4787e-04,  3.8662e-01,  1.9371e-01, -2.5370e-01,\n",
      "         5.1686e-01,  4.7265e-01,  1.3874e+00,  1.6561e+00,  1.7488e-01,\n",
      "         3.1223e-01, -1.1457e+00,  6.7954e-01, -4.6328e-01,  1.0931e+00,\n",
      "        -1.8031e-01, -5.1148e-02, -1.1415e+00, -1.4564e+00, -2.1409e+00,\n",
      "         1.2218e+00, -1.4853e+00, -4.2371e-01,  5.8552e-02, -4.8069e-02,\n",
      "         6.2347e-01, -7.4163e-01,  1.5641e+00,  8.0746e-01, -1.8197e+00,\n",
      "        -9.8870e-01,  2.2681e-01,  6.7259e-01, -1.0937e+00,  4.1984e-01,\n",
      "         1.2354e-01,  7.9609e-01,  1.9227e+00,  5.8682e-01, -8.4621e-01,\n",
      "        -2.7316e-01, -8.2723e-01, -6.6811e-01, -1.6487e+00, -1.2285e+00,\n",
      "        -6.4610e-01,  9.6252e-01,  9.1570e-01,  2.0233e-01, -4.2204e-01,\n",
      "        -3.4222e-01, -1.4430e-01,  1.2569e+00,  4.5863e-01,  1.5787e+00,\n",
      "         2.6811e-01,  5.3112e-01,  9.0662e-01, -5.8903e-01, -2.3962e-01,\n",
      "        -2.3606e+00, -4.1559e-01, -1.5379e-01, -3.1408e-02,  5.9724e-01,\n",
      "         8.0533e-02, -5.4447e-01,  1.5772e-01, -9.6826e-02,  2.0465e-01,\n",
      "        -4.9360e-01, -8.1467e-01,  5.3285e-01, -1.1031e+00,  1.7485e-01,\n",
      "         7.2805e-01, -3.2677e+00,  3.1684e-01,  7.9653e-01, -1.0023e+00,\n",
      "         1.0586e+00, -6.0206e-01, -1.4041e+00, -2.9468e-01,  1.5900e+00,\n",
      "        -3.9742e-01,  7.7016e-01, -5.4905e-01,  6.6624e-01, -3.3869e-01,\n",
      "        -1.9782e-01,  3.2276e-01,  4.9011e-01,  4.9391e-01,  7.9482e-01,\n",
      "         9.5320e-01, -1.1591e+00,  2.5278e-01,  2.1663e+00, -3.8787e-01,\n",
      "         7.8097e-01, -6.6166e-01,  1.2409e+00,  3.1721e-01, -1.5158e-01,\n",
      "        -1.4952e+00,  3.0164e-01,  9.9046e-01, -6.1769e-01, -9.2716e-02,\n",
      "        -7.6265e-02, -8.3602e-01, -2.2131e+00, -1.1500e+00, -2.7803e+00,\n",
      "        -1.8718e+00, -3.0469e+00,  7.7717e-01,  2.6127e-01,  7.3508e-02,\n",
      "        -7.2526e-01, -1.1701e+00, -3.9109e-01, -9.1339e-01,  4.0621e-01,\n",
      "        -2.1739e+00, -2.3541e-02,  4.1758e-01,  1.8640e-01,  2.5454e-01,\n",
      "         5.5782e-01,  1.5423e+00,  8.9363e-03,  2.0581e+00,  1.3317e+00,\n",
      "         1.0482e+00,  9.7399e-01, -2.0417e+00, -1.0542e+00,  8.0093e-01,\n",
      "         1.3092e+00, -3.5777e-01, -1.4895e+00,  7.8532e-01,  1.4665e+00,\n",
      "        -8.4920e-01,  1.0046e+00,  1.1601e+00,  1.7930e-01, -4.6181e-01,\n",
      "         1.4162e-01,  1.0516e+00, -1.8358e-01,  2.9896e-03, -1.3641e-01,\n",
      "        -4.2434e-01, -3.7151e-01,  2.6657e+00, -1.3582e+00,  8.2084e-01,\n",
      "         5.9090e-01, -2.4172e+00,  4.8679e-01, -5.7218e-01,  7.2174e-01,\n",
      "        -8.7474e-01, -2.2682e-01, -6.6114e-01,  7.6970e-01, -1.2855e+00,\n",
      "         1.3430e-01,  1.6261e+00,  1.1888e+00, -1.6298e+00,  7.8212e-01,\n",
      "        -7.3374e-01, -2.5756e-01,  2.2563e-01,  5.8384e-01, -1.2601e+00,\n",
      "         1.3137e+00, -9.4048e-01,  3.6340e-01, -1.2359e+00, -8.0351e-01,\n",
      "        -3.6785e-01,  1.6215e+00, -5.9459e-01, -1.1406e+00,  7.3910e-01,\n",
      "        -5.2536e-01, -6.9264e-01,  5.5694e-01,  3.7339e-01,  1.2103e-01,\n",
      "         7.6081e-02, -5.3887e-01, -3.0115e-01,  5.5235e-01,  1.0532e+00,\n",
      "        -1.4490e+00,  1.5598e-01, -1.1971e+00, -1.2939e+00, -8.1929e-01,\n",
      "         5.9641e-01,  1.7379e+00, -7.7979e-01,  9.7815e-01,  1.3706e-01,\n",
      "        -7.2082e-01,  1.4945e+00,  5.1033e-02,  1.0608e+00, -4.7713e-01,\n",
      "        -5.0263e-01, -2.5994e-01, -8.5715e-01,  3.3785e-01,  1.5411e-01,\n",
      "         1.0512e+00, -4.5928e-01,  2.2648e-02,  4.8071e-01, -2.9716e-01,\n",
      "         4.0865e-01,  7.9106e-01, -3.5934e-01,  1.0250e-01, -1.1335e+00,\n",
      "        -7.7716e-01], grad_fn=<SqueezeBackward1>)\n",
      "Word: 서비스\n",
      "tensor([ 1.4786,  0.3905, -0.8332,  0.6824,  0.6825, -0.2595,  0.2169,  1.3872,\n",
      "        -0.0100, -0.1580,  1.7148,  1.0895, -0.3647, -0.1574, -0.7596,  0.2503,\n",
      "        -1.3668,  1.4090,  0.2179,  0.5259, -1.1860, -0.0156,  0.5940, -0.4192,\n",
      "        -0.0226, -0.5109, -0.4486, -0.3341, -0.1876,  0.7814,  0.4346,  0.1015,\n",
      "         0.4027,  0.0152,  1.4765,  0.3903, -0.9880,  0.1375, -0.1512, -0.4297,\n",
      "         0.2159,  1.7700,  0.7895,  0.6924,  0.4461, -0.2688, -0.4754, -1.2588,\n",
      "        -0.5341, -0.3940,  0.9390, -0.0104,  0.2510,  1.4521, -0.2193,  1.4028,\n",
      "        -1.3823,  2.2198, -0.4409,  0.9056,  1.4681, -0.8094, -0.2513,  0.4803,\n",
      "        -0.6269,  0.1240, -0.0824,  0.5644,  0.2763, -1.3303, -0.5262, -1.5006,\n",
      "        -0.4220,  0.7490,  0.7180, -0.1650, -1.1135,  0.0506,  0.4551, -1.1252,\n",
      "        -0.4166,  0.6934, -0.1839, -0.2096,  1.1315,  1.9822, -1.5004,  0.2958,\n",
      "        -0.2023,  0.3634,  1.1735, -0.8921, -2.3070, -1.4083,  0.7632,  0.8900,\n",
      "        -0.0410, -0.5672, -0.8005, -2.7274,  0.6574,  0.6026, -0.0748,  0.3155,\n",
      "         1.0800,  1.6752,  0.3543, -0.5470,  0.8532, -0.1549, -0.8645,  0.1838,\n",
      "        -0.0161, -0.6124,  1.9449,  0.2742, -0.8567,  0.0487,  0.4903,  0.4960,\n",
      "        -1.2039,  1.6423,  1.2369, -0.9319, -0.1591, -0.2267, -0.1603,  0.6340,\n",
      "        -0.8392,  0.2487, -0.6730, -1.3916,  0.2334, -0.3258,  1.2465,  0.9748,\n",
      "         0.2215, -1.0458,  1.0770,  0.1202, -0.5817, -2.0978,  1.9575, -0.1301,\n",
      "        -2.3554, -0.5285,  0.9435, -0.8807, -0.4890, -0.9258,  0.1643,  0.2921,\n",
      "         0.9330,  1.2865,  0.6115, -0.1013,  0.2073, -0.5822,  0.9370,  0.4972,\n",
      "        -1.2810,  0.3117, -0.0859,  0.4496,  0.0574, -0.2001, -1.4972, -0.5973,\n",
      "         0.1081, -1.3723, -0.6360, -0.4325,  1.1219,  0.2154,  0.0724, -0.9239,\n",
      "         0.1641, -0.3740, -0.7120,  0.1564, -1.5373,  1.1095, -0.7086, -0.0225,\n",
      "        -0.1747,  0.3444, -0.1286,  1.6360,  0.9330,  0.1188,  0.8471,  1.4425,\n",
      "         0.7179, -0.7834,  1.1145, -1.2957, -1.2940,  1.2295,  0.4106,  0.5870,\n",
      "        -0.7864,  0.6853, -0.0083,  0.3745, -0.5534, -1.4724,  1.0150, -0.8643,\n",
      "        -1.1095, -0.8150,  1.8162, -1.9256, -0.5253,  0.9136, -1.4277,  0.3404,\n",
      "         0.7384,  1.4030, -0.1202, -0.9031,  0.7609,  0.8396, -0.0690,  1.1332,\n",
      "        -0.9305, -0.6475,  0.7680, -2.6760, -0.2516,  0.6909, -0.3202, -1.7600,\n",
      "        -0.1745, -0.0107,  0.9714,  0.5449,  0.0936,  0.8449,  0.4360, -1.2196,\n",
      "         0.8882, -0.9520,  0.5715,  0.3791, -1.4208,  0.3457,  0.2081,  0.7361,\n",
      "        -0.4422,  0.3124, -0.0735,  0.8806, -0.8429,  1.7410, -0.2047, -2.0036],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 위생\n",
      "tensor([-0.1306, -0.5503,  0.5981,  1.8302,  0.1846, -1.8209,  0.5263, -0.3091,\n",
      "         0.0841, -1.3783, -0.5422,  1.2531, -1.0689, -0.6920,  1.1438, -1.3189,\n",
      "        -1.4783,  0.8767,  0.8526, -1.6881, -1.2899,  0.0588,  0.9375,  1.2213,\n",
      "        -0.5093, -2.4534,  1.3259, -0.0463,  1.1031,  0.7655,  0.2626, -0.5971,\n",
      "        -1.2813,  0.2130,  0.6178, -0.5844,  0.6627,  0.3429,  0.3647, -0.6265,\n",
      "         0.7559,  0.5838,  0.1278, -0.1372, -1.4410, -2.1312,  1.6435, -0.0296,\n",
      "        -0.2525,  0.0705,  0.9042,  1.2123, -0.6495,  0.2127,  0.5034, -0.4762,\n",
      "        -0.7084, -1.7937, -0.6218,  1.5456, -0.7402,  0.3890,  2.2705, -1.1000,\n",
      "        -0.4446,  0.1800, -1.1886,  0.8471, -0.5484, -1.7029, -0.2535, -0.0286,\n",
      "         1.2128, -0.4033,  2.0873,  1.1846, -0.7220, -0.6984, -0.8581, -0.0054,\n",
      "         0.2558, -1.0960,  0.4160,  0.3282, -0.4621, -0.7780,  0.2706, -0.2725,\n",
      "        -0.4773,  1.2636,  0.3878, -0.0492, -1.5415,  0.2287,  0.9904,  0.9204,\n",
      "        -1.1925,  0.4645,  0.3404, -0.1524, -0.5073, -1.0112, -1.6690, -0.7504,\n",
      "         0.8497,  1.1142, -1.6302, -0.2487, -0.1865,  1.7033, -0.8740,  0.4011,\n",
      "        -0.4040,  0.0094,  0.4626,  0.1769, -1.0019, -0.3839,  0.2870, -1.8927,\n",
      "         0.1181,  1.5805, -0.0747,  0.0530, -1.0567, -0.0885, -0.7684, -0.6548,\n",
      "        -0.8156,  0.4568,  0.0609,  0.4350, -0.4085, -0.8899, -0.6338, -1.6243,\n",
      "         0.6539,  0.3071,  0.2140, -0.3226,  0.3615, -0.2384, -1.1142, -0.4122,\n",
      "         0.1026, -0.8625,  0.2809, -1.1912,  1.6659,  0.9913,  0.6064, -0.0044,\n",
      "        -1.5937,  1.2577,  0.8393, -0.8690, -1.1631, -0.0654, -1.6048, -0.1612,\n",
      "        -1.7264, -0.8909, -0.8089, -1.7673,  0.2937, -1.0551, -0.1806,  1.1926,\n",
      "        -0.1650,  1.4575,  0.3186, -0.2004,  1.2739, -0.3049,  1.8466, -0.2369,\n",
      "        -1.1611,  0.4048, -0.7719, -0.9549,  0.2042, -0.6746, -0.3499, -0.2160,\n",
      "        -0.8772,  0.5782,  0.9718,  2.3043, -0.0473,  0.5502,  0.9040,  0.1089,\n",
      "         0.6408, -0.6027, -0.7139, -0.8962,  1.1385, -1.6694, -0.2347, -1.2821,\n",
      "        -1.1654, -0.4584,  0.4595,  0.1401, -1.4371, -1.5660,  0.2138,  0.6419,\n",
      "        -0.2066, -0.2180,  2.8650,  0.8050, -0.1723, -0.6224, -1.0556, -0.7432,\n",
      "        -1.4035,  0.0217, -1.0817, -1.5586, -0.5679, -0.1897, -0.0445, -1.5548,\n",
      "         1.0591,  0.5604, -0.6568, -0.7803, -1.0942,  1.8636,  1.3162, -0.2088,\n",
      "         0.7473,  0.2843, -0.6974, -0.0235, -0.7480, -0.4158,  0.0723, -0.0562,\n",
      "         0.5322,  0.6094, -1.4957,  0.0674,  2.0369,  1.6286,  0.3153,  1.5175,\n",
      "        -0.0265,  0.9406,  0.7995, -0.0992, -0.1685,  1.5488,  0.7992, -0.3007],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Word: 가격\n",
      "tensor([-8.0965e-01,  1.8518e+00,  7.0028e-01, -9.7737e-01,  1.1395e-01,\n",
      "         8.1721e-01,  1.8472e-01, -1.6783e+00, -9.1599e-01,  9.3096e-01,\n",
      "         3.4165e-01,  5.1518e-01, -6.7325e-01, -1.8944e+00,  2.6670e-01,\n",
      "        -5.0753e-01, -1.0888e+00,  1.7228e-01,  6.5019e-01,  4.2171e-01,\n",
      "         7.4153e-01, -1.4398e+00,  1.4322e+00, -4.2825e-02,  1.0888e-03,\n",
      "        -1.2548e+00, -1.2899e-01,  1.1182e-01, -1.3197e+00,  5.6138e-01,\n",
      "         2.7946e-01,  9.6595e-01,  1.3965e-01,  1.0532e+00,  4.3431e-01,\n",
      "         7.7353e-01, -8.8760e-01,  4.7291e-01,  2.0867e+00, -1.3279e-01,\n",
      "        -1.9776e-01, -1.5983e-01,  8.8798e-01,  2.0015e+00, -9.9080e-01,\n",
      "         3.7029e-01, -6.4019e-01, -1.0480e+00,  1.2524e-01, -3.3436e-01,\n",
      "        -9.7821e-01, -2.8486e+00, -1.4798e-01, -4.5723e-01,  1.3780e+00,\n",
      "         7.9205e-01, -6.8587e-01, -1.5576e-01,  1.1264e-01, -2.3960e+00,\n",
      "         1.5794e+00,  5.0979e-03, -9.8058e-01, -1.1610e+00,  1.5532e+00,\n",
      "         9.1733e-02,  1.3534e+00,  7.0596e-01, -5.3444e-01, -4.7658e-01,\n",
      "         2.0636e-01, -1.1733e+00, -5.1508e-01, -3.0931e-01, -4.1946e-01,\n",
      "         3.4572e-01, -5.8687e-01, -1.6034e+00, -1.1595e+00, -6.1469e-01,\n",
      "        -1.3930e+00, -2.1698e-01,  5.4527e-02, -2.5565e+00,  4.4855e-01,\n",
      "        -1.0519e+00, -3.0263e-01,  2.0952e-01, -5.7367e-01,  1.3690e-01,\n",
      "         1.9352e+00, -2.0027e-02,  1.9303e-01, -5.1098e-01, -4.6199e-01,\n",
      "         2.6107e-01, -3.9752e-01,  2.5428e+00, -1.7225e+00,  1.6051e+00,\n",
      "         1.0206e+00, -1.0391e+00, -3.4299e-01,  5.2178e-01, -8.9704e-01,\n",
      "        -8.4118e-01,  7.5761e-01, -1.4196e+00, -8.5993e-02, -1.2593e+00,\n",
      "        -1.1290e-01,  2.2576e-01, -5.4540e-01,  6.1779e-01,  1.5775e+00,\n",
      "        -9.7427e-01, -5.5002e-01,  4.9793e-01, -1.2230e+00, -5.1931e-01,\n",
      "         2.1633e+00, -9.0222e-01, -7.0338e-01, -9.2751e-01, -3.3058e-01,\n",
      "        -1.6480e+00, -2.2832e-01, -1.2155e-01, -8.7960e-01, -6.4992e-01,\n",
      "         1.3998e+00,  1.7956e-01, -1.0770e-01,  1.0948e+00,  1.9888e-01,\n",
      "         8.2993e-01,  2.3199e+00,  4.7803e-01,  2.9562e-02, -5.5670e-01,\n",
      "         4.7429e-01,  7.0182e-01, -8.2699e-01, -6.6544e-01,  8.4800e-01,\n",
      "         2.2838e-01,  1.2905e+00, -4.4617e-01,  6.3598e-01, -7.1121e-01,\n",
      "        -5.9919e-01,  1.8028e+00,  6.4534e-01,  1.1291e+00, -8.1661e-01,\n",
      "         1.0726e-03, -1.2488e+00,  1.2401e+00, -1.9581e+00,  6.5370e-01,\n",
      "         1.0659e+00,  8.5115e-01,  1.7011e+00,  3.4682e-01,  1.5328e+00,\n",
      "         9.6251e-01,  1.7253e+00, -1.0591e+00, -1.5345e+00, -2.0918e-01,\n",
      "        -3.9503e-01,  3.4039e-01,  9.8129e-01, -9.9683e-01, -2.3404e+00,\n",
      "         6.5583e-01, -8.4204e-01, -1.5520e+00, -1.6055e+00,  1.1070e+00,\n",
      "        -3.6491e-02, -1.1632e-01,  8.8725e-01, -7.8314e-01,  1.5193e-01,\n",
      "         6.1178e-01, -1.1269e-01,  4.8437e-01, -3.4669e-01, -7.0832e-01,\n",
      "        -6.9608e-01, -2.1627e+00, -2.4018e-01, -4.2566e-01, -1.3250e-01,\n",
      "        -5.5337e-01, -6.8013e-02,  1.1942e-01,  5.2750e-02,  1.6676e+00,\n",
      "        -9.0238e-01,  2.2026e-01, -1.2480e+00,  4.2234e-01, -1.4971e-01,\n",
      "         6.1410e-01,  3.3572e-01,  4.5658e-01, -5.6234e-01,  1.0450e+00,\n",
      "         6.0659e-01, -1.1285e+00, -1.6710e-02, -9.6300e-01,  2.6695e-01,\n",
      "        -5.5280e-01, -5.3736e-01,  9.8844e-01, -4.8514e-01,  2.0609e-01,\n",
      "        -1.4276e+00, -3.3616e-01,  6.7304e-01, -1.4876e+00,  1.2799e+00,\n",
      "        -4.5736e-01, -3.6241e-01, -4.4477e-01, -9.8067e-01,  4.0120e-01,\n",
      "        -1.0357e+00,  1.3047e+00, -2.1641e+00,  2.6026e-01,  1.3470e+00,\n",
      "         2.6431e-01,  9.2605e-01, -9.1644e-01,  8.6914e-01, -1.5532e+00,\n",
      "         1.0205e+00,  1.1146e+00,  6.0641e-02,  8.7321e-02,  1.7003e+00,\n",
      "         2.3906e-01,  8.5954e-01, -9.2422e-01, -4.6875e-02, -4.9152e-01,\n",
      "        -1.7092e-01,  4.2116e-02,  2.1851e-01, -1.0242e+00,  4.1510e-01,\n",
      "         2.1404e-02], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = cbow.embedding(input_id)\n",
    "    \n",
    "    print(f\"Word: {word}\")\n",
    "    print(emb.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 음식\n",
      "tensor(2.7317, grad_fn=<UnbindBackward0>)\n",
      "Word: 맛\n",
      "tensor(2.5442, grad_fn=<UnbindBackward0>)\n",
      "Word: 서비스\n",
      "tensor(3.7710, grad_fn=<UnbindBackward0>)\n",
      "Word: 위생\n",
      "tensor(2.9354, grad_fn=<UnbindBackward0>)\n",
      "Word: 가격\n",
      "tensor(3.1709, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    emb = skipgram.embedding(input_id)\n",
    "    \n",
    "    print(f\"Word: {word}\")\n",
    "    print(max(emb.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['음식', '맛', '서비스', '위생', '가격']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, top_k=5):\n",
    "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
    "    input_emb = skipgram.embedding(input_id)\n",
    "    score = torch.matmul(input_emb, skipgram.embedding.weight.transpose(1, 0)).view(-1)\n",
    "    \n",
    "    _, top_k_ids = torch.topk(score, top_k)\n",
    "    return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['있다', '완전', '정말', '더']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('가격')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6) Word2Vec 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib 패키지 한글 깨짐 처리\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "# plt.rc('font', family='AppleGothic) # mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Skip-Gram 결과**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_weight = pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for word_id, (x_coordinate, y_coordinate) in enumerate(pc_weight):\n",
    "    plt.scatter(x_coordinate, y_coordinate, color=\"blue\")\n",
    "    plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CBOW 결과**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_weight = pca.fit_transform(cbow.embedding.weight.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delphinus\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for word_id, (x_coordinate, y_coordinate) in enumerate(pc_weight):\n",
    "    plt.scatter(x_coordinate, y_coordinate, color=\"blue\")\n",
    "    plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. RNNs with Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'3. Deep Learning'에서 배운 Seq2Seq와 Attention에 대해서 간략하게 복습 및 보충하겠습니다. 자세한 건 '3. Deep Learning'을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Sequence to Sequecne**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-4-1](_image/4-4-1.PNG)\n",
    "\n",
    "위 그림을 보면 알 수 있듯이 시퀀스 투 시퀀스는 입력을 받는 인코더와 출력하는 디코더로  이루어져있습니다. 그리고 인코더와 디코더는 각각의 RNN, LSTM, GRU 중 하나로 구성되어 있습니다. \n",
    "\n",
    "문장의 의미를 더욱 정확하게 파악하기 위해서 양방향 RNN을 사용할 수도 있습니다. 문장을 순방향으로 읽는 RNN과 역방향으로 읽는 RNN을 이용해 나온 최종 은닉 상태를 합하여 사용합니다. 이때 디코더는 순방향으로 하나씩 출력해야하므로 디코더에선 양방향을 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq는 큰 문제점을 하나 가지고 있었습니다. 마지막 은닉 상태가 문장의 모든 의미를 함축해야 하는데 문장의 길이가 길어질수록 그것이 불가능하다는 것입니다. 즉, 앞쪽의 의미는 잊어버리게 됩니다. 이를 방지하기 위해 우리는 Attention이란 방법을 도입했었습니다.\n",
    "\n",
    "간단하게 말하면 인코더에서 구한 은닉 상태의 결과들을 디코더가 필요한 결과들을 위주로 사용하는 방법이었습니다. 예를 들어 곱셈 기반의 attention을 보겠습니다.\n",
    "\n",
    "![4-4-2](_image/4-4-2.PNG)\n",
    "\n",
    "먼저 인코더에서 구해진 은닉 상태를 다 저장합니다. 이제 디코더에서 은닉 상태를 거칠 때, 인코더의 은닉 상태들과 내적으로 유사도를 구하여 소프트맥스합니다. 그리고 그 비율을 각각 인코더의 은닉 상태에 곱하여 합한 뒤, 구해진 값을 디코더의 추가 입력값으로 사용합니다.\n",
    "\n",
    "예를 들어 기계 번역에선 문장에서 나와야 할 품사가 동일할 때 가중치가 더 높게 측정되는 것입니다. \n",
    "\n",
    "단어마다 어떤 단어에 가중치가 주어졌는지 그래프로 그려보면 다음과 같습니다. 가로축은 input, 세로축은 output 단어들입니다.\n",
    "\n",
    "![4-4-3](_image/4-4-3.PNG)\n",
    "\n",
    "(a)를 보면 중간에 어순이 바뀐 것을 확인할 수 있습니다. 또한 (b)를 보면 어떤 언어에선 하나의 단어가 다른 언어에선 여러 단어로 표현되는 것을 확인할 수 있습니다. \n",
    "\n",
    "이 이외도 다른 attention 방법이 있습니다. 예를 들어 concat 기반의 어텐션이 있습니다. 이는 인코더의 각각 은닉 상태들을 디코더의 은닉 상태와 concat하여 fc를 거치게 하고 하나의 노드를 출력으로 받습니다. 모든 인코더의 은닉 상태에게 적용하여 나온 값들을 가중치로 사용하여 적용하는 방법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습1. Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습은 Seq2Seq 모델을 구현하고 attention 모듈을 추가해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1) Seq2Seq with toy example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src_data를 trg_data로 바꾸는 번역 task를 수행하기 위한 sample data를 준비합니다.\n",
    "\n",
    "전체 단어 수는 100개이고 다음과 같이 pad token, start, token, end token의 id도 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "sos_id = 1 # <sos> : start of sentence\n",
    "eos_id = 2 # <eos> : end of sentnece\n",
    "\n",
    "src_data = [\n",
    "    [3, 77, 56, 26, 3, 55, 12, 36, 31],\n",
    "    [58, 20, 65, 46, 26, 10, 76, 44],\n",
    "    [58, 17, 8],\n",
    "    [59],\n",
    "    [29, 3, 52, 74, 73, 51, 39, 75, 19],\n",
    "    [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93],\n",
    "    [39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99, 5],\n",
    "    [75, 34, 17, 3, 86, 88],\n",
    "    [63, 39, 5, 35, 67, 56, 68, 89, 55, 66],\n",
    "    [12, 40, 69, 39, 49]\n",
    "]\n",
    "\n",
    "trg_data = [\n",
    "    [75, 13, 22, 77, 89, 21, 13, 86, 95],\n",
    "    [79, 14, 91, 41, 32, 79, 88, 34, 8, 68, 32, 77, 58, 7, 9, 87],\n",
    "    [85, 8, 50, 30],\n",
    "    [47, 30],\n",
    "    [8, 85, 87, 77, 47, 21, 23, 98, 83, 4, 47, 97, 40, 43, 70, 8, 65, 71, 69, 88],\n",
    "    [32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18],\n",
    "    [37, 14, 49, 24, 93, 37, 54, 51, 39, 84],\n",
    "    [16, 98, 68, 57, 55, 46, 66, 85, 18],\n",
    "    [20, 70, 14, 6, 58, 90, 30, 17, 91, 18, 90],\n",
    "    [37, 93, 98, 13, 45, 28, 89, 72, 70]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# target data의 각 문장의 시작과 끝에 <sos> token id와 <eos> token id를 추가합니다.\n",
    "trg_data = [[sos_id] + seq + [eos_id] for seq in tqdm(trg_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장의 길이를 동일하게 맞춰주기 위해 <pad> token id를 추가하는 padding 함수를 정의합니다.\n",
    "def padding(data):\n",
    "    max_len = len(max(data, key=len))\n",
    "    print(f\"Maximum sequence length: {max_len}\")\n",
    "    \n",
    "    valid_lens = []\n",
    "    for i, seq in enumerate(tqdm(data)):\n",
    "        valid_lens.append(len(seq))\n",
    "        if len(seq) < max_len:\n",
    "            data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "    \n",
    "    return data, valid_lens, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10043.83it/s]\n"
     ]
    }
   ],
   "source": [
    "src_data, src_lens, src_max_len = padding(src_data)\n",
    "trg_data, trg_lens, trg_max_len = padding(trg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 15])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 22])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# B: batch size,\n",
    "# S_L: source maximum sequence length\n",
    "# T_L: target maximum sequence length\n",
    "\n",
    "src_batch = torch.LongTensor(src_data) # (B, S_L)\n",
    "src_batch_lens = torch.LongTensor(src_lens) # (B)\n",
    "trg_batch = torch.LongTensor(trg_data) # (B, T_L)\n",
    "trg_batch_lens = torch.LongTensor(trg_lens) # (B)\n",
    "\n",
    "print(src_batch.shape)\n",
    "print(src_batch_lens.shape)\n",
    "print(trg_batch.shape)\n",
    "print(trg_batch_lens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39, 47, 96, 68, 55, 16, 90, 45, 89, 84, 19, 22, 32, 99,  5],\n",
      "        [41, 55, 77, 21, 52, 92, 97, 69, 54, 14, 93,  0,  0,  0,  0],\n",
      "        [63, 39,  5, 35, 67, 56, 68, 89, 55, 66,  0,  0,  0,  0,  0],\n",
      "        [ 3, 77, 56, 26,  3, 55, 12, 36, 31,  0,  0,  0,  0,  0,  0],\n",
      "        [29,  3, 52, 74, 73, 51, 39, 75, 19,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 20, 65, 46, 26, 10, 76, 44,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [75, 34, 17,  3, 86, 88,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [12, 40, 69, 39, 49,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [58, 17,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [59,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([15, 11, 10,  9,  9,  8,  6,  5,  3,  1])\n",
      "tensor([[ 1, 37, 14, 49, 24, 93, 37, 54, 51, 39, 84,  2,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 32, 37, 31, 77, 38, 93, 45, 74, 47, 54, 31, 18,  2,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 20, 70, 14,  6, 58, 90, 30, 17, 91, 18, 90,  2,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 75, 13, 22, 77, 89, 21, 13, 86, 95,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1,  8, 85, 87, 77, 47, 21, 23, 98, 83,  4, 47, 97, 40, 43, 70,  8, 65,\n",
      "         71, 69, 88,  2],\n",
      "        [ 1, 79, 14, 91, 41, 32, 79, 88, 34,  8, 68, 32, 77, 58,  7,  9, 87,  2,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 16, 98, 68, 57, 55, 46, 66, 85, 18,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 37, 93, 98, 13, 45, 28, 89, 72, 70,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 85,  8, 50, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 1, 47, 30,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "tensor([12, 14, 13, 11, 22, 18, 11, 11,  6,  4])\n"
     ]
    }
   ],
   "source": [
    "src_batch_lens, sorted_idx = src_batch_lens.sort(descending=True)\n",
    "src_batch = src_batch[sorted_idx]\n",
    "trg_batch = trg_batch[sorted_idx]\n",
    "trg_batch_lens = trg_batch_lens[sorted_idx]\n",
    "\n",
    "print(src_batch)\n",
    "print(src_batch_lens)\n",
    "print(trg_batch)\n",
    "print(trg_batch_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder 구현\n",
    "- Embedding layer, output layer, GRU cell을 포함한 encoder 모듈을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "num_dirs = 2 # 2 if bidirectional=True otherwise 1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True if num_dirs > 1 else False,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.linear = nn.Linear(num_dirs * hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, batch, batch_lens): # batch: (B, S_L), batch_lens: (B)\n",
    "        # d_w: word embedding size\n",
    "        batch_emb = self.embedding(batch) # (B, S_L, d_w)\n",
    "        batch_emb = batch_emb.transpose(0, 1) # (S_L, B, d_w)\n",
    "        \n",
    "        packed_input = pack_padded_sequence(batch_emb, batch_lens)\n",
    "        \n",
    "        h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size)) # (num_dirs*num_layers, B, d_h) = (4, B, d_h)\n",
    "        packed_outputs, h_n = self.gru(packed_input, h_0) # h_n: (4, B, d_h)\n",
    "        outputs = pad_packed_sequence(packed_outputs)[0] # outputs: (S_L, B, 2d_h)\n",
    "        outputs = torch.tanh(self.linear(outputs)) # (S_L, B, d_h)\n",
    "        \n",
    "        forward_hidden = h_n[-2, :, :]\n",
    "        backward_hidden = h_n[-1, :, :]\n",
    "        hidden = torch.tanh(self.linear(torch.cat((forward_hidden, backward_hidden), dim=-1))).unsqueeze(0) # (1, B, d_h)\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) Dot-product Attention 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention 중 대표적 형태인 dot-product attention을 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs): # (1, B, d_h), (S_L, B, d_h)\n",
    "        query = decoder_hidden.squeeze(0) # (B, d_h)\n",
    "        key = encoder_outputs.transpose(0, 1) # (B, S_L, d_h)\n",
    "        \n",
    "        energy = torch.sum(torch.mul(key, query.unsqueeze(1)), dim=-1) # (B, S_L)\n",
    "        \n",
    "        attn_scores = F.softmax(energy, dim=-1) # (B, S_L)\n",
    "        attn_values = torch.sum(torch.mul(encoder_outputs.transpose(0, 1), attn_scores.unsqueeze(2)), dim=1) # (B, d_h)\n",
    "        \n",
    "        return attn_values, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_attn = DotAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 attention 모듈을 가지는 decoder 클래스를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "        )\n",
    "        self.output_linear = nn.Linear(2 * hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, batch, encoder_outputs, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch : (B)\n",
    "            encoder_outputs: (L, B, d_h)\n",
    "            hidden: (1, B, d_h)\n",
    "        \"\"\"\n",
    "        batch_emb = self.embedding(batch) # (B, d_w)\n",
    "        batch_emb = batch_emb.unsqueeze(0) # (1, B, d_w)\n",
    "        \n",
    "        outputs, hidden = self.rnn(batch_emb, hidden) # (1, B, d_h), (1, B, d_h)\n",
    "        attn_values, attn_scores = self.attention(hidden, encoder_outputs) # (B, d_h), (B, S_L)\n",
    "        concat_outputs = torch.cat((outputs, attn_values.unsqueeze(0)), dim=-1) # (1, B, 2d_h)\n",
    "        \n",
    "        return self.output_linear(concat_outputs).squeeze(0), hidden # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(dot_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) Seq2Seq 모델 구축**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_batch, src_batch_lens, trg_batch, teacher_forcing_prob=0.5):\n",
    "        # src_batch: (B, S_L), src_batch_lens: (B), trg_batch: (B, T_L)\n",
    "        \n",
    "        # encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\n",
    "        encoder_outputs, hidden = self.encoder(src_batch, src_batch_lens) \n",
    "        \n",
    "        input_ids = trg_batch[:, 0] # (B)\n",
    "        batch_size = src_batch.shape[0]\n",
    "        outputs = torch.zeros(trg_max_len, batch_size, vocab_size) # (T_L, B, V)\n",
    "        \n",
    "        for t in range(1, trg_max_len):\n",
    "            # decoder_outputs: (B, V), hidden: (1, B, d_h)\n",
    "            decoder_outputs, hidden = self.decoder(input_ids, encoder_outputs, hidden)\n",
    "            \n",
    "            outputs[t] = decoder_outputs\n",
    "            _, top_ids = torch.max(decoder_outputs, dim=-1) # top_ids: (B)\n",
    "            \n",
    "            input_ids = trg_batch[:, t] if random.random() > teacher_forcing_prob else top_ids\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 모델 사용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0113, -0.0776,  0.0139,  ...,  0.0056, -0.0097, -0.0801],\n",
      "         [-0.0295, -0.0791,  0.0305,  ..., -0.0422, -0.0008, -0.0755],\n",
      "         [-0.0770, -0.1050,  0.0398,  ..., -0.0124, -0.0159, -0.0521],\n",
      "         ...,\n",
      "         [-0.0363, -0.0961,  0.0102,  ..., -0.0386, -0.0404, -0.0805],\n",
      "         [-0.0198, -0.1032,  0.0158,  ..., -0.0103, -0.0457, -0.0972],\n",
      "         [-0.0141, -0.1064,  0.0251,  ..., -0.0305, -0.0023, -0.0846]],\n",
      "\n",
      "        [[ 0.0427, -0.0626,  0.0652,  ..., -0.0445, -0.1404, -0.0828],\n",
      "         [-0.0090,  0.0018,  0.0606,  ..., -0.1247, -0.0898, -0.0371],\n",
      "         [ 0.0400, -0.0426,  0.0570,  ..., -0.0639,  0.0080, -0.0535],\n",
      "         ...,\n",
      "         [ 0.0221, -0.0685,  0.0429,  ..., -0.0961, -0.1682, -0.0762],\n",
      "         [-0.0140, -0.0055,  0.0220,  ...,  0.0548, -0.0277,  0.0350],\n",
      "         [ 0.0175,  0.0545, -0.0776,  ...,  0.0275, -0.0391, -0.1087]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1123, -0.1323,  0.3348,  ..., -0.1114,  0.0595, -0.3055],\n",
      "         [-0.1401, -0.1335,  0.3203,  ..., -0.1237,  0.0749, -0.2965],\n",
      "         [-0.1697, -0.1395,  0.3247,  ..., -0.0982,  0.0859, -0.2972],\n",
      "         ...,\n",
      "         [-0.1374, -0.1166,  0.3214,  ..., -0.1430,  0.0663, -0.3024],\n",
      "         [-0.1298, -0.1635,  0.3342,  ..., -0.1087,  0.0834, -0.3199],\n",
      "         [-0.1252, -0.1599,  0.3381,  ..., -0.1085,  0.0787, -0.3120]],\n",
      "\n",
      "        [[-0.1170, -0.1346,  0.3471,  ..., -0.1130,  0.0640, -0.3137],\n",
      "         [-0.1427, -0.1327,  0.3358,  ..., -0.1319,  0.0704, -0.3119],\n",
      "         [-0.1686, -0.1487,  0.3450,  ..., -0.1079,  0.0873, -0.3069],\n",
      "         ...,\n",
      "         [-0.1363, -0.1244,  0.3355,  ..., -0.1428,  0.0645, -0.3107],\n",
      "         [-0.1334, -0.1608,  0.3464,  ..., -0.1179,  0.0813, -0.3257],\n",
      "         [-0.1288, -0.1576,  0.3500,  ..., -0.1175,  0.0768, -0.3178]],\n",
      "\n",
      "        [[ 0.0423, -0.1345,  0.1009,  ...,  0.0794, -0.0268, -0.1687],\n",
      "         [ 0.0183, -0.1277,  0.0908,  ...,  0.0513, -0.0287, -0.1724],\n",
      "         [-0.0017, -0.1473,  0.1037,  ...,  0.0729, -0.0064, -0.1639],\n",
      "         ...,\n",
      "         [ 0.0216, -0.1262,  0.0957,  ...,  0.0489, -0.0264, -0.1686],\n",
      "         [ 0.0233, -0.1521,  0.1011,  ...,  0.0645, -0.0156, -0.1803],\n",
      "         [ 0.0273, -0.1501,  0.1052,  ...,  0.0639, -0.0195, -0.1736]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# V: vocab size\n",
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch) # (T_L, B, V)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent = [4, 10, 88, 46, 72, 34, 14, 51]\n",
    "sample_len = len(sample_sent)\n",
    "\n",
    "sample_batch = torch.LongTensor(sample_sent).unsqueeze(0) # (1, L)\n",
    "sample_batch_len = torch.LongTensor([sample_len]) # (1)\n",
    "\n",
    "encoder_output, hidden = seq2seq.encoder(sample_batch, sample_batch_len) # hidden: (4, 1, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = torch.LongTensor([sos_id]) # (1)\n",
    "output = []\n",
    "\n",
    "for t in range(1, trg_max_len):\n",
    "    # decoder_output: (1, V), hidden: (4, 1, d_h)\n",
    "    decoder_output, hidden = seq2seq.decoder(input_id, encoder_output, hidden)\n",
    "    \n",
    "    _, top_id = torch.max(decoder_output, dim=-1) # top_ids: (1)\n",
    "    \n",
    "    if top_id == eos_id:\n",
    "        break\n",
    "    else:\n",
    "        output += top_id.tolist()\n",
    "        input_id = top_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 23,\n",
       " 7,\n",
       " 10,\n",
       " 83,\n",
       " 83,\n",
       " 59,\n",
       " 47,\n",
       " 67,\n",
       " 18,\n",
       " 40,\n",
       " 40,\n",
       " 88,\n",
       " 59,\n",
       " 40,\n",
       " 62,\n",
       " 40,\n",
       " 59,\n",
       " 92,\n",
       " 92,\n",
       " 18]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) Concat Attention 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahdanau Attention이라고도 불리는 Concat Attention을 구현해보겠습니다.\n",
    "- self.w: concat한 query와 key 벡터를 1차적으로 linear transformation.\n",
    "- self.v: attention logit 값을 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs): # (1, B, d_h), (S_L, B, d_h)\n",
    "        src_max_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        decoder_hidden = decoder_hidden.transpose(0, 1).repeat(1, src_max_len, 1) # (B, S_L, d_h)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1) # (B, S_L, d_h)\n",
    "        \n",
    "        concat_hiddens = torch.cat((decoder_hidden, encoder_outputs), dim=2) # (B, S_L, 2d_h)\n",
    "        energy = torch.tanh(self.w(concat_hiddens)) # (B, S_L, d_h)\n",
    "        \n",
    "        attn_scores = F.softmax(self.v(energy), dim=1) # (B, S_L, 1)\n",
    "        attn_values = torch.sum(torch.mul(encoder_outputs, attn_scores), dim=1) # (B, d_h)\n",
    "        \n",
    "        return attn_values, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_attn = ConcatAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_size + hidden_size,\n",
    "            hidden_size,\n",
    "        )\n",
    "        self.output_linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, batch, encoder_outputs, hidden): \n",
    "        # batch: (B), encoder_outputs: (S_L, B, d_h), hidden: (1, B, d_h)\n",
    "        batch_emb = self.embedding(batch) # (B, d_w)\n",
    "        batch_emb = batch_emb.unsqueeze(0) # (1, B, d_w)\n",
    "        \n",
    "        attn_values, attn_scores = self.attention(hidden, encoder_outputs) # (B, d_h), (B, S_L)\n",
    "        concat_emb = torch.cat((batch_emb, attn_values.unsqueeze(0)), dim=-1) # (1, B, d_w + d_h)\n",
    "        outputs, hidden = self.rnn(concat_emb) # (1, B, d_h), (1, B, d_h)\n",
    "        \n",
    "        return self.output_linear(outputs).squeeze(0), hidden # (B, V), (1, B, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(concat_attn)\n",
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-1.9526e-02,  1.1674e-01,  1.6659e-01,  ..., -2.2380e-02,\n",
      "          -9.3701e-02,  6.6963e-02],\n",
      "         [-1.3735e-02,  1.1081e-01,  1.6176e-01,  ..., -1.6449e-02,\n",
      "          -9.6613e-02,  7.5349e-02],\n",
      "         [-1.5554e-02,  1.1270e-01,  1.6276e-01,  ..., -2.8512e-02,\n",
      "          -9.9880e-02,  7.8048e-02],\n",
      "         ...,\n",
      "         [-1.8401e-02,  1.1381e-01,  1.5651e-01,  ..., -2.7389e-02,\n",
      "          -9.6269e-02,  8.6183e-02],\n",
      "         [-1.5361e-02,  1.1114e-01,  1.5898e-01,  ..., -2.9307e-02,\n",
      "          -9.0788e-02,  8.4057e-02],\n",
      "         [-1.7702e-02,  1.1069e-01,  1.5592e-01,  ..., -2.6148e-02,\n",
      "          -9.2098e-02,  8.4750e-02]],\n",
      "\n",
      "        [[-1.3795e-02,  5.6028e-02,  1.1478e-01,  ..., -1.4282e-02,\n",
      "           5.0719e-03,  7.2712e-02],\n",
      "         [-1.3543e-02,  5.3773e-02,  1.1540e-01,  ..., -1.2732e-02,\n",
      "           7.7541e-04,  8.4071e-02],\n",
      "         [-1.1047e-02,  5.3852e-02,  1.1162e-01,  ..., -2.2373e-02,\n",
      "          -3.2122e-04,  8.1539e-02],\n",
      "         ...,\n",
      "         [-1.2340e-02,  5.3527e-02,  1.0558e-01,  ..., -2.2595e-02,\n",
      "           9.3396e-04,  9.0009e-02],\n",
      "         [-1.3516e-02,  5.2301e-02,  1.0631e-01,  ..., -2.4633e-02,\n",
      "           3.7673e-03,  8.6534e-02],\n",
      "         [-1.4959e-02,  5.0545e-02,  1.0385e-01,  ..., -2.2370e-02,\n",
      "           1.5872e-03,  8.6337e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0462e-01,  1.2648e-02,  1.4776e-01,  ...,  1.5499e-02,\n",
      "           3.5589e-02, -1.6101e-01],\n",
      "         [-1.0555e-01,  1.0302e-02,  1.4038e-01,  ...,  1.8804e-02,\n",
      "           3.2270e-02, -1.4994e-01],\n",
      "         [-1.0695e-01,  1.2397e-02,  1.4229e-01,  ...,  8.0341e-03,\n",
      "           3.3431e-02, -1.5234e-01],\n",
      "         ...,\n",
      "         [-1.0689e-01,  1.3973e-02,  1.3641e-01,  ...,  9.1950e-03,\n",
      "           3.6688e-02, -1.4216e-01],\n",
      "         [-1.0852e-01,  1.1662e-02,  1.3719e-01,  ...,  8.0198e-03,\n",
      "           4.0715e-02, -1.4465e-01],\n",
      "         [-1.0993e-01,  1.1202e-02,  1.3443e-01,  ...,  1.0746e-02,\n",
      "           3.8316e-02, -1.4420e-01]],\n",
      "\n",
      "        [[-4.3686e-02, -2.6546e-01, -1.6880e-01,  ..., -8.8381e-02,\n",
      "           2.2960e-03, -1.0207e-01],\n",
      "         [-4.1934e-02, -2.6840e-01, -1.7258e-01,  ..., -7.7625e-02,\n",
      "          -2.0135e-03, -9.7329e-02],\n",
      "         [-4.2075e-02, -2.6836e-01, -1.7181e-01,  ..., -8.9395e-02,\n",
      "           8.4858e-05, -9.6919e-02],\n",
      "         ...,\n",
      "         [-4.3590e-02, -2.6756e-01, -1.7398e-01,  ..., -8.8556e-02,\n",
      "           1.1597e-03, -9.1041e-02],\n",
      "         [-4.2943e-02, -2.6881e-01, -1.7420e-01,  ..., -8.7583e-02,\n",
      "           4.4861e-03, -9.3757e-02],\n",
      "         [-4.4039e-02, -2.6950e-01, -1.7618e-01,  ..., -8.5350e-02,\n",
      "           3.1042e-03, -9.3565e-02]],\n",
      "\n",
      "        [[ 8.5264e-02, -1.4816e-01, -1.1402e-01,  ...,  1.2730e-01,\n",
      "           6.8443e-02,  1.7028e-01],\n",
      "         [ 8.5304e-02, -1.5023e-01, -1.1828e-01,  ...,  1.2890e-01,\n",
      "           6.6891e-02,  1.7767e-01],\n",
      "         [ 8.6687e-02, -1.5200e-01, -1.1874e-01,  ...,  1.2048e-01,\n",
      "           6.1046e-02,  1.8087e-01],\n",
      "         ...,\n",
      "         [ 8.5590e-02, -1.5087e-01, -1.2387e-01,  ...,  1.1889e-01,\n",
      "           6.6544e-02,  1.8754e-01],\n",
      "         [ 8.4544e-02, -1.5255e-01, -1.2100e-01,  ...,  1.1974e-01,\n",
      "           7.1014e-02,  1.8529e-01],\n",
      "         [ 8.3256e-02, -1.5403e-01, -1.2309e-01,  ...,  1.2204e-01,\n",
      "           7.0187e-02,  1.8564e-01]]], grad_fn=<CopySlices>)\n",
      "torch.Size([22, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "outputs = seq2seq(src_batch, src_batch_lens, trg_batch)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "168b3bbc19afd1ef550d68b948460bcb86336de7649712fa882c5012c218f57c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
