{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 미니 배치와 데이터 로드(Mini Batch and Data Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 미니 배치와 배치 크기(Mini Batch and Batch Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 사용한 데이터의 샘플 개수는 5개이지만 실제 데이터는 매우 방대하다. 그렇기에 전체 데이터에 대해서 경사 하강법을 수행하는 것은 매우 느리고 심지어 메모리의 한계로 계산이 불가할 수도 있다. 그렇기에 전체 데이터를 더 작은 단위로 나누어 해당 단위로 학습하는 개념이 나오고 그 단위를 미니 배치(Mini Batch)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![미니배치](../image/미니배치.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니 배치 학습을 하면 미니 배치만큼만 가져가서 미니 배치에 대한 비용(cost)를 계산하고 경사 하강법을 수행한다. 그리고 다음 미니 배치를 가져가서 경사 하강법을 수행하고 마지막 미니 배치까지 반복한다. 이렇게 전체 데이터에 대한 학습이 1회 끝나면 1 Epoch가 끝나게 된다. 미니 배치 개수는 미니 배치의 크기를 몇으로 하느냐에 따라 결정되는데 이때 미니 배치의 크기를 배치 크기(Batch size)라고 한다.  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "> CPU와 GPU의 메모리가 2의 배수이므로 배치 크기가 2의 제곱수일 경우에 데이터 송수신 효율을 높일 수 있다. 그렇기에  배치 크기는 보통 2의 제곱수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 경사 하강법\n",
    "1. 전체 데이터에 대해 한 번에 경사 하강법을 수행하는 방법\n",
    "1. 전체 데이터를 사용하므로 가중치 값이 최적값으로 수렴한다.\n",
    "1. 계산량이 너무 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니 배치 경사 하강법\n",
    "1. 미니 배치 단위로 경사 하강법을 수행하는 방법\n",
    "1. 전체 데이터의 일부만을 보고 수행하므로 구해지는 값이 최적값과 조금 차이가 있다.\n",
    "1. 훈련 속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 이터레이션(Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이터레이션](../image/이터레이션.png)  \n",
    "위의 그림은 epoch와 배치 크기, 이터레이션의 관계를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이터레이션은 한 번의 에포크 내에서 이루어지는 매개변수인 W와 b의 업데이트 횟수다. 전체 데이터가 2000개일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10개이다. 이는 한 epoch 당 10번의 업데이트가 있음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 데이터 로드하기(Data Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train) # 데이터셋 생성\n",
    "\n",
    "# 미니 배치의 크기는 통상적으로 2의 배수를 사용\n",
    "# shuffle은 epoch마다 데이터셋을 섞어 모델이 데이터셋 순서에 익숙해지는 것을 방지한다.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Batch 1/3 Cost: 16474.558594\n",
      "Epoch    0/2000 Batch 2/3 Cost: 6701.916992\n",
      "Epoch    0/2000 Batch 3/3 Cost: 749.676392\n",
      "Epoch  100/2000 Batch 1/3 Cost: 0.936019\n",
      "Epoch  100/2000 Batch 2/3 Cost: 8.401047\n",
      "Epoch  100/2000 Batch 3/3 Cost: 2.017132\n",
      "Epoch  200/2000 Batch 1/3 Cost: 2.985704\n",
      "Epoch  200/2000 Batch 2/3 Cost: 6.101801\n",
      "Epoch  200/2000 Batch 3/3 Cost: 0.973596\n",
      "Epoch  300/2000 Batch 1/3 Cost: 5.860054\n",
      "Epoch  300/2000 Batch 2/3 Cost: 2.351194\n",
      "Epoch  300/2000 Batch 3/3 Cost: 0.309305\n",
      "Epoch  400/2000 Batch 1/3 Cost: 0.490613\n",
      "Epoch  400/2000 Batch 2/3 Cost: 0.101310\n",
      "Epoch  400/2000 Batch 3/3 Cost: 11.629374\n",
      "Epoch  500/2000 Batch 1/3 Cost: 0.376386\n",
      "Epoch  500/2000 Batch 2/3 Cost: 0.209860\n",
      "Epoch  500/2000 Batch 3/3 Cost: 10.003665\n",
      "Epoch  600/2000 Batch 1/3 Cost: 0.124687\n",
      "Epoch  600/2000 Batch 2/3 Cost: 3.710432\n",
      "Epoch  600/2000 Batch 3/3 Cost: 1.646493\n",
      "Epoch  700/2000 Batch 1/3 Cost: 3.708957\n",
      "Epoch  700/2000 Batch 2/3 Cost: 0.101562\n",
      "Epoch  700/2000 Batch 3/3 Cost: 0.755803\n",
      "Epoch  800/2000 Batch 1/3 Cost: 0.087579\n",
      "Epoch  800/2000 Batch 2/3 Cost: 0.264622\n",
      "Epoch  800/2000 Batch 3/3 Cost: 7.492252\n",
      "Epoch  900/2000 Batch 1/3 Cost: 0.134977\n",
      "Epoch  900/2000 Batch 2/3 Cost: 3.147816\n",
      "Epoch  900/2000 Batch 3/3 Cost: 1.469438\n",
      "Epoch 1000/2000 Batch 1/3 Cost: 0.009740\n",
      "Epoch 1000/2000 Batch 2/3 Cost: 0.733034\n",
      "Epoch 1000/2000 Batch 3/3 Cost: 6.097590\n",
      "Epoch 1100/2000 Batch 1/3 Cost: 0.239346\n",
      "Epoch 1100/2000 Batch 2/3 Cost: 3.064860\n",
      "Epoch 1100/2000 Batch 3/3 Cost: 0.390053\n",
      "Epoch 1200/2000 Batch 1/3 Cost: 0.077288\n",
      "Epoch 1200/2000 Batch 2/3 Cost: 2.559065\n",
      "Epoch 1200/2000 Batch 3/3 Cost: 1.494702\n",
      "Epoch 1300/2000 Batch 1/3 Cost: 0.109414\n",
      "Epoch 1300/2000 Batch 2/3 Cost: 0.244420\n",
      "Epoch 1300/2000 Batch 3/3 Cost: 5.202361\n",
      "Epoch 1400/2000 Batch 1/3 Cost: 2.244672\n",
      "Epoch 1400/2000 Batch 2/3 Cost: 0.101770\n",
      "Epoch 1400/2000 Batch 3/3 Cost: 0.046204\n",
      "Epoch 1500/2000 Batch 1/3 Cost: 0.019397\n",
      "Epoch 1500/2000 Batch 2/3 Cost: 0.401182\n",
      "Epoch 1500/2000 Batch 3/3 Cost: 4.205015\n",
      "Epoch 1600/2000 Batch 1/3 Cost: 0.057225\n",
      "Epoch 1600/2000 Batch 2/3 Cost: 1.574414\n",
      "Epoch 1600/2000 Batch 3/3 Cost: 1.677336\n",
      "Epoch 1700/2000 Batch 1/3 Cost: 0.172240\n",
      "Epoch 1700/2000 Batch 2/3 Cost: 2.462078\n",
      "Epoch 1700/2000 Batch 3/3 Cost: 0.019934\n",
      "Epoch 1800/2000 Batch 1/3 Cost: 1.495901\n",
      "Epoch 1800/2000 Batch 2/3 Cost: 1.426302\n",
      "Epoch 1800/2000 Batch 3/3 Cost: 0.017676\n",
      "Epoch 1900/2000 Batch 1/3 Cost: 0.471127\n",
      "Epoch 1900/2000 Batch 2/3 Cost: 1.657949\n",
      "Epoch 1900/2000 Batch 3/3 Cost: 0.057940\n",
      "Epoch 2000/2000 Batch 1/3 Cost: 0.398306\n",
      "Epoch 2000/2000 Batch 2/3 Cost: 0.749710\n",
      "Epoch 2000/2000 Batch 3/3 Cost: 2.614555\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "            cost.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때, 예측값 : tensor([[150.9443]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_val = torch.FloatTensor([[73, 80, 75]])\n",
    "pred_y = model(new_val)\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때, 예측값 :\", pred_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c33b62b3c09fa157f5fdff31c4db06b5670300ba67ef90747934390b2c977575"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('main': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
